%% Generated by Sphinx.
\def\sphinxdocclass{jupyterBook}
\documentclass[a4paper,12pt,polish]{jupyterBook}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax
\ifdefined\pdfimageresolution
    \pdfimageresolution= \numexpr \dimexpr1in\relax/\sphinxpxdimen\relax
\fi
%% let collapsible pdf bookmarks panel have high depth per default
\PassOptionsToPackage{bookmarksdepth=5}{hyperref}
%% turn off hyperref patch of \index as sphinx.xdy xindy module takes care of
%% suitable \hyperpage mark-up, working around hyperref-xindy incompatibility
\PassOptionsToPackage{hyperindex=false}{hyperref}
%% memoir class requires extra handling
\makeatletter\@ifclassloaded{memoir}
{\ifdefined\memhyperindexfalse\memhyperindexfalse\fi}{}\makeatother

\PassOptionsToPackage{warn}{textcomp}

\catcode`^^^^00a0\active\protected\def^^^^00a0{\leavevmode\nobreak\ }
\usepackage{cmap}
\usepackage{fontspec}
\defaultfontfeatures[\rmfamily,\sffamily,\ttfamily]{}
\usepackage{amsmath,amssymb,amstext}
\usepackage{polyglossia}
\setmainlanguage{polish}



\setmainfont{FreeSerif}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Italic,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldItalic
]
\setsansfont{FreeSans}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Oblique,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldOblique,
]
\setmonofont{FreeMono}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Oblique,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldOblique,
]



\usepackage[Sonny]{fncychap}
\ChNameVar{\Large\normalfont\sffamily}
\ChTitleVar{\Large\normalfont\sffamily}
\usepackage[,numfigreset=1,mathnumfig]{sphinx}

\fvset{fontsize=\small}
\usepackage{geometry}


% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}


\usepackage{sphinxmessages}



        % Start of preamble defined in sphinx-jupyterbook-latex %
         \usepackage[Latin,Greek]{ucharclasses}
        \usepackage{unicode-math}
        % fixing title of the toc
        \addto\captionsenglish{\renewcommand{\contentsname}{Contents}}
        \hypersetup{
            pdfencoding=auto,
            psdextra
        }
        % End of preamble defined in sphinx-jupyterbook-latex %
        

\title{Sieci neuronowe dla początkujących w Pythonie: wykłady w Jupyter Book}
\date{12 kwi 2022}
\release{}
\author{Wojciech Broniowski}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{}
\makeindex
\begin{document}

\pagestyle{empty}
\sphinxmaketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{docs/index::doc}}




\sphinxAtStartPar
\sphinxhref{https://www.ujk.edu.pl/~broniows}{\sphinxstylestrong{Wojciech Broniowski}}





\sphinxAtStartPar
Niniejsze wykłady były pierwotnie prowadzone dla studentów inżynierii danych na  \sphinxhref{https://www.ujk.edu.pl}{Uniwersytecie Jana Kochanowskiego} w Kielcach i dla \sphinxhref{https://kisd.ifj.edu.pl/news/}{Krakowskiej Szkoły Interdyscyplinarnych Studiów Doktoranckich}. Wyjaśniają bardzo podstawowe koncepcje sieci neuronowych na najbardziej przystępnym poziomie, wymagając od studenta jedynie bardzo podstawowej znajomości Pythona, a właściwie dowolnego języka programowania. W trosce o prostotę, kod dla różnych algorytmów sieci neuronowych pisany jest od podstaw, tj. bez użycia dedykowanych bibliotek wyższego poziomu. W ten sposób można dokładnie prześledzić wszystkie etapy programowania.

\begin{sphinxadmonition}{note}{Zwięzłość}

\sphinxAtStartPar
Tekst jest zwięzły (wydruk pdf ma \textasciitilde{}130 stron wraz z załącznikami), więc pilny student może ukończyć kurs w kilka popołudni!
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{Linki}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Jupyter Book:
\sphinxurl{https://bronwojtek.github.io/nn\_polish/docs/index.html}

\end{itemize}



\sphinxAtStartPar
Pierwotna angielska wersja książki:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Jupyter Book:
\sphinxurl{https://bronwojtek.github.io/neuralnets-in-raw-python/docs/index.html}

\item {} 
\sphinxAtStartPar
pdf i kody: \sphinxhref{https://www.ifj.edu.pl/~broniows/nn}{www.ifj.edu.pl/\textasciitilde{}broniows/nn} lub \sphinxhref{https://www.ujk.edu.pl/~broniows/nn}{www.ujk.edu.pl/\textasciitilde{}broniows/nn}

\end{itemize}
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{Jak uruchamiać kody w książce}

\sphinxAtStartPar
Główną zaletą książek wykonywalnych jest to, że czytelnik może cieszyć się z samodzielnego uruchamiania kodów źródłowych, modyfikowania ich, czy zabawy z parametrami. Nie jest potrzebne pobieranie, instalacja ani konfiguracja. Po prostu przejdź do

\sphinxAtStartPar
\sphinxurl{https://bronwojtek.github.io/nn\_polish/docs/index.html},

\sphinxAtStartPar
w menu po lewej stronie wybierz dowolny rozdział poniżej Wstępu, kliknij ikonę „rakiety” w prawym górnym rogu ekranu i wybierz „Colab” lub „Binder”. Po pewnym czasie inicjalizacji (za pierwszym razem dla Bindera trwa to dość długo) można uruchomić notebook.

\sphinxAtStartPar
Dla wykonywania lokalnego, kody dla każdego rozdziału w postaci
notebooków \sphinxhref{https://jupyter.org}{Jupytera} można pobrać klikając ikonę „strzałki w dół” w prawym górnym rogu ekranu. Pełen zestaw plików jest również dostępny z linków podanych powyżej.

\sphinxAtStartPar
Dodatek {\hyperref[\detokenize{docs/appendix:app-run}]{\sphinxcrossref{\DUrole{std,std-ref}{Jak uruchamiać kody książki}}}} wyjaśnia, jak postępować przy lokalnym wykonywaniu programów.
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{\protect\(~\protect\)}

\sphinxAtStartPar
Książka wykonywalna, utworzona przez oprogramowanie \sphinxhref{https://beta.jupyterbook.org/intro.html}{Jupyter Book
2.0}, będące częścią
\sphinxhref{https://ebp.jupyterbook.org/en/latest/}{ExecutableBookProject}.
\end{sphinxadmonition}




\chapter{Wstęp}
\label{\detokenize{docs/intro:wstep}}\label{\detokenize{docs/intro::doc}}

\section{Cel wykładu}
\label{\detokenize{docs/intro:cel-wykladu}}
\sphinxAtStartPar
Celem kursu jest wyłożenie podstaw wszechobecnych sieci neuronowych z pomocą \sphinxhref{https://www.python.org/}{Pythona} {[}\hyperlink{cite.docs/conclusion:id5}{Bar16}, \hyperlink{cite.docs/conclusion:id3}{Gut16}, \hyperlink{cite.docs/conclusion:id2}{Mat19}{]}. Zarówno kluczowe pojęcia sieci neuronowych, jak i programy ilustrujące są wyjaśniane na bardzo podstawowym poziomie, niemal „licealnym”. Kody, bardzo proste, zostały szczegółowo opisane. Ponadto są utworzone bez użycia specjalistycznych bibliotek wyższego poziomu dla sieci neuronowych, co pomaga w lepszym zrozumieniu przedstawionych algorytmów i pokazuje, jak programować je od podstaw.

\begin{sphinxadmonition}{note}{Dla kogo jest ta książka?}

\sphinxAtStartPar
\sphinxstylestrong{Czytelnik może być zupełnym nowicjuszem, tylko w niewielkim stopniu zaznajomionym z Pythonem (a właściwie każdym innym językiem programowania) i Jupyterem.}
\end{sphinxadmonition}

\sphinxAtStartPar
Materiał obejmuje takie klasyczne zagadnienia, jak perceptron i jego najprostsze zastosowania, nadzorowane uczenie z propagacją wsteczną do klasyfikacji danych, uczenie nienadzorowane i klasteryzacja, sieci samoorganizujące się Kohonena oraz sieci Hopfielda ze sprzężeniem zwrotnym. Ma to na celu przygotowanie niezbędnego gruntu dla najnowszych i aktualnych postępów (nie omówionych tutaj) w sieciach neuronowych, takich jak uczenie głębokie, sieci konwolucyjne, sieci rekurencyjne, generatywne sieci przeciwników, uczenie ze wzmacnianiem itp.

\sphinxAtStartPar
W trakcie kursu nowicjuszom zostanie delikatnie przemycone kilka podstawowych programów w Pythonie. W kodach znajdują się objaśnienia i komentarze.

\begin{sphinxadmonition}{note}{Ćwiczenia}

\sphinxAtStartPar
Na końcu każdego rozdziału proponujemy kilka ćwiczeń, których celem jest zapoznanie czytelnika z poruszanymi tematami i kodami. Większość ćwiczeń polega na prostych modyfikacjach/rozszerzeniach odpowiednich fragmentów materiału wykładowego.
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{Literatura}

\sphinxAtStartPar
Podręczników i notatek do wykładów poświęconych zagadnieniom poruszanym na tym kursie jest niezliczona ilość, stąd autor nie będzie próbował przedstawiać nawet niepełnego spisu literatury. Przytaczamy tylko pozycje, na które może spojrzeć bardziej zainteresowany czytelnik.
\end{sphinxadmonition}

\sphinxAtStartPar
Z prostotą jako drogowskazem, wybór tematów był inspirowany szczegółowymi wykładami \sphinxhref{http://vision.psych.umn.edu/users/kersten/kersten-lab/courses/Psy5038WF2014/IntroNeuralSyllabus.html}{Daniela Kerstena} w programie Mathematica, z internetowej książki \sphinxhref{https://page.mi.fu-berlin.de/rojas/neural/}{Raula Rojasa} (dostępna również w wersji drukowanej {[}\hyperlink{cite.docs/conclusion:id8}{FR13}{]}) oraz z punktu widzenia \sphinxstylestrong{fizyków} (jak ja!) z {[}\hyperlink{cite.docs/conclusion:id7}{MullerRS12}{]}.


\section{Inspiracja biologiczna}
\label{\detokenize{docs/intro:inspiracja-biologiczna}}
\sphinxAtStartPar
Inspiracją do opracowania matematycznych modeli obliczeniowych omawianych w tym kursie jest struktura biologiczna naszego układu nerwowego {[}\hyperlink{cite.docs/conclusion:id6}{KSJ+12}{]}. Centralny układ nerwowy (mózg) zawiera ogromną liczbę (\(\sim 10^{11}\)) \sphinxhref{https://human-memory.net/brain-neurons-synapses/}{neuronów}, które można postrzegać jako maleńkie  elementarne procesory. Otrzymują one sygnał poprzez \sphinxstylestrong{dendryty}, a jeśli jest on wystarczająco silny, jądro decyduje (obliczenie jest wykonane tutaj!) „wystrzelić” sygnał wyjściowy wzdłuż \sphinxstylestrong{aksonu}, gdzie jest on następnie przekazywany przez zakończenia aksonów do dendrytów innych neuronów. Połączenia aksonowo\sphinxhyphen{}dendryczne (połączenia \sphinxstylestrong{synaptyczne}) mogą być słabe lub silne, modyfikując przekazywany bodziec. Co więcej, siła połączeń synaptycznych może się zmieniać w czasie (\sphinxhref{https://en.wikipedia.org/wiki/Hebbian\_theory}{reguła Hebba} mówi nam, że połączenia stają się silniejsze, jeśli są używane wielokrotnie). W tym sensie neuron jest „programowalny”.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=450\sphinxpxdimen]{{neuron-structure}.jpg}
\caption{Biologiczny neuron (\sphinxurl{https://training.seer.cancer.gov/anatomy/nervous/tissue.html}).}\label{\detokenize{docs/intro:neuron-fig}}\end{figure}

\sphinxAtStartPar
Możemy zadać sobie pytanie, czy liczbę neuronów w mózgu rzeczywiście należy określać jako tak „ogromną”, jak się zwykle twierdzi. Porównajmy to do urządzeń obliczeniowych z układami pamięci. Liczba neuronów 10\(^{11}\) z grubsza odpowiada liczbie tranzystorów w chipie pamięci o pojemności 10 GB, co nie robi na nas specjalnego wrażenia, skoro w dzisiejszych czasach możemy kupić takie urządzenie za około 2\$.

\sphinxAtStartPar
Co więcej, prędkość przemieszczania się impulsów nerwowych, która jest wynikiem procesów elektrochemicznych, również nie jest imponująca. Najszybsze sygnały, takie jak te związane z pobudzaniem mięśni, przemieszczają się z prędkością do 120 m/s (osłonki mielinowe są niezbędne do ich osiągnięcia). Sygnały dotykowe osiągają około 80m/s, podczas gdy ból jest przenoszony ze stosunkowo bardzo małymi prędkościami 0,6m/s. To dlatego kiedy upuszczasz młotek na palec u nogi, czujesz to natychmiast, ale ból dociera do mózgu z opóźnieniem \textasciitilde{}1s, ponieważ musi pokonać odległość \textasciitilde{}1,5m. Z drugiej strony, w urządzeniach elektronicznych sygnał przemieszcza się w przewodach z prędkością rzędu prędkości światła, \(\sim 300000{\rm km/s}=3\times 10^{8}{\rm m/ s}\)!

\sphinxAtStartPar
W przypadku ludzi średni \sphinxhref{https://backyardbrains.com/experiments/reactiontime}{czas reakcji} wynosi 0,25 s na bodziec wizualny, 0,17 s na bodziec dźwiękowy i 0,15 s na dotyk. W ten sposób ustawienie progowego czasu dla falstartu w sprintach na 0,1 s jest bezpiecznie poniżej możliwej reakcji biegacza. Są to niezwykle powolne reakcje w porównaniu z odpowiedziami elektronicznymi.

\sphinxAtStartPar
Na podstawie zużycia energii przez mózg można oszacować, że neuron kory mózgowej \sphinxhref{https://aiimpacts.org/rate-of-neuron-firing/}{odpala} średnio raz na 6 sekund. Jest też mało prawdopodobne, aby przeciętny neuron odpalał częściej niż raz na sekundę. Pomnożenie tej szybkości wyzwalania przez liczbę wszystkich neuronów korowych, \(\sim 1.6 \times 10^{10}\), daje około 3 \(\times 10^{9}\) wyładowań/s w korze, czyli 3GHz. To jest cżęstotliwość This aktowania typowego chipa procesora! Jeśli więc odpalanie neuronu utożsamić z elementarnym obliczeniem, to tak określona moc mózgu jest z grubsza porównywalna z mocą standardowego procesora komputerowego.

\sphinxAtStartPar
Powyższe fakty wskazują, że z punktu widzenia naiwnych porównań z chipami krzemowymi ludzki mózg nie jest niczym szczególnym. Co zatem daje nam nasze wyjątkowe zdolności: niesłychanie wydajne rozpoznawanie wzorców wizualnych i dźwiękowych, myślenie, świadomość, intuicję, wyobraźnię? Odpowiedź wiąże się z niesamowicie rozbudowaną architekturą mózgu, w której każdy neuron (jednostka procesora) jest połączony poprzez synapsy średnio aż z 10000 (!) innych neuronów. Ta cecha sprawia, że ​​jest ona radykalnie inna i znacznie bardziej skomplikowana niż architektura składająca się z jednostki sterującej, procesora i pamięci w naszych komputerach (architektura \sphinxhref{https://en.wikipedia.org/wiki/Von\_Neumann\_architecture}{maszyny von Neumanna}) . Tam liczba połączeń jest rzędu liczby bitów pamięci, natomiast w ludzkim mózgu jest około \(10^{15}\) połączeń synaptycznych. Jak wspomniano, połączenia można „zaprogramować”, aby były silniejsze lub słabsze. Jeśli, dla prostego oszacowania, przybliżylibyśmy siłę połączenia tylko przez dwa stany synapsy, 0 lub 1, to całkowita liczba konfiguracji kombinatorycznych takiego systemu wynosiłaby \(2^{10^{15}}\) \sphinxhyphen{} „hiper\sphinxhyphen{}ogromna” liczba. Większość takich konfiguracji, oczywiście, nigdy nie jest realizowana w praktyce, niemniej jednak liczba możliwych stanów konfiguracyjnych mózgu lub „programów”, które może on realizować, jest naprawdę ogromna.

\sphinxAtStartPar
W ostatnich latach, wraz z rozwojem potężnych technik obrazowania, możliwe stało się mapowanie połączeń w mózgu z niespotykaną dotąd rozdzielczością, gdzie widoczne są pojedyncze wiązki nerwów. Wysiłki te są częścią {[}Projektu Human Connectome{]} (\sphinxurl{http://www.humanconnectomeproject.org}), którego ostatecznym celem jest dokłdne odwzorowanie architektury ludzkiego mózgu. W przypadku znacznie prostszej muszki owocowej, \sphinxhref{https://en.wikipedia.org/wiki/Drosophila\_connectome}{projekt drosophila connectome} jest bardzo zaawansowany.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=280\sphinxpxdimen]{{brain}.jpg}
\caption{Architektura mózgu włókna istoty białej (z projektu Human Connectome \sphinxhref{http://www.humanconnectomeproject.org/gallery/}{humanconnectomeproject.org})}\label{\detokenize{docs/intro:connectome-fig}}\end{figure}

\begin{sphinxadmonition}{important}{Ważne:}
\sphinxAtStartPar
Cecha „ogromnej łączności”, z miriadami neuronów służących jako równoległe procesory elementarne, sprawia, że ​​mózg jest zupełnie innym urządzeniem obliczeniowym niż \sphinxhref{https://en.wikipedia.org/wiki/Von\_Neumann\_architecture}{maszyna von Neumanna} (tj. nasze codzienne komputery).
\end{sphinxadmonition}


\section{Sieci feed\sphinxhyphen{}forward}
\label{\detokenize{docs/intro:sieci-feed-forward}}
\sphinxAtStartPar
Neurofizjologiczne badania mózgu dostarczają ważnych wskazówek dla modeli matematycznych stosowanych w sztucznych sieciach neuronowych (\sphinxstylestrong{ANN}). I odwrotnie, postępy w algorytmice ANN często przybliżają nas do zrozumienia, jak faktycznie może działać nasz „komputer mózgowy”!

\sphinxAtStartPar
Najprostsze sieci ANN to tak zwane sieci \sphinxstylestrong{feed forward}, zilustrowane w \hyperref[\detokenize{docs/intro:ffnn-fig}]{Rys.\@ \ref{\detokenize{docs/intro:ffnn-fig}}}. Składają się one z warstwy \sphinxstylestrong{wejściowej} (czarne kropki), która reprezentuje tylko dane cyfrowe, oraz warstw neuronów (kolorowych kropek). Liczba neuronów w każdej warstwie może być różna. Złożoność sieci i zadań, które może ona realizować, rośnie rzecz jasna wraz z liczbą warstw i liczbą neuronów.

\sphinxAtStartPar
W dalszej części tego rozdziału podamy, w dość skondensownej postaci, kilka ważnych definicji:

\sphinxAtStartPar
Sieci z jedną warstwą neuronów nazywane są sieciami \sphinxstylestrong{jednowarstwowymi}. Ostatnia warstwa (jasnoniebieskie kropki) nazywana jest \sphinxstylestrong{warstwą wyjściową}. W sieciach wielowarstwowych (więcej niż jedna warstwa neuronowa) warstwy neuronowe poprzedzające warstwę wyjściową (fioletowe kropki) nazywane są \sphinxstylestrong{warstwami pośrednimi}. Jeśli liczba warstw jest duża (np. 64, 128, …), mamy do czynienia ze stosowanymi od niedawna „przełomowymi” \sphinxstylestrong{głębokimi sieciami}.

\sphinxAtStartPar
Neurony w różnych warstwach nie muszą działać w ten sam sposób, w szczególności neurony wyjściowe mogą zachowywać się inaczej niż pośrednie.

\sphinxAtStartPar
Sygnał z wejścia wędruje po wskazanych strzałkami łączach (krawędziach, połączeniach synaptycznych) do neuronów w kolejnych warstwach. W sieciach typu feed\sphinxhyphen{}forward, jak ta na \hyperref[\detokenize{docs/intro:ffnn-fig}]{Rys.\@ \ref{\detokenize{docs/intro:ffnn-fig}}}, sygnał może poruszać się tylko do przodu (na rysunku od lewej do prawej): od wejścia do pierwszej warstwy neuronowej, od pierwszej do drugiej, i tak dalej, aż do osiągnięcia wyjścia. Nie jest dozwolone cofanie się do poprzednich warstw ani równoległa propagacja pomiędzy neuronami tej samej warstwy. Byłaby to wówczas sieć z \sphinxstylestrong{powracaniem}, o czym nieco mówimy w rozdziale {\hyperref[\detokenize{docs/som:lat-lab}]{\sphinxcrossref{\DUrole{std,std-ref}{Lateral inhibition}}}}.

\sphinxAtStartPar
Jak szczegółowo opisujemy w kolejnych rozdziałach, wędrujący sygnał jest odpowiednio \sphinxstylestrong{przetwarzany} przez neurony, stąd urządzenie wykonuje obliczenia: wejście jest przekształcane w wyjście.

\sphinxAtStartPar
W przykładowej sieci \hyperref[\detokenize{docs/intro:ffnn-fig}]{Rys.\@ \ref{\detokenize{docs/intro:ffnn-fig}}} każdy neuron z poprzedniej warstwy jest połączony z każdym neuronem w następnej warstwie. Takie sieci ANN są nazywane \sphinxstylestrong{w pełni połączonymi}.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=300\sphinxpxdimen]{{feed_f}.png}
\caption{Przykładowa, w pełni połączona sztuczna sieć neuronowa typu feed\sphinxhyphen{}forward. Kolorowe plamy reprezentują neurony, a krawędzie uskazują połączenia synaptyczne. Sygnał rozchodzi się od wejścia (czarne kropki), przez neurony w kolejnych warstwach pośrednich (ukrytych) (fioletowe kropki), do warstwy wyjściowej (jasnoniebieskie kropki). Siła połączeń jest kontrolowana przez wagi (hiperparametry) przypisane do krawędzi.}\label{\detokenize{docs/intro:ffnn-fig}}\end{figure}

\sphinxAtStartPar
Jak omówimy bardziej szczegółowo późniwj, każda krawędź (połączenie synaptyczne) w sieci ma pewną „siłę” opisaną liczbą o nazwie \sphinxstylestrong{waga} (wagi są również określane jako \sphinxstylestrong{hiperparametry}). Nawet bardzo małe w pełni połączone sieci, takie jak ta z \hyperref[\detokenize{docs/intro:ffnn-fig}]{Rys.\@ \ref{\detokenize{docs/intro:ffnn-fig}}}, mają bardzo wiele połączeń (tutaj 30), stąd zawierają dużo hiperparametrów. Tak więc, choć czasami wyglądają niewinnie, ANN są w rzeczywistości bardzo złożonymi systemami wieloparametrycznymi. Co więcej, kluczową cechą jest tutaj nieliniowość odpowiedzi neuronów, co omawiamy w kolejnym rozdziale {\hyperref[\detokenize{docs/mcp:mcp-lab}]{\sphinxcrossref{\DUrole{std,std-ref}{Neuron MCP}}}}.


\section{Dlaczego Python}
\label{\detokenize{docs/intro:dlaczego-python}}
\sphinxAtStartPar
Wybór języka \sphinxhref{https://en.wikipedia.org/wiki/Python\_(programming\_language)}{Python} dla prościutkich kodów tego kursu prawie nie wymaga wyjaśnienia. Zacytujmy tylko \sphinxhref{https://en.wikipedia.org/wiki/Tim\_Peters\_(software\_engineer)}{Tima Petersa}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Piękne jest lepsze niż brzydkie.

\item {} 
\sphinxAtStartPar
Jawne jest lepsze niż niejawne.

\item {} 
\sphinxAtStartPar
Proste jest lepsze niż złożone.

\item {} 
\sphinxAtStartPar
Złożone jest lepsze niż skomplikowane.

\item {} 
\sphinxAtStartPar
Liczy się czytelność.

\end{itemize}

\sphinxAtStartPar
Według \sphinxhref{https://developer-tech.com/news/2021/apr/27/slashdata-javascript-python-boast-largest-developer-communities/}{SlashData}, na świecie jest obecnie ponad 10 milionów programistów używających Pythona, zaraz po jezyku JavaScript (\textasciitilde{}14 milionów). W szczególności Python okazuje się bardzo praktyczny w zastosowaniach do sieci ANN.


\subsection{Importowane pakiety}
\label{\detokenize{docs/intro:importowane-pakiety}}
\sphinxAtStartPar
W trakcie tego kursu używamy kilku standardowych pakietów bibliotecznych Pythona do obliczeń numerycznych, wykresów itp. Jak podkreśliliśmy, nie korzystamy z żadnych bibliotek specjalnie dedykowanych sieciom neuronowym. Notebook każdego wykładu zaczyna się od zaimportowania niektórych z tych bibliotek:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}              \PYG{c+c1}{\PYGZsh{} numerical}
\PYG{k+kn}{import} \PYG{n+nn}{statistics} \PYG{k}{as} \PYG{n+nn}{st}         \PYG{c+c1}{\PYGZsh{} statistics}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt} \PYG{c+c1}{\PYGZsh{} plotting}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib} \PYG{k}{as} \PYG{n+nn}{mpl}        \PYG{c+c1}{\PYGZsh{} plotting}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{cm} \PYG{k}{as} \PYG{n+nn}{cm}      \PYG{c+c1}{\PYGZsh{} contour plots }

\PYG{k+kn}{from} \PYG{n+nn}{mpl\PYGZus{}toolkits}\PYG{n+nn}{.}\PYG{n+nn}{mplot3d}\PYG{n+nn}{.}\PYG{n+nn}{axes3d} \PYG{k+kn}{import} \PYG{n}{Axes3D}   \PYG{c+c1}{\PYGZsh{} 3D plots}
\PYG{k+kn}{from} \PYG{n+nn}{IPython}\PYG{n+nn}{.}\PYG{n+nn}{display} \PYG{k+kn}{import} \PYG{n}{display}\PYG{p}{,} \PYG{n}{Image}\PYG{p}{,} \PYG{n}{HTML} \PYG{c+c1}{\PYGZsh{} display imported graphics}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxadmonition}{note}{\sphinxstylestrong{neural} package}

\sphinxAtStartPar
Tworzone podczas tego kursu funkcje, które są później wielokrotnie używane, są umieszczane w pakiecie prywatnej biblioteki \sphinxstylestrong{neural}, opisanym w załączniku {\hyperref[\detokenize{docs/appendix:app-lab}]{\sphinxcrossref{\DUrole{std,std-ref}{Pakiet neural}}}}.
\end{sphinxadmonition}

\sphinxAtStartPar
Celem kompatybilności z środowikiem Google Colab, pakiet  \sphinxstylestrong{neural} importowany jest z repozytorium w następujący sposób:

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-output}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{os}\PYG{n+nn}{.}\PYG{n+nn}{path} 

\PYG{n}{isdir} \PYG{o}{=} \PYG{n}{os}\PYG{o}{.}\PYG{n}{path}\PYG{o}{.}\PYG{n}{isdir}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{lib\PYGZus{}nn}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} check whether \PYGZsq{}lib\PYGZus{}nn\PYGZsq{} exists}

\PYG{k}{if} \PYG{o+ow}{not} \PYG{n}{isdir}\PYG{p}{:}
   \PYG{o}{!}git clone https://github.com/bronwojtek/lib\PYGZus{}nn.git \PYGZsh{} cloning the library from github

\PYG{k+kn}{import} \PYG{n+nn}{sys}                     
\PYG{n}{sys}\PYG{o}{.}\PYG{n}{path}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{./lib\PYGZus{}nn}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)} 

\PYG{k+kn}{from} \PYG{n+nn}{neural} \PYG{k+kn}{import} \PYG{o}{*}            \PYG{c+c1}{\PYGZsh{} importing my library package}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
Więcej informacji można znaleźć w dodatku {\hyperref[\detokenize{docs/appendix:app-lab}]{\sphinxcrossref{\DUrole{std,std-ref}{Pakiet neural}}}}.

\begin{sphinxadmonition}{note}{Informacja:}
\sphinxAtStartPar
Dla zwięzłości prezentacji, niektóre zbędne (np. import bibliotek) lub nieistotne fragmenty kodu są obecne tylko w notebookach Jupytera (do pobrania) i nie są ukazywane w książce. Dzięki temu tekst jest krótszy i czytelny.
\end{sphinxadmonition}


\chapter{Neuron MCP}
\label{\detokenize{docs/mcp:neuron-mcp}}\label{\detokenize{docs/mcp:mcp-lab}}\label{\detokenize{docs/mcp::doc}}

\section{Definicja}
\label{\detokenize{docs/mcp:definicja}}
\sphinxAtStartPar
Potrzebujemy podstawowego składnika ANN: sztucznego neuronu. Pierwszy model matematyczny pochodzi od Warrena McCullocha i Waltera Pittsa (MCP){[}\hyperlink{cite.docs/conclusion:id9}{MP43}{]}, którzy zaproponowali go w 1942 roku, a więc na samym początku ery komputerów elektronicznych podczas II wojny światowej. Neuron MCP przedstawiony na \hyperref[\detokenize{docs/mcp:mcp1-fig}]{Rys.\@ \ref{\detokenize{docs/mcp:mcp1-fig}}} jest podstawowym składnikiem wszystkich ANN omawianych w tym kursie. Jest zbudowany na bardzo prostych ogólnych zasadach, inspirowanych przez neuron biologiczny:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Sygnał wchodzi do jądra przez dendryty z innych neuronów.

\item {} 
\sphinxAtStartPar
Połączenie synaptyczne dla każdego dendrytu może mieć inną (i regulowaną) siłę (wagę).

\item {} 
\sphinxAtStartPar
W jądrze sygnał ważony ze wszystkich dendrytów jest sumowany i oznaczony jako \(s\).

\item {} 
\sphinxAtStartPar
Jeżeli sygnał \(s\) jest silniejszy niż pewien zadany próg, to neuron odpala sygnał wzdłuż aksonu, w przeciwnym przypadku pozostaje pasywny.

\item {} 
\sphinxAtStartPar
W najprostszej realizacji, siła odpalanego sygnału ma tylko dwa możliwe poziomy: włączony lub wyłączony, tj. 1 lub 0. Nie są potrzebne wartości pośrednie.

\item {} 
\sphinxAtStartPar
Akson łączy się z dendrytami innych neuronów, przekazując im swój sygnał.

\end{itemize}

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=320\sphinxpxdimen]{{mcp-1a}.png}
\caption{Neuron MCP: \(x_i\) oznaczają wejście, \(w_i\)  wagi, \(s\) zsumowany sygnał, \(b\) próg, a \(f(s;b)\) reprezentuje funkcję aktywacji, dającą wyjście \(y =f(s;b)\). Niebieski owal otacza cały neuron, jak np. w notacji \hyperref[\detokenize{docs/intro:ffnn-fig}]{Rys.\@ \ref{\detokenize{docs/intro:ffnn-fig}}}.}\label{\detokenize{docs/mcp:mcp1-fig}}\end{figure}

\sphinxAtStartPar
Przekładając to na matematyczną receptę, przypisuje się komórkom wejściowym liczby \(x_1, x_2 \dots, x_n\) (punkt danych wejściowych). Siła połączeń synaptycznych jest kontrolowana przez \sphinxstylestrong{wagi} \(w_i\). Następnie łączny sygnał jest zdefiniowany jako suma ważona
\begin{equation*}
\begin{split}s=\sum_{i=1}^n x_i w_i.\end{split}
\end{equation*}
\sphinxAtStartPar
Sygnał staje się argumentem \sphinxstylestrong{funkcji aktywacji}, która w najprostszym przypadku przybiera postać funkcji schodkowej
\begin{equation*}
\begin{split}f(s;b) = \left \{ \begin{array}{l} 1 {\rm ~dla~}s \ge b \\ 0 {\rm ~dla~}s < b \end{array} \right .\end{split}
\end{equation*}
\sphinxAtStartPar
Gdy łączny sygnał \(s\) jest większy niż próg \(b\), jądro odpala. tj. sygnał idący wzdłuż aksonu wynosi 1. W przeciwnym przypadku wartość generowanego sygnału wynosi 0 (brak odpalenia). Właśnie tego potrzebujemy, aby naśladować biologiczny prototyp!

\sphinxAtStartPar
Istnieje wygodna konwencja, która jest często używana. Zamiast oddzielać próg od danych wejściowych, możemy traktować te liczby wrównoważny sposób. Warunek odpalenia może być trywialnie przekształcony jako
\begin{equation*}
\begin{split}
s \ge b \to s-b \ge 0 \to \sum_{i=1}^n x_i w_i - b \ge 0 \to \sum_{i=1}^n x_i w_i +x_0 w_0 \ge 0
\to \sum_{i=0}^n x_i w_i \ge 0,
\end{split}
\end{equation*}
\sphinxAtStartPar
gdzie \(x_0=1\) i \(w_0=-b\). Innymi słowy, możemy traktować próg jako wagę na krawędzi połączonej z dodatkową komórką z wejściem zawsze ustawionym na 1. Ta notacja jest pokazana na \hyperref[\detokenize{docs/mcp:mcp2-fig}]{Rys.\@ \ref{\detokenize{docs/mcp:mcp2-fig}}}. Teraz funkcja aktywacji wynosi po prostu
\begin{equation}\label{equation:docs/mcp:eq-f}
\begin{split}f(s) = \left \{ \begin{array}{l} 1 {\rm ~for~} s \ge 0 \\ 0 {\rm ~for~} s < 0 \end{array} \right .,\end{split}
\end{equation}
\sphinxAtStartPar
ze wskaźnikiem sumowania w \(s\) zaczynającym się \(0\):
\begin{equation}\label{equation:docs/mcp:eq-f0}
\begin{split}s=\sum_{i=0}^n x_i w_i = x_0 w_0+x_1 w_1 + \dots + x_n w_n.\end{split}
\end{equation}
\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=320\sphinxpxdimen]{{mcp-2a}.png}
\caption{Alternatywna, bardziej jednorodna representacja neuronu MCP, z \(x_0=1\) i \(w_0=-b\).}\label{\detokenize{docs/mcp:mcp2-fig}}\end{figure}

\sphinxAtStartPar
Wagi \(w_0=-b,w_1,\dots,w_n\) są ogólnie określane jako \sphinxstylestrong{hiperparametry}. Określają one funkcjonalność neuronu MCP i mogą ulegać zmianie podczas procesu uczenia się (trenowania) sieci (patrz kolejne rozdziały). Natomiast są one ustalone podczas używania już wytrenowanej sieci na określonej próbce danych wejściowych.

\begin{sphinxadmonition}{important}{Ważne:}
\sphinxAtStartPar
Istotną właściwością neuronów w ANN jest \sphinxstylestrong{nieliniowość} funkcji aktywacji. Bez tej cechy neuron MCP reprezentowałby po prostu iloczyn skalarny, a (wielowarstwowe) sieci feed\sphinxhyphen{}forward sprowadzałyby się do trywialnego mnożenia macierzy.
\end{sphinxadmonition}


\section{Neuron MCP w Pythonie}
\label{\detokenize{docs/mcp:neuron-mcp-w-pythonie}}\label{\detokenize{docs/mcp:mcp-p-lab}}
\sphinxAtStartPar
Zaimplementujemy teraz model matematyczny neuronu MCP w Pythonie. Rzecz jasna, potrzebujemy tablic (wektorów), które są reprezentowane jako

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{x} \PYG{o}{=} \PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{,}\PYG{l+m+mi}{7}\PYG{p}{]}
\PYG{n}{w} \PYG{o}{=} \PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mf}{2.5}\PYG{p}{]}
\PYG{n}{x}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
[1, 3, 7]
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
i (\sphinxstylestrong{co ważne}) są indeksowane począwszy od 0, np.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{x}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
1
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Zauważ, że wypisanie nazwy zmiennej na końcu komórki powoduje wydrukowanie jej zawartości.

\sphinxAtStartPar
Funkcje biblioteczne numpy mają przedrostek \sphinxstylestrong{np}, który jest aliasem podanym podczas importu. Funkcje te działają \sphinxstyleemphasis{dystrybucyjnie} na tablice, np.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{np}\PYG{o}{.}\PYG{n}{sin}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
array([0.84147098, 0.14112001, 0.6569866 ])
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
co jest bardzo wygodną własnością przy programowaniu. Mamy też do dyspozycji iloczyn skalarny \(x \cdot w = \sum_i x_i w_i\), którego używamy do określenia sygnału \(s\) wchodzącego do neuronu MCP:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,}\PYG{n}{w}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
21.5
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Następnie musimy zdefiniować funkcję aktywacji neuronu, która w najprostszej postaci jest funkcją schodkową \eqref{equation:docs/mcp:eq-f}:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{step}\PYG{p}{(}\PYG{n}{s}\PYG{p}{)}\PYG{p}{:}       \PYG{c+c1}{\PYGZsh{} step function (in the neural library)}
     \PYG{k}{if} \PYG{n}{s} \PYG{o}{\PYGZgt{}} \PYG{l+m+mi}{0}\PYG{p}{:}     \PYG{c+c1}{\PYGZsh{} condition satisfied}
        \PYG{k}{return} \PYG{l+m+mi}{1}
     \PYG{k}{else}\PYG{p}{:}         \PYG{c+c1}{\PYGZsh{} otherwise}
        \PYG{k}{return} \PYG{l+m+mi}{0}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
Funkcja znajduje się też a pakiecie \sphinxstylestrong{neural}, zob. {\hyperref[\detokenize{docs/appendix:app-lab}]{\sphinxcrossref{\DUrole{std,std-ref}{dodatek}}}}. Dla wzrokowców, wykres funkcji schodkowej jest następujący:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mf}{2.3}\PYG{p}{,}\PYG{l+m+mf}{2.3}\PYG{p}{)}\PYG{p}{,}\PYG{n}{dpi}\PYG{o}{=}\PYG{l+m+mi}{120}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} set the size and resolution of the figure}

\PYG{n}{s} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{linspace}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{100}\PYG{p}{)}   \PYG{c+c1}{\PYGZsh{} array of 100+1 equally spaced points in [\PYGZhy{}2, 2]}
\PYG{n}{fs} \PYG{o}{=} \PYG{p}{[}\PYG{n}{step}\PYG{p}{(}\PYG{n}{z}\PYG{p}{)} \PYG{k}{for} \PYG{n}{z} \PYG{o+ow}{in} \PYG{n}{s}\PYG{p}{]}     \PYG{c+c1}{\PYGZsh{} corresponding array of function values}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{signal s}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}      \PYG{c+c1}{\PYGZsh{} axes labels}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{response f(s)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{step function}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} plot title}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{s}\PYG{p}{,} \PYG{n}{fs}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{mcp_25_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Ponieważ z definicji \(x_0=1\), nie chcemy przekazywać tej wartości w argumentach funkcji modelujących neuron MCP. Będziemy zatem dodawać \(x_0=1\) na początku danych wejściowych, jak w tym przykładzie:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{x}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mi}{5}\PYG{p}{,}\PYG{l+m+mi}{7}\PYG{p}{]}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{insert}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} insert 1 in x at position 0}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
array([1, 5, 7])
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Jesteśmy teraz gotowi by zdefiniwać {\hyperref[\detokenize{docs/mcp:mcp1-fig}]{\sphinxcrossref{\DUrole{std,std-ref}{neuron MCP}}}}:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{neuron}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,}\PYG{n}{w}\PYG{p}{,}\PYG{n}{f}\PYG{o}{=}\PYG{n}{step}\PYG{p}{)}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} (in the neural library)}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}                 }
\PYG{l+s+sd}{    MCP neuron}

\PYG{l+s+sd}{    x: array of inputs  [x1, x2,...,xn]}
\PYG{l+s+sd}{    w: array of weights [w0, w1, w2,...,wn]}
\PYG{l+s+sd}{    f: activation function, with step as default}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    return: signal=weighted sum w0 + x1 w1 + x2 w2 +...+ xn wn = x.w}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}} 
    \PYG{k}{return} \PYG{n}{f}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{insert}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{,}\PYG{n}{w}\PYG{p}{)}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} insert x0=1 into x, output f(x.w)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
Starannie umieszczamy stosowne komentarze w potrójnych cudzysłowach, aby w razie potrzeby móc uzyskać pomoc:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{help}\PYG{p}{(}\PYG{n}{neuron}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
Help on function neuron in module \PYGZus{}\PYGZus{}main\PYGZus{}\PYGZus{}:

neuron(x, w, f=\PYGZlt{}function step at 0x7fe94a7f8670\PYGZgt{})
    MCP neuron
    
    x: array of inputs  [x1, x2,...,xn]
    w: array of weights [w0, w1, w2,...,wn]
    f: activation function, with step as default
    
    return: signal=weighted sum w0 + x1 w1 + x2 w2 +...+ xn wn = x.w
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Zauważ, że funkcja \sphinxstylestrong{f} jest argumentem \sphinxstylestrong{neuron}u. Argument ten jest domyślnie ustawiony jako \sphinxstylestrong{step}, więc nie musi być obecny na liście argumentów. Przykładowe użycie z \(x_1=3\), \(w_0=-b=-2\) i \(w_1=1\) to

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{neuron}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{3}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
1
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Jak widzimy, w tym przypadku neuron odpalił, poniważ \(s=1*(-2)+3*1>0\).

\sphinxAtStartPar
Poniżej pokazujemy, jak neuron działa na daną wejściową \(x_1\) wziętą z przedziału \([-2,2]\). Zmieniamy również wartość progu, aby zilustrować jego rolę: jeśli sygnał \(x_1 w_1\) jest większy niż \(b=-x_0\), neuron odpala.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mf}{2.3}\PYG{p}{,}\PYG{l+m+mf}{2.3}\PYG{p}{)}\PYG{p}{,}\PYG{n}{dpi}\PYG{o}{=}\PYG{l+m+mi}{120}\PYG{p}{)} 

\PYG{n}{s} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{linspace}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{200}\PYG{p}{)}
\PYG{n}{fs1} \PYG{o}{=} \PYG{p}{[}\PYG{n}{neuron}\PYG{p}{(}\PYG{p}{[}\PYG{n}{x1}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)} \PYG{k}{for} \PYG{n}{x1} \PYG{o+ow}{in} \PYG{n}{s}\PYG{p}{]}      \PYG{c+c1}{\PYGZsh{} more function on one plot}
\PYG{n}{fs0} \PYG{o}{=} \PYG{p}{[}\PYG{n}{neuron}\PYG{p}{(}\PYG{p}{[}\PYG{n}{x1}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)} \PYG{k}{for} \PYG{n}{x1} \PYG{o+ow}{in} \PYG{n}{s}\PYG{p}{]}
\PYG{n}{fsm12} \PYG{o}{=} \PYG{p}{[}\PYG{n}{neuron}\PYG{p}{(}\PYG{p}{[}\PYG{n}{x1}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)} \PYG{k}{for} \PYG{n}{x1} \PYG{o+ow}{in} \PYG{n}{s}\PYG{p}{]}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZdl{}x\PYGZus{}1\PYGZdl{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{response}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Change of bias}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{s}\PYG{p}{,} \PYG{n}{fs1}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{b=\PYGZhy{}1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{s}\PYG{p}{,} \PYG{n}{fs0}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{b=0}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{s}\PYG{p}{,} \PYG{n}{fsm12}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{b=1/2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{legend}\PYG{p}{(}\PYG{p}{)}     \PYG{c+c1}{\PYGZsh{} legend}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{mcp_35_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Kiedy znak wagi \(w_1\) jest ujemny, dostajemy \sphinxstylestrong{odwrotne} zachowanie: neuron odpala dla \(x_1 |w_1| < w_0\):

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{mcp_37_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\begin{sphinxadmonition}{note}{Informacja:}
\sphinxAtStartPar
Począwszy od teraz, dla zwięzłości prezentacji, ukrywamy niektóre komórki kodu o powtarzającej się strukturze. Czytelnik może znaleźć pełny kod w oryginalnych notatnikach Jupytera.
\end{sphinxadmonition}

\sphinxAtStartPar
Trzeba przyznać, że w ostatnim przykładzie odchodzi się od biologicznego wzorca, ponieważ ujemne wagi nie są możliwe do zrealizowania w biologicznym neuronie. Przyjeta swoboda wzbogaca jednak model matematyczny, który w oczywisty sposób można budować bez ograniczeń biologicznych.


\section{Funkcje logiczne}
\label{\detokenize{docs/mcp:funkcje-logiczne}}\label{\detokenize{docs/mcp:bool-sec}}
\sphinxAtStartPar
Skonstruowawszy neuronu MCP w Pythonie możemy zadać pytanie: \sphinxstyleemphasis{Jaka jest najprostsze (ale wciąż nietrywialne) zastosowanie, w którym możemy go użyć?} Są to {[}funkcje logiczne{]}(\sphinxurl{https://en} .wikipedia.org/wiki/Boolean\_function) lub sieci logiczne utworzone za pomocą sieci neuronów MCP.

\sphinxAtStartPar
Funkcje logiczne z definicji mają argumenty i wartości zawierające się w zbiorze \(\{ 0,1 \}\) lub \{Prawda, Fałsz\}.

\sphinxAtStartPar
Na rozgrzewkę zacznijmy od zgadywania, gdzie bierzemy neuron o wagach \(w=[w_0,w_1,w_2]=[-1,0.6,0.6]\) (dlaczego nie). Oznaczmy też \(x_1=p\), \(x_2=q\), zgodnie z tradycyjną notacją zmiennych logicznych, gdzie \(p,q \in \{0,1\}\).

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{p q n(p,q)}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} print the header}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{p}{)}             \PYG{c+c1}{\PYGZsh{} print space}

\PYG{k}{for} \PYG{n}{p} \PYG{o+ow}{in} \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{:}       \PYG{c+c1}{\PYGZsh{} loop over p}
    \PYG{k}{for} \PYG{n}{q} \PYG{o+ow}{in} \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{:}   \PYG{c+c1}{\PYGZsh{} loop over q}
        \PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{p}\PYG{p}{,}\PYG{n}{q}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{n}{neuron}\PYG{p}{(}\PYG{p}{[}\PYG{n}{p}\PYG{p}{,}\PYG{n}{q}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mf}{.6}\PYG{p}{,}\PYG{l+m+mf}{.6}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} print all cases}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
p q n(p,q)

0 0  0
0 1  0
1 0  0
1 1  1
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Natychmiast rozpoznajemy w powyższym wyniku tabelkę logiczną dla koniunkcji, \(n(p,q)=p \land q\) lub logicznej operacji \sphinxstylestrong{AND}. Jest zupełnie jasne, dlaczego tak działa nasz neuron. Warunek odpalenia \(n(p,q)=1\) wynosi \(-1+p*0.6+q*0.6 \ge 0\) i jest spełniony wtedy i tylko wtedy, gdy \(p=q=1\), co jest definicją koniunkcji logicznej. Oczywiście moglibyśmy użyć tutaj 0.7 zamiast 0.6, lub ogólnie \(w_1\) i \(w_2\) takie, że \(w_1<1, w_2<1, w_1+w_2 \ge 1\). W terminologii elektronicznej obecny neuron możemy więc nazwać \sphinxstylestrong{bramką AND}.

\sphinxAtStartPar
Możemy w ten sposób zdefiniować funkcję

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{neurAND}\PYG{p}{(}\PYG{n}{p}\PYG{p}{,}\PYG{n}{q}\PYG{p}{)}\PYG{p}{:} \PYG{k}{return} \PYG{n}{neuron}\PYG{p}{(}\PYG{p}{[}\PYG{n}{p}\PYG{p}{,}\PYG{n}{q}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mf}{.6}\PYG{p}{,}\PYG{l+m+mf}{.6}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
W podobny sposób możemy zdefiniować inne funkcje logiczne (bramki logiczne) dwóch zmiennych logicznych. W szczególności bramka NAND (negacja koniunkcji) i bramka OR (alternatywa) są realizowane poprzez następujące neurony MCP:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{neurNAND}\PYG{p}{(}\PYG{n}{p}\PYG{p}{,}\PYG{n}{q}\PYG{p}{)}\PYG{p}{:} \PYG{k}{return} \PYG{n}{neuron}\PYG{p}{(}\PYG{p}{[}\PYG{n}{p}\PYG{p}{,}\PYG{n}{q}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.6}\PYG{p}{,}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.6}\PYG{p}{]}\PYG{p}{)}
\PYG{k}{def} \PYG{n+nf}{neurOR}\PYG{p}{(}\PYG{n}{p}\PYG{p}{,}\PYG{n}{q}\PYG{p}{)}\PYG{p}{:}   \PYG{k}{return} \PYG{n}{neuron}\PYG{p}{(}\PYG{p}{[}\PYG{n}{p}\PYG{p}{,}\PYG{n}{q}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mf}{1.2}\PYG{p}{,}\PYG{l+m+mf}{1.2}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
Odpowiadają następującym tabelkom logicznym

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{p q  NAND OR}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} print the header}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{p}{)}

\PYG{k}{for} \PYG{n}{p} \PYG{o+ow}{in} \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{:} 
    \PYG{k}{for} \PYG{n}{q} \PYG{o+ow}{in} \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{:} 
        \PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{p}\PYG{p}{,}\PYG{n}{q}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{n}{neurNAND}\PYG{p}{(}\PYG{n}{p}\PYG{p}{,}\PYG{n}{q}\PYG{p}{)}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{n}{neurOR}\PYG{p}{(}\PYG{n}{p}\PYG{p}{,}\PYG{n}{q}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
p q  NAND OR

0 0   1   0
0 1   1   1
1 0   1   1
1 1   0   1
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}

\subsection{Problem z bramką XOR}
\label{\detokenize{docs/mcp:problem-z-bramka-xor}}
\sphinxAtStartPar
Bramka XOR, lub \sphinxstylestrong{alternatywa wykluczjąca}, jest zdefiniowana za pomocą następującej tabelki logicznej:
\begin{equation*}
\begin{split}
\begin{array}{ccc}
p & q & p \oplus q \\
0 & 0 & 0 \\
0 & 1 & 1 \\
1 & 0 & 1 \\
1 & 1 & 0
\end{array}
\end{split}
\end{equation*}
\sphinxAtStartPar
Jest to jedna z możliwych funkcji binarnych dwóch argumentów (w sumie mamy 16 różnych funkcji tego rodzaju, dlaczego?). Moglibyśmy teraz próbować dobrać wagi w naszym neuronie, aby zachowywał się jak bramka XOR, ale jesteśmy skazani na porażkę. Oto jej powód:

\sphinxAtStartPar
Z pierwszego wiersza powyższej tabelki wynika, że dla wejścia 0, 0 neuron nie powinien odpalić. Stąd

\sphinxAtStartPar
\(w_0 + 0* w_1 + 0*w_2 <0\) lub \(-w_0>0\).

\sphinxAtStartPar
W przypadku wierszy 2 i 3 neuron musi odpalić, zatem

\sphinxAtStartPar
\(w_0+w_2 \ge 0\) i \(w_0+w_1 \ge 0\).

\sphinxAtStartPar
Dodając stronami te trzy uzyskane nierówności otrzymujemy \(w_0+w_1+w_2 >0\). Jednak czwarty rząd tabelki daje
\(w_0+w_1+w_2<0\) (brak odpalenia), więc uzyskujemy sprzeczność. Dlatego nie istnieje taki wybor \(w_0, w_1, w_2\), aby neuron działał jak bramka XOR!

\begin{sphinxadmonition}{important}{Ważne:}
\sphinxAtStartPar
Pojedynczy neuron MCP nie może działać jak bramka \sphinxstylestrong{XOR}.
\end{sphinxadmonition}


\subsection{XOR ze złożenia bramek AND, NAND i OR}
\label{\detokenize{docs/mcp:xor-ze-zlozenia-bramek-and-nand-i-or}}
\sphinxAtStartPar
Można rozwiązać problem konstrukcji bramki XOR, składając trzy neurony MCP, np.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{neurXOR}\PYG{p}{(}\PYG{n}{p}\PYG{p}{,}\PYG{n}{q}\PYG{p}{)}\PYG{p}{:} \PYG{k}{return} \PYG{n}{neurAND}\PYG{p}{(}\PYG{n}{neurNAND}\PYG{p}{(}\PYG{n}{p}\PYG{p}{,}\PYG{n}{q}\PYG{p}{)}\PYG{p}{,}\PYG{n}{neurOR}\PYG{p}{(}\PYG{n}{p}\PYG{p}{,}\PYG{n}{q}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{p q XOR}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)} 
\PYG{n+nb}{print}\PYG{p}{(}\PYG{p}{)}

\PYG{k}{for} \PYG{n}{p} \PYG{o+ow}{in} \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{:} 
    \PYG{k}{for} \PYG{n}{q} \PYG{o+ow}{in} \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{:} 
        \PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{p}\PYG{p}{,}\PYG{n}{q}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{n}{neurXOR}\PYG{p}{(}\PYG{n}{p}\PYG{p}{,}\PYG{n}{q}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
p q XOR

0 0  0
0 1  1
1 0  1
1 1  0
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Powyższa konstrukcja odpowiada prostj sieci \hyperref[\detokenize{docs/mcp:xor-fig}]{Rys.\@ \ref{\detokenize{docs/mcp:xor-fig}}}.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=260\sphinxpxdimen]{{xor}.png}
\caption{Bramka XOR złożona z neuronów NAND, OR i AND.}\label{\detokenize{docs/mcp:xor-fig}}\end{figure}

\sphinxAtStartPar
Zauważmy, że po raz pierwszy mamy tu do czynienia z siecią posiadającą warstwę pośrednią, składającą się z neuronów NAND i OR. Ta warstwa jest nieodzowna do budowy bramki XOR.


\subsection{Bramka XOR złożona z bramek NAND}
\label{\detokenize{docs/mcp:bramka-xor-zlozona-z-bramek-nand}}
\sphinxAtStartPar
W ramach teorii sieci logicznych udowadnia się, że dowolna sieć (lub dowolna funkcja logiczna) może składać się wyłącznie z bramek NAND lub wyłącznie z bramek NOR. Mówi się, że bramki NAND (lub NOR) są \sphinxstylestrong{zupełne}. W szczególności bramka XOR może być skonstruowana jako

\sphinxAtStartPar
{[} p NAND ( p NAND q ) {]} NAND {[} q NAND ( p NAND q ) {]},

\sphinxAtStartPar
co możemy napisać w Pythonie jako

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{nXOR}\PYG{p}{(}\PYG{n}{i}\PYG{p}{,}\PYG{n}{j}\PYG{p}{)}\PYG{p}{:} \PYG{k}{return} \PYG{n}{neurNAND}\PYG{p}{(}\PYG{n}{neurNAND}\PYG{p}{(}\PYG{n}{i}\PYG{p}{,}\PYG{n}{neurNAND}\PYG{p}{(}\PYG{n}{i}\PYG{p}{,}\PYG{n}{j}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,}\PYG{n}{neurNAND}\PYG{p}{(}\PYG{n}{j}\PYG{p}{,}\PYG{n}{neurNAND}\PYG{p}{(}\PYG{n}{i}\PYG{p}{,}\PYG{n}{j}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{p q XOR}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)} 
\PYG{n+nb}{print}\PYG{p}{(}\PYG{p}{)}

\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{:} 
    \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{:} 
        \PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{i}\PYG{p}{,}\PYG{n}{j}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{n}{nXOR}\PYG{p}{(}\PYG{n}{i}\PYG{p}{,}\PYG{n}{j}\PYG{p}{)}\PYG{p}{)} 
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
p q XOR

0 0  0
0 1  1
1 0  1
1 1  0
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\begin{sphinxadmonition}{note}{Informacja:}
\sphinxAtStartPar
Dowodzi się, że sieci logiczne są zupełne w sensie \sphinxhref{https://en.wikipedia.org/wiki/Church-Turing\_thesis}{Churcha\sphinxhyphen{}Turinga}, tj. (jeśli są wystarczająco duże) mogą wykonać każde możliwe obliczenie. Ta własność jest bezpośrednio przenoszona na sieci ANN. Historycznie, było to podstawowe odkrycie przełomowego artykułu MCP {[}\hyperlink{cite.docs/conclusion:id9}{MP43}{]}.
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{Wniosek}

\sphinxAtStartPar
Dostatecznie duże ANN mogą wykonac każde obliczenie!
\end{sphinxadmonition}


\section{Ćwiczenia}
\label{\detokenize{docs/mcp:cwiczenia}}
\begin{sphinxadmonition}{note}{\protect\(~\protect\)}

\sphinxAtStartPar
Skonstruuj (wszystko w Pythonie)
\begin{itemize}
\item {} 
\sphinxAtStartPar
bramkę realizująca koniunkcję kilku zmiennych logicznych;

\item {} 
\sphinxAtStartPar
bramki NOT, NOR;

\item {} 
\sphinxAtStartPar
bramki OR, AND i NOT poprzez \sphinxhref{https://en.wikipedia.org/wiki/NAND\_logic}{złożenie bramek NAND};

\item {} 
\sphinxAtStartPar
\sphinxhref{https://en.wikipedia.org/wiki/Adder\_(electronics)}{pół sumator i pełny sumator},

\end{itemize}

\sphinxAtStartPar
jako sieci neuronów MCP.
\end{sphinxadmonition}


\chapter{Modele pamięci}
\label{\detokenize{docs/memory:modele-pamieci}}\label{\detokenize{docs/memory::doc}}

\section{Pamieć skojarzeniowa (heteroasocjacyjna)}
\label{\detokenize{docs/memory:pamiec-skojarzeniowa-heteroasocjacyjna}}\label{\detokenize{docs/memory:het-lab}}

\subsection{Skojarzenia par}
\label{\detokenize{docs/memory:skojarzenia-par}}
\sphinxAtStartPar
Przechodzimy teraz do dalszych ilustracji elementarnych możliwości ANN, opisujących dwa bardzo proste modele pamięci oparte na algebrze liniowej, uzupełnione o (nieliniowe) filtrowanie. Mówiąc o pamięci, na miejscu jest słowo przestrogi. Mamy tu do czynienia z dość uproszczonymi narzędziami, które są dalekie od złożonego i dotychczas niezrozumiałego mechanizmu pamięci działającego w naszym mózgu. Obecne rozumienie jest takie, że te mechanizmy obejmują sprzężenie zwrotne w sieciach, co wykracza poza rozważane tutaj sieci typu feed\sphinxhyphen{}forward.

\sphinxAtStartPar
Pierwszy rozważany model dotyczy tzw. pamięci \sphinxstylestrong{heterasocjacyjnej}, w której niektóre obiekty (tutaj graficzne symbole bitmapowe) są kojarzone w pary. Dla konkretnego przykładu bierzemy zbiór pięciu symboli graficznych \{A, a, I, i, Y\} i definiujemy dwa skojarznia par: A \(\leftrightarrow\) a oraz I \(\leftrightarrow\) i, czyli pomiędzy różnymi (hetero) symbolami. Symbol Y pozostaje nieskojarzony.

\sphinxAtStartPar
Symbole są zdefiniowane jako 2\sphinxhyphen{}wymiarowe (\(12 \times 12\)) tablice pikseli, np.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
    \PYG{n}{A} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}
       \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
       \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
       \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
       \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
       \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
       \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
       \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
       \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}     
       \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
       \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}  
       \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
       \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)} 
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
Pozostałe symbole są zdefiniowane podobnie.

\sphinxAtStartPar
Użyjemy standardowego pakietu do rysowania, zaimportowanego wcześniej. Cały zestaw naszych symboli wygląda jak poniżej, z kolorem żółtym=1 i fioletowym=0:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sym}\PYG{o}{=}\PYG{p}{[}\PYG{n}{A}\PYG{p}{,}\PYG{n}{a}\PYG{p}{,}\PYG{n}{ii}\PYG{p}{,}\PYG{n}{I}\PYG{p}{,}\PYG{n}{Y}\PYG{p}{]}   \PYG{c+c1}{\PYGZsh{} array of symbols, numbered from 0 to 4}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{16}\PYG{p}{,} \PYG{l+m+mi}{6}\PYG{p}{)}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} figure with horizontal and vertical size}

\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{6}\PYG{p}{)}\PYG{p}{:}     \PYG{c+c1}{\PYGZsh{} loop over 5 figure panels, i is from 1 to 5}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplot}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{,} \PYG{n}{i}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} panels, numbered from 1 to 5}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{axis}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{off}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}      \PYG{c+c1}{\PYGZsh{} no axes}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{sym}\PYG{p}{[}\PYG{n}{i}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} plot symbol, numbered from 0 to 4}
    
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}    
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{memory_13_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\begin{sphinxadmonition}{warning}{Ostrzeżenie:}
\sphinxAtStartPar
W Pythonie zakres range \((i,j)\) zawiera \(i\), ale nie obejmuje \(j\), tj. równa się tablicy \([i, i+1, \dots, j-1]\). Ponadto range\((i)\) obejmuje \(0, 1, \dots, i-1\). Różni się to od konwencji przyjętej w niektórych innych językach programowania.
\end{sphinxadmonition}

\sphinxAtStartPar
Wygodniej jest pracować nie z powyższymi tablicami dwuwymiarowymi, ale z jednowymiarowymi wektorami uzyskanymi za pomocą tzw. procedury \sphinxstylestrong{spłaszczania}, gdzie macierz jest ,,pocięta” wzdłuż swoich wierszy, złożonych w wektor. Na przykład

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{t}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{4}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{l+m+mi}{3}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{7}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} a matrix}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{t}\PYG{p}{)}                            
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{t}\PYG{o}{.}\PYG{n}{flatten}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}                    \PYG{c+c1}{\PYGZsh{} matrix flattened into a vector   }
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
[[1 2 3]
 [0 4 0]
 [3 2 7]]
[1 2 3 0 4 0 3 2 7]
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
A zatem przeprowadzamy spłaszczenie na naszym zestawie symboli

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{fA}\PYG{o}{=}\PYG{n}{A}\PYG{o}{.}\PYG{n}{flatten}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{fa}\PYG{o}{=}\PYG{n}{a}\PYG{o}{.}\PYG{n}{flatten}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{fi}\PYG{o}{=}\PYG{n}{ii}\PYG{o}{.}\PYG{n}{flatten}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{fI}\PYG{o}{=}\PYG{n}{I}\PYG{o}{.}\PYG{n}{flatten}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{fY}\PYG{o}{=}\PYG{n}{Y}\PYG{o}{.}\PYG{n}{flatten}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
aby otrzymać, np.

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{memory_20_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{memory_21_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
Zaletą pracy z wektorami jest to, że możemy użyć wbudowanego iloczynu skalarnego. Zauważmy, że tutaj iloczyn skalarny wektorów odpowiadających dwóm symbolom jest po prostu równy liczbie wspólnych żółtych pikseli. Na przykład dla spłaszczonych symboli A oraz i, narysowanych powyżej, mamy tylko dwa wspólne żółte piksele:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{fA}\PYG{p}{,}\PYG{n}{fi}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
2
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Jasne jest, że można użyć iloczynu skalarnego jako miary podobieństwa między symbolami. Aby poniżej pzedstawiony model pamięci skojarzeniowej działał, symbole nie powinny być zbyt podobne, ponieważ mogą być wtedy „mylone”.


\subsection{Macierz pamięci}
\label{\detokenize{docs/memory:macierz-pamieci}}
\sphinxAtStartPar
Następna koncepcja algebraiczna, której potrzebujemy, to \sphinxstylestrong{iloczyn zewnętrzny}. Dla dwóch wektorów \(v\) i \(w\) jest on zdefiniowany jako \(v w^T = v \otimes w\) (w przeciwieństwie do iloczynu skalarnego, gdzie \(w^T v = w \cdot v\)). Tutaj \(T\) oznacza transpozycję. Wynikiem jest macierz z liczbą wierszy równą długości \(v\) i liczbą kolumn równą długości \(w\).

\sphinxAtStartPar
Na przykład dla
\begin{equation*}
\begin{split} v = \left ( \begin{array}{c} v_1 \\ v_2 \\v_3 \end{array}  \right ), \;\;\;\; w = \left ( \begin{array}{c} w_1 \\ w_2 \end{array}  \right ), \end{split}
\end{equation*}
\sphinxAtStartPar
mamy
\begin{equation*}
\begin{split} 
v \otimes w = v w^T=
\left ( \begin{array}{c} v_1 \\ v_2 \\v_3 \end{array}  \right ) (w_1,w_2)
= \left ( \begin{array}{cc} v_1 w_1 & v_1 w_2 \\ v_2 w_1 & v_2 w_2 \\v_3 v_1 & v_3 w_2 \end{array}  \right ).
\end{split}
\end{equation*}
\sphinxAtStartPar
(przypomnij sobie z algebry, że mnożymy „wiersze przez kolumny”). w \sphinxstylestrong{numpy}

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{outer}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{7}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} outer product of two vectors}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
[[ 2  7]
 [ 4 14]
 [ 6 21]]
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Następnie konstruujemy \sphinxstylestrong{macierz pamięci} potrzebną do modelowania pamięci heteroasocjacyjnej. Załóżmy najpierw, dla uproszczenia notacji, że mamy tylko dwa skojarzenia: \(a \to A\) i \(b \to B\). Niech
\begin{equation*}
\begin{split}M = A a^T/a\cdot a + B b^T/b\cdot b.\end{split}
\end{equation*}
\sphinxAtStartPar
Wówczas
\begin{equation*}
\begin{split}M a=  A + B \, a\cdot b /b \cdot a, \end{split}
\end{equation*}
\sphinxAtStartPar
i jeśli \(a\) and \(b\) byłyby \sphinxstylestrong{ortogonalne}, tj. \(a \cdot b =0\), to

\sphinxAtStartPar
\( M a =  A\),

\sphinxAtStartPar
dając dokładne skojarzenie. Podobnie otrzymalibyśmy \(Mb = B\). Ponieważ jednak w ogólnym przypadku wektory nie są dokładnie ortogonalne, generowany jest pewien błąd \(B \, b \cdot a/a \cdot a\) (dla asocjacji \(a\)). Zwykle jest on mały, jeśli liczba pikseli w naszych symbolach jest duża, a symbole są, ogólnie rzecz biorąc, niezbyt do siebie podobne (nie mają zbyt wielu wspólnych pikseli). Jak wkrótce zobaczymy, pojawiający się błąd można wydajnie „odfiltrować”  odpowiednią funkcją aktywacji neuronów.

\sphinxAtStartPar
Wracając do naszego konkretnego przypadku, potrzebujemy zatem czterech członów w \(M\), ponieważ
\(a \to A\), \(A\to a\), \(I \to i\) oraz \(i \to I\):

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{M}\PYG{o}{=}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{outer}\PYG{p}{(}\PYG{n}{fA}\PYG{p}{,}\PYG{n}{fa}\PYG{p}{)}\PYG{o}{/}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{fa}\PYG{p}{,}\PYG{n}{fa}\PYG{p}{)}\PYG{o}{+}\PYG{n}{np}\PYG{o}{.}\PYG{n}{outer}\PYG{p}{(}\PYG{n}{fa}\PYG{p}{,}\PYG{n}{fA}\PYG{p}{)}\PYG{o}{/}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{fA}\PYG{p}{,}\PYG{n}{fA}\PYG{p}{)}
   \PYG{o}{+}\PYG{n}{np}\PYG{o}{.}\PYG{n}{outer}\PYG{p}{(}\PYG{n}{fi}\PYG{p}{,}\PYG{n}{fI}\PYG{p}{)}\PYG{o}{/}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{fI}\PYG{p}{,}\PYG{n}{fI}\PYG{p}{)}\PYG{o}{+}\PYG{n}{np}\PYG{o}{.}\PYG{n}{outer}\PYG{p}{(}\PYG{n}{fI}\PYG{p}{,}\PYG{n}{fi}\PYG{p}{)}\PYG{o}{/}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{fi}\PYG{p}{,}\PYG{n}{fi}\PYG{p}{)}\PYG{p}{)}\PYG{p}{;} \PYG{c+c1}{\PYGZsh{} associated pairs}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
Teraz, jako test jak to działa, dla każdego spłaszczonego symbolu \(s\) obliczamy \(Ms\). Wynikiem jest wektor, który chcemy przywrócić do postaci tablicy pikseli \(12\times 12\). Operacją odwrotną do spłaszczania w Pythonie jest \sphinxstylestrong{reshape}. Na przykład

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{tt}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{,}\PYG{l+m+mi}{5}\PYG{p}{]}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} test vector}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{tt}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} cutting into 2 rows of length 2}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
[[1 2]
 [3 5]]
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Dla naszych wektorów mamy

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{Ap}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{M}\PYG{p}{,}\PYG{n}{fA}\PYG{p}{)}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{l+m+mi}{12}\PYG{p}{,}\PYG{l+m+mi}{12}\PYG{p}{)}
\PYG{n}{ap}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{M}\PYG{p}{,}\PYG{n}{fa}\PYG{p}{)}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{l+m+mi}{12}\PYG{p}{,}\PYG{l+m+mi}{12}\PYG{p}{)}
\PYG{n}{Ip}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{M}\PYG{p}{,}\PYG{n}{fI}\PYG{p}{)}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{l+m+mi}{12}\PYG{p}{,}\PYG{l+m+mi}{12}\PYG{p}{)}
\PYG{n}{ip}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{M}\PYG{p}{,}\PYG{n}{fi}\PYG{p}{)}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{l+m+mi}{12}\PYG{p}{,}\PYG{l+m+mi}{12}\PYG{p}{)}
\PYG{n}{Yp}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{M}\PYG{p}{,}\PYG{n}{fY}\PYG{p}{)}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{l+m+mi}{12}\PYG{p}{,}\PYG{l+m+mi}{12}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} we also try the unassociated symbol Y}

\PYG{n}{symp}\PYG{o}{=}\PYG{p}{[}\PYG{n}{Ap}\PYG{p}{,}\PYG{n}{ap}\PYG{p}{,}\PYG{n}{Ip}\PYG{p}{,}\PYG{n}{ip}\PYG{p}{,}\PYG{n}{Yp}\PYG{p}{]}          \PYG{c+c1}{\PYGZsh{} array of associated symbols}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
W przypadku skojarzenia z A (które powinno wynosić a) procedura daje nastepujący wynik (stosujemy tu dla ładniejszego wydruku zaokrąglanie do 2 cyfr dziesiętnych poprzez \sphinxstylestrong{np.round})

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(}\PYG{n}{Ap}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} pixel map for the association of the symbol A}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
[[0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]
 [0.   0.   0.   0.   0.25 0.85 0.25 0.   0.   0.   0.   0.  ]
 [0.   0.   0.   0.   0.   0.85 0.   0.   0.   0.   0.   0.  ]
 [0.   0.   0.   1.   1.6  1.85 1.89 0.   0.   0.   0.   0.  ]
 [0.   0.   1.   0.   0.6  0.25 1.6  0.   0.   0.   0.   0.  ]
 [0.   0.   1.   0.6  0.   0.54 1.29 0.6  0.   0.   0.   0.  ]
 [0.   0.   1.   0.6  0.   0.25 1.29 0.6  0.   0.   0.   0.  ]
 [0.   0.   0.6  1.6  1.6  1.85 1.89 1.6  0.6  0.   0.   0.  ]
 [0.   0.   0.6  0.   0.   0.25 0.29 0.   0.6  0.   0.   0.  ]
 [0.   0.6  0.   0.   0.   0.25 0.   0.29 0.29 0.6  0.   0.  ]
 [0.   0.6  0.   0.   0.25 0.25 0.25 0.   0.   0.6  0.   0.  ]
 [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]]
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Zauważamy, że intensywność pikseli niekoniecznie jest teraz równa 0 lub 1, tak jak w oryginalnych symbolach. Przedstawienie graficzne wygląda następująco:

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{memory_37_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
Powinniśmy być w stanie zobaczyć na powyższym obrazku sekwencję a, A, i, I oraz nic szczególnego w powiązaniu z Y. Prawie tak jest, ale sytuacja nie jest idealna ze względu na omówiony powyżej błąd wynikający z nieortogonalności.


\subsection{Nakładanie filtra}
\label{\detokenize{docs/memory:nakladanie-filtra}}
\sphinxAtStartPar
Wynik znacznie się poprawi, gdy do powyższych map pikseli zostanie zastosowany filtr. Patrząc na powyższy rysunek zauważamy, że powinniśmy pozbyć się „słabych cieni”, a pozostawić tylko piksele o wystarczającej sile, które następnie powinny otrzymac warość 1. Innymi słowy, piksele poniżej progu filtra \(b\) powinny zostać zresetowane do 0, a te powyżej lub równe \(b\) powinny zostać zresetowane do 1. Można to zgrabnie osiągnąć za pomocą naszego \sphinxstylestrong{neuronu} z rozdz. {\hyperref[\detokenize{docs/mcp:mcp-p-lab}]{\sphinxcrossref{\DUrole{std,std-ref}{Neuron MCP w Pythonie}}}}. Funkcja ta została umieszczona w bibliotece \sphinxstylestrong{neural} (patrz {\hyperref[\detokenize{docs/appendix:app-lab}]{\sphinxcrossref{\DUrole{std,std-ref}{Dodatek}}}}).

\sphinxAtStartPar
A zatem definiujemy filtry jako neurony MCP o wagach \(w_0=-b\) i \(w_1=1\):

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{filter}\PYG{p}{(}\PYG{n}{a}\PYG{p}{,}\PYG{n}{b}\PYG{p}{)}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} a \PYGZhy{} symbol (2\PYGZhy{}dim pixel array), b \PYGZhy{} bias}
    \PYG{n}{n}\PYG{o}{=}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{a}\PYG{p}{)}     \PYG{c+c1}{\PYGZsh{} number of rows (and columns)}
    \PYG{k}{return} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{n}{func}\PYG{o}{.}\PYG{n}{neuron}\PYG{p}{(}\PYG{p}{[}\PYG{n}{a}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{n}{j}\PYG{p}{]}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{n}{b}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)} \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{n}\PYG{p}{)}\PYG{p}{]} \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{n}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}
       \PYG{c+c1}{\PYGZsh{} 2\PYGZhy{}dim array with the filter applied}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
Działając na symbol Ap z odpowiednio dobranym \(b=0.9\) (przyjęty poziom progu jest tutaj bardzo istotny), uzyskujemy

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n+nb}{filter}\PYG{p}{(}\PYG{n}{Ap}\PYG{p}{,}\PYG{l+m+mf}{.9}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
[[0 0 0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0 0 0]
 [0 0 0 1 1 1 1 0 0 0 0 0]
 [0 0 1 0 0 0 1 0 0 0 0 0]
 [0 0 1 0 0 0 1 0 0 0 0 0]
 [0 0 1 0 0 0 1 0 0 0 0 0]
 [0 0 0 1 1 1 1 1 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 0 0 0 0]]
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
gdzie możemy zauważyć „czysty” symbol a. Sprawdzamy, czy faktycznie filtrowanie działa tak doskonale we wszystkich naszych skojarzeniach:

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{memory_47_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
Można łatwo podać reprezentację właśnie przedstawionego modelu pamięci heteroasocjacyjnej jako \sphinxstylestrong{jednowarstwową} sieć neuronów MCP. Na poniższym wykresie ukazujemy wszystkie operacje, idąc od lewej strony do prawej. Symbol wejściowy jest spłaszczony. Warstwy wejściowa i wyjściowa są w pełni połączone krawędziami (których nie pokazano) łączącymi komórki wejściowe z neuronami w warstwie wyjściowej. Wagi krawędzi są równe elementom macierzy \(M_{ij}\), oznaczonej symbolem M. Funkcja aktywacji jest taka sama dla wszystkich neuronów i ma postać funkcji schodkowej.

\sphinxAtStartPar
Na dole rysunku wskazujemy elementy wektora wejściowego \(x_i\), sygnału docierającego do neuronu \(j\), tj. \(s_j=\sum_i x_i M_{ij}\) oraz wynik końcowy \(y_j=f(s_j)\).

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{memory_49_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\begin{sphinxadmonition}{note}{Podsumowanie modelu pamieci heteroassociatywnej}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Zdefiniuj pary skojarzonych symboli i skonstruuj macierz pamięci \(M\).

\item {} 
\sphinxAtStartPar
Wejście to symbol w postaci 2\sphinxhyphen{}wymiarowej tablicy pikseli o wartościach 0 lub 1.

\item {} 
\sphinxAtStartPar
Spłaszcz symbol do wektora, który tworzy warstwę danych wejściowych \(x_i\).

\item {} 
\sphinxAtStartPar
Macierz wag w pełni połączonej sieci ANN to \(M\).

\item {} 
\sphinxAtStartPar
Sygnał wchodzący do neuronu \(j\) w warstwie wyjściowej to \(s_j=\sum_i x_i M_{ij}\).

\item {} 
\sphinxAtStartPar
Funkcja aktywacji to funkcja schodkowa z odpowiednio dobranym progiem. Daje ona \(y_j=f(s_j)\).

\item {} 
\sphinxAtStartPar
Potnij wektor wyjściowy na macierz pikseli, która stanowi ostateczny wynik.
Powinien to być symbol skojarzony z symbolem na wejściu.

\end{enumerate}
\end{sphinxadmonition}


\section{Pamieć autoasocjatywna}
\label{\detokenize{docs/memory:pamiec-autoasocjatywna}}

\subsection{Samo\sphinxhyphen{}skojarzenia}
\label{\detokenize{docs/memory:samo-skojarzenia}}
\sphinxAtStartPar
Model pamięci autoasocjacyjnej jest w bliskiej analogii do przypadku pamięci skojarzeniowej, ale teraz każdy symbol jest kojarzony \sphinxstylestrong{z samym sobą}. Dlaczego robimy coś takiego, stanie się jasne, gdy weźmiemy pod uwagę zniekształcone dane wejściowe. Definiujemy macierz asocjacji w następujący sposób:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{Ma}\PYG{o}{=}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{outer}\PYG{p}{(}\PYG{n}{fA}\PYG{p}{,}\PYG{n}{fA}\PYG{p}{)}\PYG{o}{/}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{fA}\PYG{p}{,}\PYG{n}{fA}\PYG{p}{)}\PYG{o}{+}\PYG{n}{np}\PYG{o}{.}\PYG{n}{outer}\PYG{p}{(}\PYG{n}{fa}\PYG{p}{,}\PYG{n}{fa}\PYG{p}{)}\PYG{o}{/}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{fa}\PYG{p}{,}\PYG{n}{fa}\PYG{p}{)}
    \PYG{o}{+}\PYG{n}{np}\PYG{o}{.}\PYG{n}{outer}\PYG{p}{(}\PYG{n}{fi}\PYG{p}{,}\PYG{n}{fi}\PYG{p}{)}\PYG{o}{/}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{fi}\PYG{p}{,}\PYG{n}{fi}\PYG{p}{)}\PYG{o}{+}\PYG{n}{np}\PYG{o}{.}\PYG{n}{outer}\PYG{p}{(}\PYG{n}{fI}\PYG{p}{,}\PYG{n}{fI}\PYG{p}{)}\PYG{o}{/}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{fI}\PYG{p}{,}\PYG{n}{fI}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
Po przemnożeniu spłaszczonego symbolu przez macierz Ma i przefiltrowaniu (wszystkie kroki jak w przypadku skojarzeniowym) otrzymujemy poprawnie oryginalne symbole (poza Y, który nie był z niczym powiązany).

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{memory_57_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}

\subsection{Zniekształcone symbole}
\label{\detokenize{docs/memory:znieksztalcone-symbole}}
\sphinxAtStartPar
Teraz wyobraź sobie, że oryginalny symbol zostaje częściowo zniszczony, a niektóre piksele są losowo zmieniane z 1 na 0 i odwrotnie.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{ne}\PYG{o}{=}\PYG{l+m+mi}{62}   \PYG{c+c1}{\PYGZsh{} number of alterations}

\PYG{k}{for} \PYG{n}{s} \PYG{o+ow}{in} \PYG{n}{sym}\PYG{p}{:}                     \PYG{c+c1}{\PYGZsh{} loop over symbols}
    \PYG{k}{for} \PYG{n}{\PYGZus{}} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ne}\PYG{p}{)}\PYG{p}{:}           \PYG{c+c1}{\PYGZsh{} loop over alteratons}
        \PYG{n}{i}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randint}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{12}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} random position in row}
        \PYG{n}{j}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randint}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{12}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} random position in column}
        \PYG{n}{s}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{n}{j}\PYG{p}{]}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{o}{\PYGZhy{}}\PYG{n}{s}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{n}{j}\PYG{p}{]}           \PYG{c+c1}{\PYGZsh{} trick to switch 1 and 0}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
Po tym zniszczeniu symbole wejściowe wyglądają tak:

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{memory_62_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}

\subsection{Odtworzenie symboli}
\label{\detokenize{docs/memory:odtworzenie-symboli}}
\sphinxAtStartPar
Następnie stosujemy nasz model pamięci autoasocjacyjnej do wszystkich „zniszczonych” symboli:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{Ap}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{Ma}\PYG{p}{,}\PYG{n}{fA}\PYG{p}{)}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{l+m+mi}{12}\PYG{p}{,}\PYG{l+m+mi}{12}\PYG{p}{)}
\PYG{n}{ap}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{Ma}\PYG{p}{,}\PYG{n}{fa}\PYG{p}{)}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{l+m+mi}{12}\PYG{p}{,}\PYG{l+m+mi}{12}\PYG{p}{)}
\PYG{n}{Ip}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{Ma}\PYG{p}{,}\PYG{n}{fI}\PYG{p}{)}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{l+m+mi}{12}\PYG{p}{,}\PYG{l+m+mi}{12}\PYG{p}{)}
\PYG{n}{ip}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{Ma}\PYG{p}{,}\PYG{n}{fi}\PYG{p}{)}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{l+m+mi}{12}\PYG{p}{,}\PYG{l+m+mi}{12}\PYG{p}{)}
\PYG{n}{Yp}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{Ma}\PYG{p}{,}\PYG{n}{fY}\PYG{p}{)}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{l+m+mi}{12}\PYG{p}{,}\PYG{l+m+mi}{12}\PYG{p}{)}

\PYG{n}{symp}\PYG{o}{=}\PYG{p}{[}\PYG{n}{Ap}\PYG{p}{,}\PYG{n}{ap}\PYG{p}{,}\PYG{n}{Ip}\PYG{p}{,}\PYG{n}{ip}\PYG{p}{,}\PYG{n}{Yp}\PYG{p}{]} 
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
co daje

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{memory_67_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
Po przefiltrowaniu z progiem \(b=0.9\) odzyskujemy originale symbole:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{16}\PYG{p}{,} \PYG{l+m+mi}{6}\PYG{p}{)}\PYG{p}{)}

\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{6}\PYG{p}{)}\PYG{p}{:}     
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplot}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{,} \PYG{n}{i}\PYG{p}{)}  
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{axis}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{off}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}       
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n+nb}{filter}\PYG{p}{(}\PYG{n}{symp}\PYG{p}{[}\PYG{n}{i}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{l+m+mf}{0.9}\PYG{p}{)}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} plot filtered symbol}
    
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{memory_69_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Zastosowanie algorytmu może zatem odszyfrować „zniszczony” tekst lub, bardziej ogólnie, zapewnić mechanizm korekcji błędów.

\begin{sphinxadmonition}{note}{Podsumowanie modelu pamieci autoasocjatywnej}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Skonstruuj macierz pamięci \(Ma\).

\item {} 
\sphinxAtStartPar
Wejście to symbol w postaci 2\sphinxhyphen{}wymiarowej tablicy pikseli o wartościach 0 lub 1, gdzie
pewna liczba pikseli jest losowo przekłamana.

\item {} 
\sphinxAtStartPar
Spłaszcz symbol do wektora, który tworzy warstwę danych wejściowych \(x_i\).

\item {} 
\sphinxAtStartPar
Macierz wag w pełni połączonej sieci ANN to \(Ma\).

\item {} 
\sphinxAtStartPar
Sygnał wchodzący do neuronu \(j\) w warstwie wyjściowej to \(s_j=\sum_i x_i M_{ij}\).

\item {} 
\sphinxAtStartPar
Funkcja aktywacji to funkcja schodkowa z odpowiednio dobranym progiem. Daje ona \(y_j=f(s_j)\).

\item {} 
\sphinxAtStartPar
Potnij wektor wyjściowy na macierz pikseli, która stanowi ostateczny wynik. Powinien zostać przywrócony oryginalny symbol.

\end{enumerate}
\end{sphinxadmonition}

\begin{sphinxadmonition}{important}{Ważne:}
\sphinxAtStartPar
Konkluzja: ANN z jedną warstwą neuronów MPC mogą służyć jako bardzo proste modele pamięci!
\end{sphinxadmonition}

\sphinxAtStartPar
Zauważmy jednak, że skonstruowaliśmy macierze pamięciowe algebraicznie, niejako zewnętrznie. Dlatego sieć tak naprawdę nie nauczyła się skojarzeń z doświadczenia. Są na to sposoby, ale wymagają one bardziej zaawansowanych metod (patrz np. {[}\hyperlink{cite.docs/conclusion:id11}{FS91}{]}), podobne do omówionych w kolejnych częściach tego wykładu.

\begin{sphinxadmonition}{note}{Informacja:}
\sphinxAtStartPar
Implementacja omawianych modeli pamięci w programie Mathematica znajduje się w
{[}\hyperlink{cite.docs/conclusion:id10}{Fre93}{]} (\sphinxurl{https://library.wolfram.com/infocenter/Books/3485}) oraz we wspomnianych już wykładach \sphinxhref{http://vision.psych.umn.edu/users/kersten/kersten-lab/courses/Psy5038WF2014/IntroNeuralSyllabus.html}{Daniela Kerstena}.
\end{sphinxadmonition}


\section{Ćwiczenia}
\label{\detokenize{docs/memory:cwiczenia}}
\begin{sphinxadmonition}{note}{\protect\(~\protect\)}

\sphinxAtStartPar
Pobaw się kodem z wykładu i
\begin{itemize}
\item {} 
\sphinxAtStartPar
dodawaj coraz więcej symboli;

\item {} 
\sphinxAtStartPar
zmieniaj poziom filtra;

\item {} 
\sphinxAtStartPar
zwiększ liczbę przekłamań.

\end{itemize}

\sphinxAtStartPar
Omów swoje spostrzeżenia i przedyskutuj ograniczenia modeli.
\end{sphinxadmonition}


\chapter{Perceptron}
\label{\detokenize{docs/perceptron:perceptron}}\label{\detokenize{docs/perceptron:perc-lab}}\label{\detokenize{docs/perceptron::doc}}

\section{Uczenie nadzorowane}
\label{\detokenize{docs/perceptron:uczenie-nadzorowane}}
\sphinxAtStartPar
W poprzednich rozdziałach pokazaliśmy, że nawet najprostsze sieci ANN mogą wykonywać przydatne zadania (emulować sieci logiczne lub dostarczać proste modele pamięci). Ogólnie rzecz biorąc, każdy ANN ma
\begin{itemize}
\item {} 
\sphinxAtStartPar
pewną \sphinxstylestrong{architekturę}, czyli liczbę warstw, liczbę neuronów w każdej warstwie, schemat połączeń między neuronami (w pełni połączone lub nie, feed\sphinxhyphen{}forward, rekurencyjne, …);

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{wagi (hiperparametry)} na połącznieach, z określonymi wartościami definiującymi funkcjonalność sieci.

\end{itemize}

\sphinxAtStartPar
Podstawowym pytaniem praktycznym jest to, jak ustawić (dla danej architektury) wagi tak, aby żądany cel funkcjonalności sieci został zrealizowany, tj. dla określonych danych wejściowych uzuskać pożądany wynik na wyjściu.
W zadaniach omówionych wcześniej wagi mogą być skonstruowane \sphinxstyleemphasis{a priori}, czy to dla bramek logicznych, czy dla modeli pamięci. Jednak dla bardziej skomplikowanych aplikacji chcemy mieć „łatwiejszy” sposób określania wag. Co więcej, dla skomplikowanych problemów „teoretyczne” określenie wag a priori nie jest w ogóle możliwe. To podstawowy powód, dla którego wymyślono \sphinxstylestrong{algorytmy uczenia się} sieci, które ,,automatycznie” dostosowują wagi na podstawie dostępnych danych.

\sphinxAtStartPar
W tym rozdziale rozpoczynamy badanie takich algorytmów, poczynając od podejścia \sphinxstylestrong{uczenia nadzorowanego}, stosowanego \sphinxhref{http://m.in}{m.in}. do klasyfikacji danych.

\begin{sphinxadmonition}{note}{Uczenie nadzorowane}

\sphinxAtStartPar
W tej strategii dane muszą posiadać \sphinxstylestrong{etykiety}, które a priori określają poprawną kategorię dla każdego punktu. Pomyślmy na przykład o zdjęciach zwierząt (dane lub cechy, ang. features) i ich opisach (kot, pies,…), które nazywane są etykietami (ang. labels).
Te etykietowane dane są następnie dzielone na próbkę \sphinxstylestrong{szkoleniową} i próbkę \sphinxstylestrong{testową}.

\sphinxAtStartPar
Podstawowe kroki uczenia nadzorowanego dla danej ANN są następujące:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Zainicjuj w jakiś sposób wagi, na przykład losowo lub na zero.

\item {} 
\sphinxAtStartPar
Odczytuj kolejno punkty danych z próbki szkoleniowej i przepuszczaj je przez swoją sieć ANN. Otrzymana odpowiedź może różnić się od prawidłowej, zawartej w etykiecie. W takim przypadku wagi są zmieniane zgodnie z konkretną receptą (o czym później).

\item {} 
\sphinxAtStartPar
W razie potrzeby powtórz poprzedni krok. Zazwyczaj wagi zmienia się coraz mniej w miarę postępu algorytmu.

\item {} 
\sphinxAtStartPar
Zakończ szkolenie sieci po osiągnięciu kryterium zatrzymania (wagi nie zmieniają się już znacznie lub została osiągnięta maksymalna liczba iteracji).

\item {} 
\sphinxAtStartPar
Przetestuj tak wyszkoloną ANN na próbce testowej.

\end{itemize}

\sphinxAtStartPar
Jeśli jesteśmy zadowoleni, mamy pożądaną wyszkoloną sieć ANN wykonującą określone zadanie (takie jak np. klasyfikacja danych), której można teraz używać na nowych, nieetykietowanych danych. Jeśli nie, możemy inaczej podzielić próbkę na część szkoleniową i testową, po czym powtórzyć procedurę uczenia od początku. Możemy także spróbować pozyskać więcej danych (co może być kosztowne), lub też zmienić architekturę sieci.

\sphinxAtStartPar
Termin „nadzorowany” pochodzi z interpretacji procedury, w której etykiety posiadane są przez „nauczyciela”, który w ten sposób wie, które odpowiedzi są prawidłowe, a które błędne i który \sphinxstylestrong{nadzoruje} w ten sposób proces szkolenia. Oczywiście program komputerowy ma wbudowanego nauczyciela. tj. „nadzoruje się” sam.
\end{sphinxadmonition}


\section{Perceptron jako klasyfikator binarny}
\label{\detokenize{docs/perceptron:perceptron-jako-klasyfikator-binarny}}
\sphinxAtStartPar
Najprostszy algorytm uczenia nadzorowanego
to \sphinxhref{https://en.wikipedia.org/wiki/Perceptron}{perceptron}, wymyślony w 1958 roku przez Franka Rosenblatta. Może służyć \sphinxhref{http://m.in}{m.in}. do
konstruowania \sphinxstylestrong{klasyfikatorów binarnych} danych. \sphinxstyleemphasis{Binarny} oznacza, że sieć
służy do oceny, czy element danych ma określoną cechę, czy nie \sphinxhyphen{} są tylko dwie możliwości. Klasyfikacja wieloetykietowa jest również możliwa w przypadku ANN (patrz ćwiczenia), ale nie omawiamy jej tutaj.

\begin{sphinxadmonition}{note}{Uwaga}

\sphinxAtStartPar
Termin \sphinxstyleemphasis{perceptron} jest również używany dla ANN (bez lub z warstwami pośrednimi) składających się z neuronów MCP (por. rys. \hyperref[\detokenize{docs/intro:ffnn-fig}]{Rys.\@ \ref{\detokenize{docs/intro:ffnn-fig}}} i \hyperref[\detokenize{docs/mcp:mcp1-fig}]{Rys.\@ \ref{\detokenize{docs/mcp:mcp1-fig}}}), na których wykonywany jest algorytm perceptronu.
\end{sphinxadmonition}


\subsection{Próbka ze znaną regułą klasyfikacji}
\label{\detokenize{docs/perceptron:probka-ze-znana-regula-klasyfikacji}}
\sphinxAtStartPar
Na początek potrzebujemy danych treningowych, które wygenerujemy jako losowe punkty w kwadracie. Zatem współrzędne punktu, \(x_1\) i \(x_2\), należą do przedziału \([0,1]\). Definiujemy dwie kategorie: jedną dla punktów leżących powyżej linii \(x_1=x_2\) (nazwijmy je różowymi) oraz drugą dla punktów leżących poniżej tej linii (niebieskie). Podczas losowego generowania danych sprawdzamy, czy \(x_2 > x_1\) czy nie i przypisujemy odpowiednią  \sphinxstylestrong{etykietę} do każdego punktu, równą odpowiednio 1 lub 0. Te etykiety są oczekiwanymi „prawdziwymi” odpowiedziami sieci po jej wyszkoleniu.

\sphinxAtStartPar
Funkcja generująca opisany powyżej punkt danych z etykietą to

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{point}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}     \PYG{c+c1}{\PYGZsh{} generates random coordinates x1, x2, and 1 if x2\PYGZgt{}x1, 0 otherwise}
    \PYG{n}{x1}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)}          \PYG{c+c1}{\PYGZsh{} random number from the range [0,1]}
    \PYG{n}{x2}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)}
    \PYG{k}{if}\PYG{p}{(}\PYG{n}{x2}\PYG{o}{\PYGZgt{}}\PYG{n}{x1}\PYG{p}{)}\PYG{p}{:}                     \PYG{c+c1}{\PYGZsh{} condition met}
        \PYG{k}{return} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{x1}\PYG{p}{,}\PYG{n}{x2}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} add label 1}
    \PYG{k}{else}\PYG{p}{:}                          \PYG{c+c1}{\PYGZsh{} not met}
        \PYG{k}{return} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{x1}\PYG{p}{,}\PYG{n}{x2}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} add label 0}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
Generujemy \sphinxstylestrong{próbkę szkoleniową}, składającą się z \sphinxstylestrong{npo}=300 etykietowanych punktów danych:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{npo}\PYG{o}{=}\PYG{l+m+mi}{300} \PYG{c+c1}{\PYGZsh{} number of data points in the training sample}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{  x1         x2         label}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}       \PYG{c+c1}{\PYGZsh{} header}
\PYG{n}{samp}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{point}\PYG{p}{(}\PYG{p}{)} \PYG{k}{for} \PYG{n}{\PYGZus{}} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{npo}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} training sample, \PYGZus{} is dummy iterator}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{samp}\PYG{p}{[}\PYG{p}{:}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{p}{:}\PYG{p}{]}\PYG{p}{)}                           \PYG{c+c1}{\PYGZsh{} first 5 data points}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
  x1         x2         label
[[0.32934056 0.39891895 1.        ]
 [0.87245402 0.23216823 0.        ]
 [0.34298363 0.15572279 0.        ]
 [0.85658971 0.99224611 1.        ]
 [0.78826375 0.03085404 0.        ]]
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\begin{sphinxadmonition}{note}{Pętle w tablicy}

\sphinxAtStartPar
W Pythonie można wygodnie zdefiniować tablicę poprzez pętlę, np.
{[}i**2 for i in range(4){]} daje {[}1,4,9{]}.

\sphinxAtStartPar
W pętlach, jeśli wskaźnik nie występuje jawnie w wyrażeniu, można użyć symbolu \sphinxstylestrong{\_} , np.

\sphinxAtStartPar
{[}point() for \_ in range(npo){]}
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{Zakresy w tablicach}

\sphinxAtStartPar
Aby nie drukować niepotrzebnie bardzo długiej tabeli, po raz pierwszy użyliśmy powyżej \sphinxstylestrong{zakresów dla wskaźników tablic}. Np. 2:5 oznacza od 2 do 4 (przypomnijmy, że ostatni jest wykluczony!), :5 \sphinxhyphen{} od 0 do 4, 5: \sphinxhyphen{} od 5 do końca, wreszcie : \sphinxhyphen{} wszystkie elementy.
\end{sphinxadmonition}

\sphinxAtStartPar
Nasze wygenerowane dane przedstawia graficznie poniższy rysunek. Wykreślamy również linię \(x_2=x_1\), która oddziela niebieskie i różowe punkty. W tym przypadku podział jest możliwy a priori (znamy regułę) w sposób dokładny.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mf}{2.3}\PYG{p}{,}\PYG{l+m+mf}{2.3}\PYG{p}{)}\PYG{p}{,}\PYG{n}{dpi}\PYG{o}{=}\PYG{l+m+mi}{120}\PYG{p}{)}                 
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlim}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{.1}\PYG{p}{,}\PYG{l+m+mf}{1.1}\PYG{p}{)}                                  \PYG{c+c1}{\PYGZsh{} axes limits}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylim}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{.1}\PYG{p}{,}\PYG{l+m+mf}{1.1}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{samp}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}\PYG{n}{samp}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{n}{samp}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,}       \PYG{c+c1}{\PYGZsh{} label determines the color}
            \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{5}\PYG{p}{,}\PYG{n}{cmap}\PYG{o}{=}\PYG{n}{mpl}\PYG{o}{.}\PYG{n}{cm}\PYG{o}{.}\PYG{n}{cool}\PYG{p}{)}                  \PYG{c+c1}{\PYGZsh{} point size and color}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.1}\PYG{p}{,} \PYG{l+m+mf}{1.1}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.1}\PYG{p}{,} \PYG{l+m+mf}{1.1}\PYG{p}{]}\PYG{p}{)}                 \PYG{c+c1}{\PYGZsh{} separating line}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZdl{}x\PYGZus{}1\PYGZdl{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}                    
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZdl{}x\PYGZus{}2\PYGZdl{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{perceptron_17_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\begin{sphinxadmonition}{note}{Zbiory liniowo rozłączne}

\sphinxAtStartPar
Dwa zbiory punktów (tutaj niebieski i różowy) na płaszczyźnie, które można rozdzielić linią prostą, nazywamy \sphinxstylestrong{liniowo rozłącznymi} (separowalnymi). W trzech wymiarach zbiory muszą być separowalne płaszczyzną, ogólnie w \(n\) wymiarach  zbiory muszą być separowalne za pomocą  \(n-1\) wymiarowej hiperpłaszczyzny.
\end{sphinxadmonition}

\sphinxAtStartPar
Analitycznie, jeżeli punkty w przestrzeni \(n\) wymiarowej  mają współrzędne \((x_1,x_2,\dots,x_n)\), to można dobrać parametry \((w_0,w_1,\dots,w_n)\) w taki sposób, aby zbiór jeden punktów spełniał warunek
\begin{equation}\label{equation:docs/perceptron:eq-linsep}
\begin{split}w_0+x_1 w_1+x_2 w_2 + \dots x_n w_n > 0\end{split}
\end{equation}
\sphinxAtStartPar
a drugi warunek przeciwny, ze znakiem \(>\) zastąpionym przez \(\le\).

\sphinxAtStartPar
A teraz kluczowa, choć oczywista obserwacja: powyższa nierówność jest dokładnie warunkiem zaimplementowanym w {[}neuronie MCP{]}(laboratorium MCP) (ze schodkową funkcją aktywacji) w konwencji \hyperref[\detokenize{docs/mcp:mcp2-fig}]{Rys.\@ \ref{\detokenize{docs/mcp:mcp2-fig}}}! Możemy więc zrealizować warunek \eqref{equation:docs/perceptron:eq-linsep} za pomocą funkcji \sphinxstylestrong{neuron} z biblioteki \sphinxstylestrong{neural}.

\sphinxAtStartPar
W naszym przykładzie dla różowych punktów, według konstrukcji,
\begin{equation*}
\begin{split}
x_2>x_1 \to s=-x_1+x_2 >0
\end{split}
\end{equation*}
\sphinxAtStartPar
skąd, używając równ. \eqref{equation:docs/perceptron:eq-linsep}, możemy od razu odczytać
\begin{equation*}
\begin{split}
w_0=0, \;\; w_1=-1, w_2=1.
\end{split}
\end{equation*}
\sphinxAtStartPar
Zatem funkcja \sphinxstylestrong{neuron} dla punktu próbki p jest używana w następujący sposób:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{p}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mf}{0.6}\PYG{p}{,}\PYG{l+m+mf}{0.8}\PYG{p}{]}      \PYG{c+c1}{\PYGZsh{} sample point with x\PYGZus{}2 \PYGZgt{} x\PYGZus{}1}
\PYG{n}{w}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}       \PYG{c+c1}{\PYGZsh{} weights as given above}

\PYG{n}{func}\PYG{o}{.}\PYG{n}{neuron}\PYG{p}{(}\PYG{n}{p}\PYG{p}{,}\PYG{n}{w}\PYG{p}{)} 
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
1
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Neuron, odpalił, więc punkt p jest różowy.

\begin{sphinxadmonition}{note}{Wniosek}

\sphinxAtStartPar
Pojedynczy neuron MCP z odpowiednio dobranymi wagami może być użyty jako klasyfikator binarny dla \(n\)\sphinxhyphen{}wymiarowych danych separowalnych.
\end{sphinxadmonition}


\subsection{Próbka o nieznanej regule klasyfikacji}
\label{\detokenize{docs/perceptron:probka-o-nieznanej-regule-klasyfikacji}}
\sphinxAtStartPar
W tym miejscu czytelnik może być nieco zwiedziony pozorną błahością wyników. Wątpliwości mogą wynikać z tego, że w powyższym przykładzie od początku znaliśmy regułę określającą dwie klasy punktów (\(x_2>x_1\), lub odwrotnie). Jednak w ogólnej sytuacji „z prawdziwego życia” zwykle tak nie jest! Wyobraź sobie, że napotykamy (etykietowane) dane \sphinxstylestrong{samp2} wyglądające tak:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{samp2}\PYG{p}{[}\PYG{p}{:}\PYG{l+m+mi}{5}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
[[0.50896192 0.26237741 0.        ]
 [0.50775256 0.1093865  0.        ]
 [0.44707124 0.04838339 0.        ]
 [0.26519082 0.33358304 0.        ]
 [0.5661581  0.53616119 0.        ]]
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{perceptron_28_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
Sytuacja jest teraz w pewnym sensie odwrócona. Uzyskaliśmy skądś (liniowo separowalne) dane i chcemy znaleźć regułę, która definiuje te dwie klasy. Innymi słowy, musimy narysować linię podziału, która jest równoważna ze znalezieniem wag neuronu MCP \hyperref[\detokenize{docs/mcp:mcp2-fig}]{Rys.\@ \ref{\detokenize{docs/mcp:mcp2-fig}}}, który przeprowadziłby odpowiednią klasyfikację binarną.


\section{Algorytm perceptronu}
\label{\detokenize{docs/perceptron:algorytm-perceptronu}}\label{\detokenize{docs/perceptron:lab-pa}}
\sphinxAtStartPar
Moglibyśmy spróbować jakoś obliczyć właściwe wagi dla powyższego przykładu i znaleźć linię podziału, na przykład linijką i ołówkiem, ale nie o to tutaj chodzi. Chcemy mieć systematyczną procedurę algorytmiczną, która bez trudu zadziała w tej czy każdej podobnej sytuacji. Odpowiedzią jest wspomniany już \sphinxhref{https://en.wikipedia.org/wiki/Perceptron}{algorytm perceptronu}.

\sphinxAtStartPar
Przed przedstawieniem algorytmu zauważmy, że neuron MCP z pewnym zbiorem wag \(w_0, w_1, w_2\) zawsze daje jakąś odpowiedź dla etylirtowanego punktu danych, poprawną lub błędną. Na przykład

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{w}\PYG{o}{=}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.5}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}           \PYG{c+c1}{\PYGZsh{} arbitrary choice of weights}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{label  answer}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} header}

\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} look at first 5 points}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{n+nb}{int}\PYG{p}{(}\PYG{n}{samp2}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{    }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{n}{func}\PYG{o}{.}\PYG{n}{neuron}\PYG{p}{(}\PYG{n}{samp2}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{p}{:}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,}\PYG{n}{w}\PYG{p}{)}\PYG{p}{)} 
            \PYG{c+c1}{\PYGZsh{} samp2[i,2] is the label, samp2[i,:2] is [x\PYGZus{}1,x\PYGZus{}2]}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
label  answer
0      1
0      1
0      0
0      0
0      1
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Widzimy, że niektóre odpowiedzi są równe etykietom (poprawne), a inne są od nich różne (błędne). Ogólną ideą jest teraz \sphinxstylestrong{użycie błędnych odpowiedzi}, aby sprytnie, małymi krokami zmieniać wagi, tak aby po wystarczającej liczbie iteracji wszystkie odpowiedzi dla danej próbki szkoleniowej były poprawne!

\begin{sphinxadmonition}{note}{Algorytm perceptronu}

\sphinxAtStartPar
Iterujemy po punktach próbki danych szkoleniowych.
Jeżeli dla danego punktu otrzymany wynik \(y_o\) jest równy prawdziwej wartości \(y_t\) (etykieta), tj. odpowiedź jest prawidłowa, nic nie robimy. Jeśli jednak jest błędna, zmieniamy nieco wagi, tak aby szansa na otrzymanie błędnej odpowiedzi spadła. Przepis jest następujący:

\sphinxAtStartPar
\(w_i \to w_i  +  \varepsilon  (y_t - y_o)  x_i\),

\sphinxAtStartPar
gdzie \( \varepsilon \) to mała liczba (nazywana \sphinxstylestrong{szybkością uczenia}), a \(x_i\) to współrzędne punktu wejściowego, gdzie \(i=0,\dots,n\).
\end{sphinxadmonition}

\sphinxAtStartPar
Prześledźmy, jak to działa. Załóżmy najpierw, że \(x_i> 0\). Wtedy jeśli etykieta \( y_t = 1 \) jest większa niż uzyskana odpowiedź \( y_o = 0 \), waga \(w_i\) jest zwiększana. Wtedy \(w \cdot x\) również wzrasta, a \( y_o = f (w \cdot x) \) z większą szansą przyjmie poprawną wartość 1 (pamiętamy, jak wygląda funkcja schodkowa \(f\)). Jeżeli natomiast etykieta \(y_t = 0 \) jest mniejsza niż uzyskana odpowiedź \( y_o = 1 \), to waga \(w_i\) maleje, \( w \cdot x \) maleje, a \( y_o = f(w \cdot x) \) ma większą szansę na osiągnięcie prawidłowej wartości 0.

\sphinxAtStartPar
Jeśli \( x_i < 0 \), łatwo analogicznie sprawdzić, że przepis również działa poprawnie.

\sphinxAtStartPar
Jeśli odpowiedź jest prawidłowa, \(y_t=y_0\), to \( w_i \to w_i\), więc nic się nie zmienia. Nie „psujemy” perceptronu!

\sphinxAtStartPar
Powyższy wzór można zastosować wielokrotnie dla tego samego punktu z próbki szkoleniowej. Następnie wykonujemy pętlę po wszystkich punktach próbki, a całą procedurę można jeszcze powtarzać w wielu rundach, aby uzyskać stabilne wagi (nie zmieniające się już w miarę kontynuacji procedury lub zmieniające się tylko nieznacznie).

\sphinxAtStartPar
Zazwyczaj w takich algorytmach szybkość uczenia \( \varepsilon \) jest zmniejszana w kolejnych rundach. Jest to bardzo ważne z praktycznego punktu widzenia, ponieważ zbyt duże aktualizacje mogą zepsuć uzyskane rozwiązanie.

\sphinxAtStartPar
Implementacja algorytmu perceptronu dla danych dwuwymiarowych w Pythonie wygląda następująco:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{w0}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.5}  \PYG{c+c1}{\PYGZsh{} initialize weights randomly in the range [\PYGZhy{}0.5,0.5]}
\PYG{n}{w1}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.5}
\PYG{n}{w2}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.5}

\PYG{n}{eps}\PYG{o}{=}\PYG{l+m+mf}{.3}                     \PYG{c+c1}{\PYGZsh{} initial  learning speed }
   
\PYG{k}{for} \PYG{n}{\PYGZus{}} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{20}\PYG{p}{)}\PYG{p}{:}        \PYG{c+c1}{\PYGZsh{} loop over rounds}
    \PYG{n}{eps}\PYG{o}{=}\PYG{l+m+mf}{0.9}\PYG{o}{*}\PYG{n}{eps}            \PYG{c+c1}{\PYGZsh{} in each round decrease the learning speed }
        
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{npo}\PYG{p}{)}\PYG{p}{:}   \PYG{c+c1}{\PYGZsh{} loop over the points from the data sample}
        
        \PYG{k}{for} \PYG{n}{\PYGZus{}} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} repeat 5 times for each points}
            
            \PYG{n}{yo} \PYG{o}{=} \PYG{n}{func}\PYG{o}{.}\PYG{n}{neuron}\PYG{p}{(}\PYG{n}{samp2}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{p}{:}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{n}{w0}\PYG{p}{,}\PYG{n}{w1}\PYG{p}{,}\PYG{n}{w2}\PYG{p}{]}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} obtained answer}
            
            \PYG{n}{w0}\PYG{o}{=}\PYG{n}{w0}\PYG{o}{+}\PYG{n}{eps}\PYG{o}{*}\PYG{p}{(}\PYG{n}{samp2}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{n}{yo}\PYG{p}{)}   \PYG{c+c1}{\PYGZsh{} weight update (the perceptron formula)}
            \PYG{n}{w1}\PYG{o}{=}\PYG{n}{w1}\PYG{o}{+}\PYG{n}{eps}\PYG{o}{*}\PYG{p}{(}\PYG{n}{samp2}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{n}{yo}\PYG{p}{)}\PYG{o}{*}\PYG{n}{samp2}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}
            \PYG{n}{w2}\PYG{o}{=}\PYG{n}{w2}\PYG{o}{+}\PYG{n}{eps}\PYG{o}{*}\PYG{p}{(}\PYG{n}{samp2}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{n}{yo}\PYG{p}{)}\PYG{o}{*}\PYG{n}{samp2}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Obtained weights:}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{  w0     w1     w2}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}        \PYG{c+c1}{\PYGZsh{} header }
\PYG{n}{w\PYGZus{}o}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{w0}\PYG{p}{,}\PYG{n}{w1}\PYG{p}{,}\PYG{n}{w2}\PYG{p}{]}\PYG{p}{)}           \PYG{c+c1}{\PYGZsh{} obtained weights}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(}\PYG{n}{w\PYGZus{}o}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{)}             \PYG{c+c1}{\PYGZsh{} result, rounded to 3 decimal places }
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
Obtained weights:
  w0     w1     w2
[\PYGZhy{}0.562 \PYGZhy{}1.114  2.192]
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Otrzymane wagi, jak wiemy, definiują linię podziału. Tak więc, geometrycznie, algorytm tworzy linię podziału, narysowaną poniżej wraz z próbką szkoleniową.

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{perceptron_39_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
Widzimy, że algorytm działa! Wszystkie różowe punkty znajdują się powyżej linii podziału, a wszystkie niebieskie poniżej. Podkreślmy, że linia podziału dana przez równanie
\begin{equation*}
\begin{split} w_0+x_1 w_1 + x_2 w_2=0,\end{split}
\end{equation*}
\sphinxAtStartPar
nie wynika z naszej wiedzy a priori, ale z treningu (uczenia nadzowowanego) neuronu MCP, który odpowiednio dopasowuje swoje wagi.

\begin{sphinxadmonition}{attention}{Uwaga:}
\sphinxAtStartPar
Można udowodnić, że algorytm perceptronu jest zbieżny wtedy i tylko wtedy, gdy dane są liniowo separowalne.
\end{sphinxadmonition}

\sphinxAtStartPar
Teraz możemy wyjawić nasz sekret! Dane próbki szkoleniowej \sphinxstylestrong{samp2} zostały etykietowane w momencie tworzenia regułą
\begin{equation*}
\begin{split} x_2> 0,25+0,52 x_1, \end{split}
\end{equation*}
\sphinxAtStartPar
co odpowiada wagom \(w_0=0.25\), \(w_1=-0.52\), \(w_2=1\).

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{w\PYGZus{}c}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.25}\PYG{p}{,}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.52}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} weights used for labeling the training sample}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{w\PYGZus{}c}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
[\PYGZhy{}0.25 \PYGZhy{}0.52  1.  ]
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Zwróćmy uwagę, że nie są to wcale te same wagi, jakie uzyskano podczas treningu:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(}\PYG{n}{w\PYGZus{}o}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
[\PYGZhy{}0.562 \PYGZhy{}1.114  2.192]
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Powód jest dwojaki. Po pierwsze, zauważmy, że warunek nierówności \eqref{equation:docs/perceptron:eq-linsep} pozostaje niezmieniony, jeśli pomnożymy obie stronynierówności  przez \sphinxstylestrong{dodatnią} stałą \(c\). Możemy zatem przeskalować wszystkie wagi przez \(c\), a sytuacja (odpowiedzi neuronu MCP, linia podziału) pozostaje dokładnie taka sama (napotykamy tutaj \sphinxstylestrong{klasę równoważności} wag przeskalowanych o czynnik dodatni) .

\sphinxAtStartPar
Z tego powodu dzieląc uzyskane wagi przez wagi użyte do oznaczenia próbki, otrzymujemy (prawie) stałe wartości:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(}\PYG{n}{w\PYGZus{}o}\PYG{o}{/}\PYG{n}{w\PYGZus{}c}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
[2.249 2.143 2.192]
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Powodem, dla którego wartości stosunków wag dla \(i=0,1,2\) nie są dokładnie takie same, jest to, że próbka ma skończoną liczbę punktów (tutaj 300). W ten sposób zawsze istnieje pewna luka między dwiema klasami punktów i jest trochę miejsca na nieznaczne przesuwanie linii rozdzielającej. Przy większej liczbie punktów danych efekt różnicy stosunków wag zmniejsza się (patrz ćwiczenia).


\subsection{Testowanie klasyfikatora}
\label{\detokenize{docs/perceptron:testowanie-klasyfikatora}}
\sphinxAtStartPar
Ze względu na ograniczoną wielkość próbki szkoleniowej i opisany powyżej efekt „luki”, wynik klasyfikacji na próbce testowej jest czasami błędny. Dotyczy to zawsze punktów w pobliżu linii podziału, która jest wyznaczana z dokładnością zależną od krotności próbki szkoleniowej. Poniższy kod przeprowadza sprawdzenie na próbce testowej. Próbka ta składa się z etykietowanych danych wygenerowanych losowo za pomocą tej samej funkcji \sphinxstylestrong{point2} użytej uprzednio do wygenerowania danych szkoleniowych:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{point2}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{x1}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)}          \PYG{c+c1}{\PYGZsh{} random number from the range [0,1]}
    \PYG{n}{x2}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)}
    \PYG{k}{if}\PYG{p}{(}\PYG{n}{x2}\PYG{o}{\PYGZgt{}}\PYG{n}{x1}\PYG{o}{*}\PYG{l+m+mf}{0.52}\PYG{o}{+}\PYG{l+m+mf}{0.25}\PYG{p}{)}\PYG{p}{:}           \PYG{c+c1}{\PYGZsh{} condition met}
        \PYG{k}{return} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{x1}\PYG{p}{,}\PYG{n}{x2}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} add label 1}
    \PYG{k}{else}\PYG{p}{:}                          \PYG{c+c1}{\PYGZsh{} not met}
        \PYG{k}{return} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{x1}\PYG{p}{,}\PYG{n}{x2}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} add label 0}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
Kod testujący jest następujący:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{er}\PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{empty}\PYG{p}{(}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} initialize an empty 1 x 3 array to store misclassified points}

\PYG{n}{ner}\PYG{o}{=}\PYG{l+m+mi}{0}                 \PYG{c+c1}{\PYGZsh{} initial number of misclassified points}
\PYG{n}{nt}\PYG{o}{=}\PYG{l+m+mi}{10000}               \PYG{c+c1}{\PYGZsh{} number of test points}

\PYG{k}{for} \PYG{n}{\PYGZus{}} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{nt}\PYG{p}{)}\PYG{p}{:}   \PYG{c+c1}{\PYGZsh{} loop over the test points}
    \PYG{n}{ps}\PYG{o}{=}\PYG{n}{point2}\PYG{p}{(}\PYG{p}{)}       \PYG{c+c1}{\PYGZsh{} a test point }
    \PYG{k}{if}\PYG{p}{(}\PYG{n}{func}\PYG{o}{.}\PYG{n}{neuron}\PYG{p}{(}\PYG{n}{ps}\PYG{p}{[}\PYG{p}{:}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{n}{w0}\PYG{p}{,}\PYG{n}{w1}\PYG{p}{,}\PYG{n}{w2}\PYG{p}{]}\PYG{p}{)}\PYG{o}{!=}\PYG{n}{ps}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{)}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} if wrong answer                                      }
        \PYG{n}{er}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{er}\PYG{p}{,}\PYG{p}{[}\PYG{n}{ps}\PYG{p}{]}\PYG{p}{,}\PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}           \PYG{c+c1}{\PYGZsh{} add the point to er}
        \PYG{n}{ner}\PYG{o}{+}\PYG{o}{=}\PYG{l+m+mi}{1}                                 \PYG{c+c1}{\PYGZsh{} count the number of errors}
        
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{number of misclassified points = }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{n}{ner}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ per }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{n}{nt}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ (}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(}\PYG{n}{ner}\PYG{o}{/}\PYG{n}{nt}\PYG{o}{*}\PYG{l+m+mi}{100}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZpc{}}\PYG{l+s+s2}{ )}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}        
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
number of misclassified points =  20  per  10000  ( 0.2 \PYGZpc{} )
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Jak widać, niewielka liczba punktów testowych jest błędnie sklasyfikowana. Wszystkie te punkty leżą w pobliżu linii rozdzielającej.

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{perceptron_54_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\begin{sphinxadmonition}{note}{Błędna klasyfikacja}

\sphinxAtStartPar
Przyczyną błędnej klasyfikacji jest fakt, że próbka szkoleniowa nie wyznacza dokładnie linii rozdzielającej, ponieważ między punktami występuje pewna luka. Aby uzyskać lepszy wynik, punkty treningowe musiałyby być „gęstsze” w sąsiedztwie linii rozdzielającej lub też próbka treningowa musiałaby być większa.
\end{sphinxadmonition}


\section{Ćwiczenia}
\label{\detokenize{docs/perceptron:cwiczenia}}
\begin{sphinxadmonition}{note}{\protect\(~\protect\)}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Pobaw się kodem z wykładu i zobacz, jak procent błędnie zaklasyfikowanych punktów zmniejsza się wraz ze wzrostem wielkości próbki szkoleniowej.

\item {} 
\sphinxAtStartPar
Gdy algorytm perceptronu jest zbieżny, w pewnym momencie wagi przestają się zmieniać. Popraw kod wykładu, wdrażając zatrzymywanie, gdy wagi nie zmieniają się więcej niż pewna wartość podczas przechodzenia do następnej rundy.

\item {} 
\sphinxAtStartPar
Uogólnij powyższy klasyfikator na punkty w przestrzeni trójwymiarowej.

\end{itemize}
\end{sphinxadmonition}


\chapter{Więcej warstw}
\label{\detokenize{docs/more_layers:wiecej-warstw}}\label{\detokenize{docs/more_layers:more-lab}}\label{\detokenize{docs/more_layers::doc}}

\section{Dwie warstwy neuronów}
\label{\detokenize{docs/more_layers:dwie-warstwy-neuronow}}
\sphinxAtStartPar
W poprzednim rozdziale pokazaliśmy, że neuron MCP ze schodkową funkcją aktywacji odpowiada nierówności \(x \cdot w=w_0+x_1 w_1 + \dots x_n w_n > 0\), gdzie \(n\) jest wymiarem przestrzeni wejściowej. Pouczające jest dalsze prześledzenie tej geometrycznej interpretacji. Przyjmując dla prostoty \(n=2\) (płaszczyzna), powyższa nierówność odpowiada jej podziałowi na dwie półpłaszczyzny. Jak wiemy, prosta wyrażona jest równaniem
\begin{equation*}
\begin{split}w_0+x_1 w_1 + x_2 w_2 = 0\end{split}
\end{equation*}
\sphinxAtStartPar
i stanowi \sphinxstylestrong{linię podziału} na dwie półpłaszczyzny.

\sphinxAtStartPar
Wyobraźmy sobie teraz, że mamy więcej takich warunków: dwa, trzy itd., ogólnie \(k\) niezależnych warunków. Biorąc koniunkcję tych warunków, możemy zbudować wypukłe obszary, jak przykładowo pokazano na \hyperref[\detokenize{docs/more_layers:regions-fig}]{Rys.\@ \ref{\detokenize{docs/more_layers:regions-fig}}}.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=620\sphinxpxdimen]{{regions}.png}
\caption{Przykładowe obszary wypukłe na płaszczyźnie, od lewej do prawej: z jednym warunkiem nierówności, z koniunkcjami 2 warunków i z koniukcjami 3 lub 4 warunków nierówności, dającymi \sphinxstylestrong{wielokąty}.}\label{\detokenize{docs/more_layers:regions-fig}}\end{figure}

\begin{sphinxadmonition}{note}{Obszar wypukły}

\sphinxAtStartPar
Z definicji obszar \(A\) jest wypukły wtedy i tylko wtedy, gdy odcinek pomiędzy dowolnymi dwoma punktami w \(A\) jest zawarty w \(A\). Obszar, który nie jest wypukły nazywa się \sphinxstylestrong{wklęsłym}.
\end{sphinxadmonition}

\sphinxAtStartPar
Oczywiście, \(k\) warunków nierówności można narzucić za pomocą \(k\) neuronów MCP.
Przypomnijmy sobie z rozdz. {\hyperref[\detokenize{docs/mcp:bool-sec}]{\sphinxcrossref{\DUrole{std,std-ref}{Funkcje logiczne}}}}, że możemy w prosty sposób budować funkcje logiczne za pomocą sieci neuronowych. W szczególności możemy dokonać koniunkcji \(k\) warunków, biorąc neuron o wagach \(w_0=-1\) i \(1/k < w_i < 1/(k-1)\), gdzie \(i=1,\dots ,k\). Jedną z możliwości jest np.
\begin{equation*}
\begin{split}w_i=\frac{1}{k-\frac{1}{2}}.\end{split}
\end{equation*}
\sphinxAtStartPar
Rzeczywiście, niech \(p_0=1\), a warunki narzucone przez nierówności oznaczymy jako \(p_i\), \(i=1,\dots,k\), które mogą przyjmować wartości 1 lub 0 (prawda lub fałsz). Następnie
\begin{equation*}
\begin{split}p \cdot w =-1 + p_1 w_1 + \dots + p_k w_k = -1+\frac{p_1+\dots p_k}{k-\frac{1}{2}} > 0\end{split}
\end{equation*}
\sphinxAtStartPar
wtedy i tylko wtedy, gdy wszystkie \(p_i=1\), tj. wszystkie warunki są spełnione.

\sphinxAtStartPar
Architektury sieci dla warunków \(k=1\), 2, 3 lub 4 są ukazane na \hyperref[\detokenize{docs/more_layers:nfn-fig}]{Rys.\@ \ref{\detokenize{docs/more_layers:nfn-fig}}}. Idąc od lewej do prawej począwszy od drugiego panelu, mamy sieci z dwiema warstwami neuronów i z \(k\) neuronami w warstwie pośredniej, zapewniającymi warunki nierówności, oraz jednym neuronem w warstwie wyjściowej, pełniącym funkcję bramki AND. Oczywiście dla jednego warunku wystarczy mieć jeden neuron, jak pokazano na lewym panelu \hyperref[\detokenize{docs/more_layers:nfn-fig}]{Rys.\@ \ref{\detokenize{docs/more_layers:nfn-fig}}}.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=820\sphinxpxdimen]{{nf1-4}.png}
\caption{Sieci zdolne do klasyfikowania danych w obszarach z \hyperref[\detokenize{docs/more_layers:regions-fig}]{Rys.\@ \ref{\detokenize{docs/more_layers:regions-fig}}}.}\label{\detokenize{docs/more_layers:nfn-fig}}\end{figure}

\sphinxAtStartPar
W interpretacji geometrycznej pierwsza warstwa neuronowa reprezentuje \(k\) półpłaszczyzn, a neuron w drugiej warstwie odpowiada obszarowi wypukłemu o \(k\) bokach.

\sphinxAtStartPar
Sytuacja w oczywisty sposób uogólnia się na dane w większej liczbie wymiarów. W takim przypadku mamy więcej czarnych kropek dla danych wejściowych na \hyperref[\detokenize{docs/more_layers:nfn-fig}]{Rys.\@ \ref{\detokenize{docs/more_layers:nfn-fig}}}. Geometrycznie dla \(n=3\) mamy do czynienia z dzieleniem na półprzestrzenie z pomocą płaszczyzn i tworzenie wypukłych \sphinxhref{https://en.wikipedia.org/wiki/Wielo\%C5\%9Bciany}{wielościanów}, a dla \(n>3\) z dzieleniem hiperprzestzreni {[}hiperpłaszczyznami{]}(https:/ /en.wikipedia.org/wiki/Hyperplane) i tworzeniem wypukłych \sphinxhref{https://en.wikipedia.org/wiki/Polytope}{politopów}.

\begin{sphinxadmonition}{note}{Informacja:}
\sphinxAtStartPar
Jeśli w warstwie pośredniej znajduje się wiele neuronów, powstały wielokąt ma wiele boków, które mogą przybliżać gładką granicę, taką jak łuk. Aproksymacja jest coraz lepsza w miarę wzrostu \(k\).
\end{sphinxadmonition}

\begin{sphinxadmonition}{important}{Ważne:}
\sphinxAtStartPar
Percepton z dwiema warstwami neuronów (z wystarczającą liczbą neuronów w warstwie pośredniej) może klasyfikować punkty należące do obszaru wypukłego w przestrzeni \(n\)\sphinxhyphen{}wymiarowej.
\end{sphinxadmonition}


\section{Trzy lub więcej warstw neuronów}
\label{\detokenize{docs/more_layers:trzy-lub-wiecej-warstw-neuronow}}
\sphinxAtStartPar
Pokazaliśmy właśnie, że sieć dwuwarstwowa może klasyfikować wielokąt wypukły. Wyobraźmy sobie teraz, że tworzymy dwie takie figury w drugiej warstwie neuronów, na przykład dzieki następującej sieci:

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{more_layers_14_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
Zauważmy, że pierwsza i druga warstwa neuronów nie są tutaj w pełni połączone, ponieważ „układamy na sobie” dwie sieci tworzące trójkąty, jak w trzecim panelu \hyperref[\detokenize{docs/more_layers:nfn-fig}]{Rys.\@ \ref{\detokenize{docs/more_layers:nfn-fig}}}. Następnie w trzeciej warstwie neuronowej (tutaj posiadającej pojedynczy neuron) implementujemy bramkę \(p \,\wedge \!\sim\!q\), czyli koniunkcję warunków, że punkty należą do jednego trójkąta, a nie należą do drugiego. Jak zaraz pokażemy, przy odpowiednich wagach powyższa siatka może wytworzyć obszar wklęsły, na przykład trójkąt z trójkątnym wgłębieniem:

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=200\sphinxpxdimen]{{tritri}.png}
\caption{Trójkąt z trójkątnym wgłębieniem.}\label{\detokenize{docs/more_layers:tri-fig}}\end{figure}

\sphinxAtStartPar
Uogólniając ten argument na inne kształty, można pokazać ważne twierdzenie:

\begin{sphinxadmonition}{important}{Ważne:}
\sphinxAtStartPar
Perceptron z trzema lub więcej warstwami neuronów (z wystarczającą liczbą neuronów w warstwach pośrednich) może klasyfikować punkty należące do \sphinxstylestrong{dowolnego} regionu w \(n\)\sphinxhyphen{}wymiarowej przestrzeni z \(n-1\)\sphinxhyphen{}wymiarowymi ograniczeniami przez hiperpłaszczyzny.
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{Informacja:}
\sphinxAtStartPar
Warto tutaj podkreślić, że trzy warstwy zapewniają pełną funkcjonalność! Dodawanie kolejnych warstw do klasyfikatora nie zwiększa jego możliwości.
\end{sphinxadmonition}


\section{Feed forward w Pythonie}
\label{\detokenize{docs/more_layers:feed-forward-w-pythonie}}
\sphinxAtStartPar
Zanim przejdziemy do przykładu, potrzebujemy kodu w Pythonie do propagacji sygnału w przód w ogólnej, w pełni połączonej sieci. Będziemy reprezentować architekturę sieci z \(l\) warstwami neuronów jako tablicę postaci
\begin{equation*}
\begin{split}[n_0,n_1,n_2,...,n_l],\end{split}
\end{equation*}
\sphinxAtStartPar
gdzie \(n_0\) jest liczbą węzłów wejściowych, a \(n_i\) liczbą neuronów w warstwach \(i=1,\dots,l\). Na przykład architektura sieci z czwartego panelu \hyperref[\detokenize{docs/more_layers:nfn-fig}]{Rys.\@ \ref{\detokenize{docs/more_layers:nfn-fig}}} to

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{arch}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{4}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]} 
\PYG{n}{arch}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
[2, 4, 1]
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
W kodach tego kursu posługujemy się konwencją z \hyperref[\detokenize{docs/mcp:mcp2-fig}]{Rys.\@ \ref{\detokenize{docs/mcp:mcp2-fig}}}, a mianowicie próg jest traktowany jednolicie z pozostałym sygnałem. Jednak węzły progowe nie są uwzględniane w określaniu liczb \(n_i\) zdefiniowanych powyżej. W szczególności bardziej szczegółowy widok czwartego panelu \hyperref[\detokenize{docs/more_layers:nfn-fig}]{Rys.\@ \ref{\detokenize{docs/more_layers:nfn-fig}}} to

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{n}{draw}\PYG{o}{.}\PYG{n}{plot\PYGZus{}net}\PYG{p}{(}\PYG{n}{arch}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{more_layers_23_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Czarne kropki oznaczają dane wejściowe, szare kropki odpowiadają węzłom progowym, dającym input równy 1, a niebieskie kółka to neurony.

\sphinxAtStartPar
Następnie potrzebujemy wagi połączeń. Mamy \(l\) zestawów wag, z których każdy odpowiada krawędziom wchodzącym do danej warstwy neuronowej od lewej strony.
W powyższym przykładzie pierwsza warstwa neuronów (niebieskie kółka po lewej stronie) ma wagi, które tworzą macierz \(3 \times 4\). Tutaj 3 to liczba węzłów w poprzedniej (wejściowej) warstwie (łącznie z węzłem progowym), a 4 to liczba neuronów w pierwszej warstwie neuronowej. Podobnie wagi związane z drugą (wyjściową) warstwą neuronową tworzą macierz \(4 \times 1\). Stąd w naszej konwencji macierze wag odpowiadające kolejnym warstwom neuronów \(1, 2, \dots, l\) mają wymiary
\begin{equation*}
\begin{split}
(n_0+1)\times n_1, \; (n_1+1)\times n_2, \; \dots \; (n_{l-1}+1)\times n_l.
\end{split}
\end{equation*}
\sphinxAtStartPar
Tak więc, aby przechowywać wszystkie wagi sieci, tak naprawdę potrzebujemy \sphinxstylestrong{trzech} wskaźników: jeden dla warstwy, jeden dla liczby węzłów w poprzedniej warstwie i jeden dla liczby węzłów w danej warstwie. Moglibyśmy tutaj użyć trójwymiarowej tablicy, ale ponieważ numerujemy warstwy neuronów zaczynając od 1, a tablice zaczynają się od 0, nieco wygodniej jest użyć struktury \sphinxstylestrong{słownika} Pythona. Przechowujemy zatem wagi jako
\begin{equation*}
\begin{split}w=\{1: arr^1, 2: arr^2, ..., l: arr^l\},\end{split}
\end{equation*}
\sphinxAtStartPar
gdzie \(arr^i\) jest \sphinxstylestrong{dwuwymiarową} tablicą (macierzą) wag dla warstwy neuronowej \(i\). Dla przypadku z powyższego rysunku możemy wziąć na przykład

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{w}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{l+m+mi}{1}\PYG{p}{:}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{3}\PYG{p}{,}\PYG{l+m+mf}{0.2}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{3}\PYG{p}{,}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{3}\PYG{p}{,}\PYG{l+m+mi}{5}\PYG{p}{,}\PYG{l+m+mi}{7}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{:}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{l+m+mf}{0.2}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.5}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}\PYG{p}{\PYGZcb{}}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{w}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{w}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
[[ 1.   2.   1.   1. ]
 [ 2.  \PYGZhy{}3.   0.2  2. ]
 [\PYGZhy{}3.  \PYGZhy{}3.   5.   7. ]]

[[ 1. ]
 [ 0.2]
 [ 2. ]
 [ 2. ]
 [\PYGZhy{}0.5]]
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Dla sygnału rozchodzącego się w sieci stosujemy również odpowiednio słownik w postaci
\begin{equation*}
\begin{split}x=\{0: x^0, 1: x^1, 2: x^2, ..., l: x^l\},\end{split}
\end{equation*}
\sphinxAtStartPar
gdzie \(x^0\) to wejście, a \(x^i\) to sygnał wychodzący z warstwy neuronowej \(i\), dla \(i=1, \dots, l\). Wszystkie symbole \(x^j\), \(j=0, \dots, l\) są tablicami jednowymiarowymi. Uwzględniamy tu węzły progowe, stąd wymiary \(x^j\) wynoszą \(n_j+1\), z wyjątkiem warstwy wyjściowej, która nie ma węzła odchylenia, stąd \(x^l\) ma wymiar \(n_l\). Innymi słowy, wymiary tablic sygnału są równe całkowitej liczbie węzłów w każdej warstwie.

\sphinxAtStartPar
Następnie przedstawiamy szczegółowo odpowiednie wzory, ponieważ jest to kluczowe dla uniknięcia ewentualnych pomyłek związanych z zapisem.
Wiemy już z \eqref{equation:docs/mcp:eq-f0}, że dla pojedynczego neuronu z \(n\) wejściami, sygnał wchodzący jest obliczany jako
\begin{equation*}
\begin{split}s = x_0 w_0 + x_1 w_1 + x_2 w_2 + ... + x_n w_n = \sum_{\beta=0}^n x_\beta w_\beta .\end{split}
\end{equation*}
\sphinxAtStartPar
Przy większej liczbie warstw (oznaczonych wskażnikiem górnym \(i\)) i liczbie neuronów \(n_i\) w warstwie \(i\), notacja uogólnia się na
\begin{equation*}
\begin{split}
s^i_\alpha=\sum_{\beta=0}^{n_{i-1}} x^{i-1}_\beta w^i_{\beta \alpha}, \;\; \alpha=1\dots n_i, \;\; i=1,\dots,l.
\end{split}
\end{equation*}
\sphinxAtStartPar
Zauważmy, że sumowanie zaczyna się od \(\beta=0\), aby uwzględnić węzeł progowy w poprzedniej warstwie \((i-1)\), ale \(\alpha\) zaczyna się od 1, ponieważ tylko neurony (a nie węzeł progowy) w warstwie \(i\) odbierają sygnał (patrz rysunek poniżej).

\sphinxAtStartPar
W notacji macierzowej możemy też zapisać bardziej zwięźle
\(s^{iT} = x^{(i-1)T} W^i\), gdzie \(T\) oznacza transpozycję, tzn.
\begin{equation*}
\begin{split}
\begin{pmatrix} s^i_1 & s^i_2 & ...& s^i_{n_i} \end{pmatrix} = 
\begin{pmatrix} x^{i-1}_0 & x^{i-1}_1 & ...& x^{i-1}_{n_{i-1}} \end{pmatrix}
\begin{pmatrix} w^i_{01} & w^i_{02} & ...& w^i_{0,n_i} \\ w^i_{11} & w^i_{12} & ...& w^i_{1,n_i} \\ 
 ... & ... & ...& ... \\ w^i_{n_{i-1}1} & w^i_{n_{i-1}2} & ...& w^i_{n_{i-1}n_i} \end{pmatrix}.
\end{split}
\end{equation*}
\sphinxAtStartPar
Jak już dobrze wiemy, wyjście z neuronu uzyskuje się działając na jego sygnał wejściowy funkcją aktywacji. W ten sposób w końcu mamy
\begin{equation*}
\begin{split} 
x^i_\alpha  = f(s^i_\alpha) = f \left (\sum_{\beta=0}^{n_{i-1}} x^{i-1}_\beta w^i_{\beta \alpha} \right), \;\; \alpha=1\dots n_i, \;\; i=1,\dots,l , \\
x^i_0 =1, \;\; i=1,\dots,l-1,  
\end{split}
\end{equation*}
\sphinxAtStartPar
z węzłami progowymi równymi jeden. Poniższy rysunek ilustruje naszą notację.

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{more_layers_29_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
Implementacja propagacji feed\sphinxhyphen{}forward w Pythonie jest następująca:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{feed\PYGZus{}forward}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{,} \PYG{n}{we}\PYG{p}{,} \PYG{n}{x\PYGZus{}in}\PYG{p}{,} \PYG{n}{f}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{step}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    Feed\PYGZhy{}forward propagation}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    input: }
\PYG{l+s+sd}{    ar \PYGZhy{} array of numbers of nodes in subsequent layers [n\PYGZus{}0, n\PYGZus{}1,...,n\PYGZus{}l]}
\PYG{l+s+sd}{    (from input layer 0 to output layer l, bias nodes not counted)}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    we \PYGZhy{} dictionary of weights for neuron layers 1, 2,...,l in the format}
\PYG{l+s+sd}{    \PYGZob{}1: array[n\PYGZus{}0+1,n\PYGZus{}1],...,l: array[n\PYGZus{}(l\PYGZhy{}1)+1,n\PYGZus{}l]\PYGZcb{}}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    x\PYGZus{}in \PYGZhy{} input vector of length n\PYGZus{}0 (bias not included)}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    f \PYGZhy{} activation function (default: step)}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    return: }
\PYG{l+s+sd}{    x \PYGZhy{} dictionary of signals leaving subsequent layers in the format}
\PYG{l+s+sd}{    \PYGZob{}0: array[n\PYGZus{}0+1],...,l\PYGZhy{}1: array[n\PYGZus{}(l\PYGZhy{}1)+1], l: array[nl]\PYGZcb{}}
\PYG{l+s+sd}{    (the output layer carries no bias)}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}

    \PYG{n}{l}\PYG{o}{=}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{)}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}                   \PYG{c+c1}{\PYGZsh{} number of the neuron layers}
    \PYG{n}{x\PYGZus{}in}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{insert}\PYG{p}{(}\PYG{n}{x\PYGZus{}in}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{)}      \PYG{c+c1}{\PYGZsh{} input, with the bias node inserted}
    
    \PYG{n}{x}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{p}{\PYGZcb{}}                          \PYG{c+c1}{\PYGZsh{} empty dictionary x}
    \PYG{n}{x}\PYG{o}{.}\PYG{n}{update}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{l+m+mi}{0}\PYG{p}{:} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{n}{x\PYGZus{}in}\PYG{p}{)}\PYG{p}{\PYGZcb{}}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} add input signal to x}
    
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{n}{l}\PYG{p}{)}\PYG{p}{:}          \PYG{c+c1}{\PYGZsh{} loop over layers except the last one}
        \PYG{n}{s}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{[}\PYG{n}{i}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{we}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{)}    \PYG{c+c1}{\PYGZsh{} signal, matrix multiplication }
        \PYG{n}{y}\PYG{o}{=}\PYG{p}{[}\PYG{n}{f}\PYG{p}{(}\PYG{n}{s}\PYG{p}{[}\PYG{n}{k}\PYG{p}{]}\PYG{p}{)} \PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{arch}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{)}\PYG{p}{]} \PYG{c+c1}{\PYGZsh{} output from activation}
        \PYG{n}{x}\PYG{o}{.}\PYG{n}{update}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{n}{i}\PYG{p}{:} \PYG{n}{np}\PYG{o}{.}\PYG{n}{insert}\PYG{p}{(}\PYG{n}{y}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{\PYGZcb{}}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} add bias node and update x}

                                  \PYG{c+c1}{\PYGZsh{} the output layer l \PYGZhy{} no adding of the bias node}
        \PYG{n}{s}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{[}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{we}\PYG{p}{[}\PYG{n}{l}\PYG{p}{]}\PYG{p}{)}    \PYG{c+c1}{\PYGZsh{} signal   }
        \PYG{n}{y}\PYG{o}{=}\PYG{p}{[}\PYG{n}{f}\PYG{p}{(}\PYG{n}{s}\PYG{p}{[}\PYG{n}{q}\PYG{p}{]}\PYG{p}{)} \PYG{k}{for} \PYG{n}{q} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{arch}\PYG{p}{[}\PYG{n}{l}\PYG{p}{]}\PYG{p}{)}\PYG{p}{]} \PYG{c+c1}{\PYGZsh{} output}
        \PYG{n}{x}\PYG{o}{.}\PYG{n}{update}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{n}{l}\PYG{p}{:} \PYG{n}{y}\PYG{p}{\PYGZcb{}}\PYG{p}{)}          \PYG{c+c1}{\PYGZsh{} update x}
          
    \PYG{k}{return} \PYG{n}{x}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
Dla zwięzłości przyjmujemy konwencję, w której nie przekazujemy w argumentach funkcji węzła progowego. Jest on wstawiany do funkcji za pomocą \sphinxstylestrong{np.insert(x\_in,0,1)}. Jak zwykle używamy \sphinxstylestrong{np.dot} do mnożenia macierzy.

\sphinxAtStartPar
Następnie testujemy, jak \sphinxstylestrong{feed\_forward} działa dla przykładowych danych wejściowych.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{xi}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}
\PYG{n}{x}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{feed\PYGZus{}forward}\PYG{p}{(}\PYG{n}{arch}\PYG{p}{,}\PYG{n}{w}\PYG{p}{,}\PYG{n}{xi}\PYG{p}{,}\PYG{n}{func}\PYG{o}{.}\PYG{n}{step}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZob{}0: array([ 1,  2, \PYGZhy{}1]), 1: array([1, 1, 0, 0, 0]), 2: [1]\PYGZcb{}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Końcowy output z tej sieci jest uzyskany jako

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{x}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
1
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}

\subsection{Dygresja o sieciach liniowych}
\label{\detokenize{docs/more_layers:dygresja-o-sieciach-liniowych}}
\sphinxAtStartPar
Zróbmy teraz następującą obserwację. Załóżmy, że mamy sieć z liniową funkcją aktywacji \(f(s)=c s\). Wtedy ostatnia formuła z powyższego wyprowadzenia przyjmuje postać
\begin{equation*}
\begin{split}
x^i_\alpha = c \sum_{\beta=0}^{n_{i-1}} x^{i-1}_\beta w^i_{\beta \alpha}, \;\; \alpha=1\dots n_i, \;\; i=1,\dots,l ,
\end{split}
\end{equation*}
\sphinxAtStartPar
lub w notacji macierzowej:
\begin{equation*}
\begin{split}
x^i = c x^{i-1}w^i.
\end{split}
\end{equation*}
\sphinxAtStartPar
Powtarzając to, otrzymujemy sygnał w warstwie wyjściowej
\begin{equation*}
\begin{split}
x^l = cx^{l-1} w^i = c^2 x^{l-2} w^{l-1} w^l =\dots= c^lx^0 w^1 w^2 \dots w^l =
x^0 W,
\end{split}
\end{equation*}
\sphinxAtStartPar
gdzie \(W=c^l w^1 w^2 \dots w^l\). Widzimy wiec, że taka sieć jest \sphinxstylestrong{równoważna} sieci jednowarstwowej z macierzą wag \(W\) określoną powyżej.

\begin{sphinxadmonition}{note}{Informacja:}
\sphinxAtStartPar
Z tego powodu nie ma sensu rozważanie sieci wielowarstwowych z liniową funkcją aktywacji.
\end{sphinxadmonition}


\section{Wizualizacja}
\label{\detokenize{docs/more_layers:wizualizacja}}
\sphinxAtStartPar
W celu wizualizacji prostych sieci w module \sphinxstylestrong{draw} pakietu \sphinxstylestrong{neural} udostępniamy kilka funkcji rysowania, które ukazują zarówno wagi, jak i sygnały w sieci. Funkcja \sphinxstylestrong{plot\_net\_w} rysuje wagi dodatnie na czerwono, a ujemne na niebiesko, przy czym szerokości odzwierciedlają ich wielkość. Ostatni parametr, tutaj 0.5, przeskalowuje szerokości tak, że grafika wygląda ładnie. Funkcja \sphinxstylestrong{plot\_net\_w\_x} drukuje dodatkowo wartości sygnału wychodzącego z węzłów każdej warstwy.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{n}{draw}\PYG{o}{.}\PYG{n}{plot\PYGZus{}net\PYGZus{}w}\PYG{p}{(}\PYG{n}{arch}\PYG{p}{,}\PYG{n}{w}\PYG{p}{,}\PYG{l+m+mf}{0.5}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{more_layers_40_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{n}{draw}\PYG{o}{.}\PYG{n}{plot\PYGZus{}net\PYGZus{}w\PYGZus{}x}\PYG{p}{(}\PYG{n}{arch}\PYG{p}{,}\PYG{n}{w}\PYG{p}{,}\PYG{l+m+mf}{0.5}\PYG{p}{,}\PYG{n}{x}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{more_layers_41_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}

\section{Klasyfikator z trzema warstwami neuronów}
\label{\detokenize{docs/more_layers:klasyfikator-z-trzema-warstwami-neuronow}}
\sphinxAtStartPar
Jesteśmy teraz gotowi do jawnego skonstruowania przykładu binarnego klasyfikatora punktów w obszarze wklęsłym: trójkąta z trójkątnym wycięciem z \hyperref[\detokenize{docs/more_layers:tri-fig}]{Rys.\@ \ref{\detokenize{docs/more_layers:tri-fig}}}.
Architektura sieci to

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{arch}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{6}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{n}{draw}\PYG{o}{.}\PYG{n}{plot\PYGZus{}net}\PYG{p}{(}\PYG{n}{arch}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{more_layers_44_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Warunki geometryczne i odpowiadające im wagi dla pierwszej warstwy neuronowej to


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|T|}
\hline
\sphinxstyletheadfamily 
\sphinxAtStartPar
\(\alpha\)
&\sphinxstyletheadfamily 
\sphinxAtStartPar
warunek
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\(w_{0\alpha}^1\)
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\(w_{1\alpha}^1\)
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\(w_{2\alpha}^1\)
\\
\hline
\sphinxAtStartPar
1
&
\sphinxAtStartPar
\(x_1>0.1\)
&
\sphinxAtStartPar
\sphinxhyphen{}0.1
&
\sphinxAtStartPar
1
&
\sphinxAtStartPar
0
\\
\hline
\sphinxAtStartPar
2
&
\sphinxAtStartPar
\(x_2>0.1\)
&
\sphinxAtStartPar
\sphinxhyphen{}0.1
&
\sphinxAtStartPar
0
&
\sphinxAtStartPar
1
\\
\hline
\sphinxAtStartPar
3
&
\sphinxAtStartPar
\(x_1+x_2<1\)
&
\sphinxAtStartPar
1
&
\sphinxAtStartPar
\sphinxhyphen{}1
&
\sphinxAtStartPar
\sphinxhyphen{}1
\\
\hline
\sphinxAtStartPar
4
&
\sphinxAtStartPar
\(x_1>0.25\)
&
\sphinxAtStartPar
\sphinxhyphen{}0.25
&
\sphinxAtStartPar
1
&
\sphinxAtStartPar
0
\\
\hline
\sphinxAtStartPar
5
&
\sphinxAtStartPar
\(x_2>0.25\)
&
\sphinxAtStartPar
\sphinxhyphen{}0.25
&
\sphinxAtStartPar
0
&
\sphinxAtStartPar
1
\\
\hline
\sphinxAtStartPar
6
&
\sphinxAtStartPar
\(x_1+x_2<0.8\)
&
\sphinxAtStartPar
0.8
&
\sphinxAtStartPar
\sphinxhyphen{}1
&
\sphinxAtStartPar
\sphinxhyphen{}1
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
Warunki 1\sphinxhyphen{}3 zapewniają granice dla większego trójkąta, a 4\sphinxhyphen{}6 dla mniejszego, zawartego w większym.
W drugiej warstwie neuronowej musimy zrealizować dwie bramki AND odpowiednio dla warunków 1\sphinxhyphen{}3 i 4\sphinxhyphen{}6, a zatem bierzemy


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|T|T|T|T|}
\hline
\sphinxstyletheadfamily 
\sphinxAtStartPar
\(\alpha\)
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\(w_{0\alpha}^2\)
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\(w_{1\alpha}^2\)
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\(w_{2\alpha}^2\)
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\(w_{3\alpha}^2\)
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\(w_{4\alpha}^2\)
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\(w_{5\alpha}^2\)
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\(w_{6\alpha}^2\)
\\
\hline
\sphinxAtStartPar
1
&
\sphinxAtStartPar
\sphinxhyphen{}1
&
\sphinxAtStartPar
0.4
&
\sphinxAtStartPar
0.4
&
\sphinxAtStartPar
0.4
&
\sphinxAtStartPar
0
&
\sphinxAtStartPar
0
&
\sphinxAtStartPar
0
\\
\hline
\sphinxAtStartPar
2
&
\sphinxAtStartPar
\sphinxhyphen{}1
&
\sphinxAtStartPar
0
&
\sphinxAtStartPar
0
&
\sphinxAtStartPar
0
&
\sphinxAtStartPar
0.4
&
\sphinxAtStartPar
0.4
&
\sphinxAtStartPar
0.4
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
Na koniec w warstwie wyjściowej realizujemy bramkę \(p \wedge \! \sim\! q\), skąd


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|}
\hline
\sphinxstyletheadfamily 
\sphinxAtStartPar
\(\alpha\)
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\(w_{0\alpha}^3\)
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\(w_{1\alpha}^3\)
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\(w_{2\alpha}^3\)
\\
\hline
\sphinxAtStartPar
1
&
\sphinxAtStartPar
\sphinxhyphen{}1
&
\sphinxAtStartPar
1.2
&
\sphinxAtStartPar
\sphinxhyphen{}0.6
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
Łącząc to wszystko razem, uzyskujemy nastepujacy słownik wag:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{w}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{l+m+mi}{1}\PYG{p}{:}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.1}\PYG{p}{,}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.1}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.25}\PYG{p}{,}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.25}\PYG{p}{,}\PYG{l+m+mf}{0.8}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,}
   \PYG{l+m+mi}{2}\PYG{p}{:}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{l+m+mf}{0.4}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{l+m+mf}{0.4}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{l+m+mf}{0.4}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mf}{0.4}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mf}{0.4}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mf}{0.4}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,}
   \PYG{l+m+mi}{3}\PYG{p}{:}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{l+m+mf}{1.2}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.6}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
Feed\sphinxhyphen{}forward dla prykładowego inputu daje

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{xi}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mf}{0.2}\PYG{p}{,}\PYG{l+m+mf}{0.3}\PYG{p}{]}
\PYG{n}{x}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{feed\PYGZus{}forward}\PYG{p}{(}\PYG{n}{arch}\PYG{p}{,}\PYG{n}{w}\PYG{p}{,}\PYG{n}{xi}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{n}{draw}\PYG{o}{.}\PYG{n}{plot\PYGZus{}net\PYGZus{}w\PYGZus{}x}\PYG{p}{(}\PYG{n}{arch}\PYG{p}{,}\PYG{n}{w}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{n}{x}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{more_layers_48_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Właśnie odkryliśmy, że punkt {[}0.2,0.3{]} znajduje się w naszym obszarze (1 z warstwy wyjściowej). Właściwie mamy tutaj więcej informacji z warstw pośrednich. Z drugiej warstwy neuronowej widzimy, że punkt należy do większego trójkąta (1 z dolnego neuronu), a nie należy do mniejszego trójkąta (0 z górnego neuronu). Z pierwszej warstwy neuronowej możemy odczytać warunki z sześciu nierówności.

\sphinxAtStartPar
Następnie testujemy działanie naszej sieci dla innych punktów. Najpierw definiujemy funkcję generującą losowy punkt w kwadracie \([0,1]\times [0,1]\) i propagujemy go przez sieć. Przypisujemy mu etykietę 1, jeśli należy do żądanego obszaru, a 0 w przeciwnym razie. Następnie tworzymy dużą próbkę takich punktów i generujemy grafikę, używając koloru różowego dla etykiety 1 i niebieskiego dla etykiety 0.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{po}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{xi}\PYG{o}{=}\PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)}\PYG{p}{]} \PYG{c+c1}{\PYGZsh{} random point from the [0,1]x[0,1] square}
    \PYG{n}{x}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{feed\PYGZus{}forward}\PYG{p}{(}\PYG{n}{arch}\PYG{p}{,}\PYG{n}{w}\PYG{p}{,}\PYG{n}{xi}\PYG{p}{)}             \PYG{c+c1}{\PYGZsh{} feed forward}
    \PYG{k}{return} \PYG{p}{[}\PYG{n}{xi}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}\PYG{n}{xi}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{x}\PYG{p}{[}\PYG{l+m+mi}{3}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{]}               \PYG{c+c1}{\PYGZsh{} the point\PYGZsq{}s coordinates and label}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{samp}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{po}\PYG{p}{(}\PYG{p}{)} \PYG{k}{for} \PYG{n}{\PYGZus{}} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{10000}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{samp}\PYG{p}{[}\PYG{p}{:}\PYG{l+m+mi}{5}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
[[0.7088127  0.81098538 0.        ]
 [0.52044622 0.19265106 1.        ]
 [0.45637626 0.95670484 0.        ]
 [0.51723699 0.06066656 0.        ]
 [0.00879429 0.38239462 0.        ]]
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{more_layers_53_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
Widzimy, że nasza maszynka działa doskonale!

\sphinxAtStartPar
W tym miejscu czytelnik może słusznie powiedzieć, że powyższe wyniki są trywialne: w istocie właśnie zaimplementowaliśmy pewne warunki geometryczne i ich koniunkcje.

\sphinxAtStartPar
Jednak, podobnie jak w przypadku sieci jednowarstwowych, istnieje ważny argument przeciwko tej pozornej błahości. Wyobraźmy sobie ponownie, że mamy próbkę danych z etykietami i tylko tele, podobnie jak w przykładzie pojedynczego neuronu MCP z rozdziału {\hyperref[\detokenize{docs/mcp:mcp-lab}]{\sphinxcrossref{\DUrole{std,std-ref}{Neuron MCP}}}}. Wtedy na początku nie mamy warunków granicznych i potrzebujemy jakiegoś skutecznego sposobu, aby je znaleźć. Właśnie to zadanie wykonuje za nas \sphinxstylestrong{uczenie} klasyfikatorów: ustala wagi w taki sposób, że odpowiednie warunki są domyślnie wbudowane. Po materiale z tego rozdziału czytelnik powinien być przekonany, że jest to jak najbardziej możliwe i nie ma w tym nic magicznego! W następnym rozdziale pokażemy, jak to praktycznie zrobić.


\section{Ćwiczenia}
\label{\detokenize{docs/more_layers:cwiczenia}}
\begin{sphinxadmonition}{note}{\protect\(~\protect\)}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Zaprojektuj sieć i uruchom kod z tego wykładu dla wybranego regionu wypukłego.

\item {} 
\sphinxAtStartPar
Zaprojektuj i zaprogramuj klasyfikator dla czterech kategorii punktów należących do regionów utworzonych przez dwie przecinające się linie (wskazówka: uwzględnij wiecej komórek wyjściowych).

\end{itemize}
\end{sphinxadmonition}


\chapter{Propagacja wsteczna}
\label{\detokenize{docs/backprop:propagacja-wsteczna}}\label{\detokenize{docs/backprop::doc}}
\sphinxAtStartPar
W tym rozdziale pokażemy szczegółowo, jak przeprowadzić uczenie nadzorowane dla klasyfikatorów wielowarstwowych omówionych w rozdziale {\hyperref[\detokenize{docs/more_layers:more-lab}]{\sphinxcrossref{\DUrole{std,std-ref}{Więcej warstw}}}}. Ponieważ metoda opiera się na minimalizacji liczby błędnych odpowiedzi na próbce testowej, zaczynamy od dokładnego omówienia problemu minimalizacji błędów w naszej konfiguracji.


\section{Minimalizacja błędu}
\label{\detokenize{docs/backprop:minimalizacja-bledu}}
\sphinxAtStartPar
Przypomnijmy, że w naszym przykładzie z punktami na płaszczyźnie z rozdziału {\hyperref[\detokenize{docs/perceptron:perc-lab}]{\sphinxcrossref{\DUrole{std,std-ref}{Perceptron}}}} warunek dla różowych punktów był zadany przez nierówność

\sphinxAtStartPar
\(w_0+w_1 x_1 + w_2 x_2 > 0\).

\sphinxAtStartPar
Wspomnieliśmy już pokrótce o klasie równoważności związanej z dzieleniem obu stron tej nierówności przez dodatnią stałą \(c\). Ogólnie rzecz biorąc, co najmniej jedna z wag w powyższym warunku musi być niezerowa, aby był on nietrywialny. Załóżmy zatem, że \(w_0 \neq 0\) (inne przypadki można potraktować analogicznie). Następnie podzielmy obie strony nierówności przez \(|w_0|\), co daje
\begin{equation*}
\begin{split}\frac{w_0}{|w_0|}+\frac{w_1}{|w_0|} \, x_1 + \frac{w_2}{|w_0|} \, x_2 > 0. \end{split}
\end{equation*}
\sphinxAtStartPar
Wprowadzając notację \(v_1=\frac{w_1}{w_0}\) and \(v_2=\frac{w_2}{w_0}\), możemy zatem zapisać
\begin{equation*}
\begin{split}{\rm sgn}(w_0)( 1+v_1 \, x_1 +v_2 \, x_2) > 0,\end{split}
\end{equation*}
\sphinxAtStartPar
gdzie znak \({\rm sgn}(w_0) = \frac{w_0}{|w_0|}\). Mamy więc w efekcie system dwuparametrowy (dla ustalonego znaku \(w_0\)).

\sphinxAtStartPar
Oczywiście przy pewnych wartościach \( v_1 \) i \( v_2 \) i dla danego punktu z próbki danych, perceptron poda w wyniku poprawną lub błędną odpowiedź. Naturalne jest zatem zdefiniowanie \sphinxstylestrong{funkcji błędu} \(E\) w taki sposób, że dla każdego punktu \(p\) próbki wnosi 1, jeśli odpowiedź jest niepoprawna, a 0, jeśli jest poprawna:
\begin{equation*}
\begin{split} E(v_1,v_2)=\sum_p \left\{ \begin{array}{ll} 1 -{\rm niepoprawna,~}\\ 0 -{\rm poprawna.} \end{array}\right .\end{split}
\end{equation*}
\sphinxAtStartPar
\(E\) ma zatem interpretację liczby źle sklasyfikowanych punktów.

\sphinxAtStartPar
Możemy łatwo skonstruować tę funkcję w Pythonie:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{error}\PYG{p}{(}\PYG{n}{w0}\PYG{p}{,} \PYG{n}{w1} \PYG{p}{,}\PYG{n}{w2}\PYG{p}{,} \PYG{n}{sample}\PYG{p}{,} \PYG{n}{f}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{step}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    error function for the perceptron (for 2\PYGZhy{}dim data with labels)}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    inputs:}
\PYG{l+s+sd}{    w0, w1, w2 \PYGZhy{} weights}
\PYG{l+s+sd}{    sample \PYGZhy{} array of labeled data points p }
\PYG{l+s+sd}{             p in an array in the format [x1, x1, label]}
\PYG{l+s+sd}{    f \PYGZhy{} activation function}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    returns:}
\PYG{l+s+sd}{    error}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{n}{er}\PYG{o}{=}\PYG{l+m+mi}{0}                                       \PYG{c+c1}{\PYGZsh{} initial value of the error}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{sample}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:}               \PYG{c+c1}{\PYGZsh{} loop over data points       }
        \PYG{n}{yo}\PYG{o}{=}\PYG{n}{f}\PYG{p}{(}\PYG{n}{w0}\PYG{o}{+}\PYG{n}{w1}\PYG{o}{*}\PYG{n}{sample}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{+}\PYG{n}{w2}\PYG{o}{*}\PYG{n}{sample}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} obtained answer}
        \PYG{n}{er}\PYG{o}{+}\PYG{o}{=}\PYG{p}{(}\PYG{n}{yo}\PYG{o}{\PYGZhy{}}\PYG{n}{sample}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}
                      \PYG{c+c1}{\PYGZsh{} sample[i,2] is the label}
                      \PYG{c+c1}{\PYGZsh{} adds the square of the difference of yo and the label}
                      \PYG{c+c1}{\PYGZsh{} this adds 1 if the answer is incorrect, and 0 if correct}
    \PYG{k}{return} \PYG{n}{er}  \PYG{c+c1}{\PYGZsh{} the error}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
Zastosowaliśmy tutaj małą sztuczkę, mając na uwadze przyszłe zastosowania. Oznaczając otrzymany wynik dla danego punktu danych jako \(y_o^{(p)}\), a wynik prawdziwy (etykietę) jako \(y_t^{(p)}\) (obydwa przyjmują wartości 0 lub 1), możemy zdefiniowane powyżej \(E\) zapisać równoważnie jako
\begin{equation*}
\begin{split} E(v_1,v_2)=\sum_p \left ( y_o^{(p)}-y_t^{(p)}\right )^2,\end{split}
\end{equation*}
\sphinxAtStartPar
co jest wzorem zaprogramowanym w kodzie. Rzeczywiście, kiedy \(y_o^{(p)}=y_t^{(p)}\) (prawidłowa odpowiedź), wkład punktu wynosi 0, a kiedy \(y_o^{(p)}\neq y_t^{(p) }\) (błędna odpowiedź), wkład wynosi \((\pm 1)^2=1\).

\sphinxAtStartPar
Powtarzamy teraz symulacje z podrozdziału {\hyperref[\detokenize{docs/perceptron:perc-lab}]{\sphinxcrossref{\DUrole{std,std-ref}{Perceptron}}}}, aby wygenerować etykietowaną próbkę danych \sphinxstylestrong{samp2} o 200 punktach (próbka jest utworzona z \(w_0=-0.25\), \(w_1=-0.52\) i \(w_2=1\), co odpowiada \(v_1=2.08\) i \(v_2=-4\), przy czym \({\rm sgn}(w_0)=-1\)).

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{samp2}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
array([[4.84790349e\PYGZhy{}01, 9.30022602e\PYGZhy{}01, 1.00000000e+00],
       [2.75129766e\PYGZhy{}01, 4.51703076e\PYGZhy{}01, 1.00000000e+00],
       [8.18909207e\PYGZhy{}01, 9.68446177e\PYGZhy{}01, 1.00000000e+00],
       [4.05560105e\PYGZhy{}01, 2.52401227e\PYGZhy{}01, 0.00000000e+00],
       [2.40412348e\PYGZhy{}01, 2.13325311e\PYGZhy{}01, 0.00000000e+00],
       [8.62583044e\PYGZhy{}01, 4.03098090e\PYGZhy{}01, 0.00000000e+00],
       [2.72836046e\PYGZhy{}01, 8.33365880e\PYGZhy{}01, 1.00000000e+00],
       [3.46132713e\PYGZhy{}01, 5.29409807e\PYGZhy{}01, 1.00000000e+00],
       [1.39054560e\PYGZhy{}01, 3.09977679e\PYGZhy{}01, 0.00000000e+00],
       [7.49919483e\PYGZhy{}02, 1.05191492e\PYGZhy{}02, 0.00000000e+00],
       [4.16347086e\PYGZhy{}01, 3.50005519e\PYGZhy{}02, 0.00000000e+00],
       [3.45232415e\PYGZhy{}01, 1.90842569e\PYGZhy{}01, 0.00000000e+00],
       [3.50816164e\PYGZhy{}01, 5.02619660e\PYGZhy{}01, 1.00000000e+00],
       [7.78507745e\PYGZhy{}01, 8.59842026e\PYGZhy{}01, 1.00000000e+00],
       [7.52319571e\PYGZhy{}01, 4.41475183e\PYGZhy{}01, 0.00000000e+00],
       [8.80840022e\PYGZhy{}02, 7.51082561e\PYGZhy{}01, 1.00000000e+00],
       [2.13013515e\PYGZhy{}02, 6.52557690e\PYGZhy{}01, 1.00000000e+00],
       [7.41140252e\PYGZhy{}01, 9.13520421e\PYGZhy{}03, 0.00000000e+00],
       [7.27112931e\PYGZhy{}01, 3.39692519e\PYGZhy{}01, 0.00000000e+00],
       [8.81274609e\PYGZhy{}01, 9.53318090e\PYGZhy{}01, 1.00000000e+00],
       [1.19238710e\PYGZhy{}01, 4.73487031e\PYGZhy{}01, 1.00000000e+00],
       [8.47209336e\PYGZhy{}01, 5.39175428e\PYGZhy{}01, 0.00000000e+00],
       [2.82975222e\PYGZhy{}01, 4.12199125e\PYGZhy{}01, 1.00000000e+00],
       [6.89706830e\PYGZhy{}01, 6.74234474e\PYGZhy{}01, 1.00000000e+00],
       [7.41227421e\PYGZhy{}01, 9.46430564e\PYGZhy{}01, 1.00000000e+00],
       [7.83105358e\PYGZhy{}01, 9.21166233e\PYGZhy{}01, 1.00000000e+00],
       [4.62649042e\PYGZhy{}01, 1.84123870e\PYGZhy{}01, 0.00000000e+00],
       [6.83271243e\PYGZhy{}01, 1.58938321e\PYGZhy{}01, 0.00000000e+00],
       [6.82013430e\PYGZhy{}01, 7.51888769e\PYGZhy{}01, 1.00000000e+00],
       [4.02689078e\PYGZhy{}01, 3.27213540e\PYGZhy{}01, 0.00000000e+00],
       [8.46101381e\PYGZhy{}01, 9.10939489e\PYGZhy{}02, 0.00000000e+00],
       [7.07863315e\PYGZhy{}01, 7.63568455e\PYGZhy{}01, 1.00000000e+00],
       [2.71820593e\PYGZhy{}01, 8.69620304e\PYGZhy{}02, 0.00000000e+00],
       [9.63663955e\PYGZhy{}01, 6.54242756e\PYGZhy{}01, 0.00000000e+00],
       [2.14169377e\PYGZhy{}01, 9.34938197e\PYGZhy{}01, 1.00000000e+00],
       [6.83597944e\PYGZhy{}01, 3.38718588e\PYGZhy{}01, 0.00000000e+00],
       [2.07073197e\PYGZhy{}04, 5.36514950e\PYGZhy{}01, 1.00000000e+00],
       [5.71571952e\PYGZhy{}01, 5.93806115e\PYGZhy{}01, 1.00000000e+00],
       [1.81985686e\PYGZhy{}01, 2.49319597e\PYGZhy{}01, 0.00000000e+00],
       [8.97075027e\PYGZhy{}01, 4.07687741e\PYGZhy{}01, 0.00000000e+00],
       [3.95190693e\PYGZhy{}02, 4.47475039e\PYGZhy{}01, 1.00000000e+00],
       [1.59556853e\PYGZhy{}01, 3.52346311e\PYGZhy{}01, 1.00000000e+00],
       [2.47905931e\PYGZhy{}01, 9.59523254e\PYGZhy{}01, 1.00000000e+00],
       [1.79417499e\PYGZhy{}02, 7.72166519e\PYGZhy{}01, 1.00000000e+00],
       [6.45848085e\PYGZhy{}01, 8.31002803e\PYGZhy{}01, 1.00000000e+00],
       [4.81201574e\PYGZhy{}01, 5.10332531e\PYGZhy{}01, 1.00000000e+00],
       [9.14849486e\PYGZhy{}01, 4.30558020e\PYGZhy{}01, 0.00000000e+00],
       [6.54879997e\PYGZhy{}01, 2.53363308e\PYGZhy{}01, 0.00000000e+00],
       [6.79688313e\PYGZhy{}02, 4.34116154e\PYGZhy{}01, 1.00000000e+00],
       [1.40747159e\PYGZhy{}01, 4.34277105e\PYGZhy{}01, 1.00000000e+00],
       [3.26739773e\PYGZhy{}01, 6.52112881e\PYGZhy{}01, 1.00000000e+00],
       [7.67924864e\PYGZhy{}01, 8.86792580e\PYGZhy{}01, 1.00000000e+00],
       [8.54422677e\PYGZhy{}01, 9.79771529e\PYGZhy{}01, 1.00000000e+00],
       [7.68380500e\PYGZhy{}01, 9.52123598e\PYGZhy{}01, 1.00000000e+00],
       [3.46566369e\PYGZhy{}01, 8.43860111e\PYGZhy{}01, 1.00000000e+00],
       [5.32165385e\PYGZhy{}01, 1.02628734e\PYGZhy{}02, 0.00000000e+00],
       [3.65165001e\PYGZhy{}01, 1.98404068e\PYGZhy{}01, 0.00000000e+00],
       [8.54727736e\PYGZhy{}01, 5.29254172e\PYGZhy{}01, 0.00000000e+00],
       [8.60744991e\PYGZhy{}01, 9.12861912e\PYGZhy{}02, 0.00000000e+00],
       [9.32403467e\PYGZhy{}01, 7.87992143e\PYGZhy{}01, 1.00000000e+00],
       [9.66073749e\PYGZhy{}01, 4.41940782e\PYGZhy{}01, 0.00000000e+00],
       [6.98926148e\PYGZhy{}01, 3.71687556e\PYGZhy{}01, 0.00000000e+00],
       [9.92012313e\PYGZhy{}01, 6.13879961e\PYGZhy{}01, 0.00000000e+00],
       [2.32516197e\PYGZhy{}01, 6.08937806e\PYGZhy{}01, 1.00000000e+00],
       [5.44408989e\PYGZhy{}01, 3.70143008e\PYGZhy{}02, 0.00000000e+00],
       [3.97859163e\PYGZhy{}01, 1.30116136e\PYGZhy{}01, 0.00000000e+00],
       [3.74900409e\PYGZhy{}01, 9.86262437e\PYGZhy{}01, 1.00000000e+00],
       [9.56865294e\PYGZhy{}03, 7.83729822e\PYGZhy{}01, 1.00000000e+00],
       [9.74009172e\PYGZhy{}01, 5.82864086e\PYGZhy{}01, 0.00000000e+00],
       [4.76926272e\PYGZhy{}01, 3.72393482e\PYGZhy{}02, 0.00000000e+00],
       [8.95395141e\PYGZhy{}01, 9.06068592e\PYGZhy{}01, 1.00000000e+00],
       [4.30806418e\PYGZhy{}01, 2.83396670e\PYGZhy{}01, 0.00000000e+00],
       [4.98669230e\PYGZhy{}01, 3.48837057e\PYGZhy{}02, 0.00000000e+00],
       [4.59789551e\PYGZhy{}01, 2.79465240e\PYGZhy{}01, 0.00000000e+00],
       [4.25886684e\PYGZhy{}01, 2.45739568e\PYGZhy{}01, 0.00000000e+00],
       [7.67740580e\PYGZhy{}01, 4.68087453e\PYGZhy{}01, 0.00000000e+00],
       [6.15560750e\PYGZhy{}01, 5.99461778e\PYGZhy{}01, 1.00000000e+00],
       [3.39270115e\PYGZhy{}01, 1.96552258e\PYGZhy{}01, 0.00000000e+00],
       [9.50997909e\PYGZhy{}01, 2.11274858e\PYGZhy{}01, 0.00000000e+00],
       [9.22346849e\PYGZhy{}02, 4.96461162e\PYGZhy{}01, 1.00000000e+00],
       [4.99067735e\PYGZhy{}01, 7.76327469e\PYGZhy{}02, 0.00000000e+00],
       [5.11501772e\PYGZhy{}01, 6.53949966e\PYGZhy{}02, 0.00000000e+00],
       [4.88357776e\PYGZhy{}02, 7.03588263e\PYGZhy{}01, 1.00000000e+00],
       [4.43341105e\PYGZhy{}01, 3.87800688e\PYGZhy{}02, 0.00000000e+00],
       [3.00487124e\PYGZhy{}02, 3.46736610e\PYGZhy{}01, 1.00000000e+00],
       [7.33909911e\PYGZhy{}01, 6.03645142e\PYGZhy{}01, 0.00000000e+00],
       [9.74396387e\PYGZhy{}01, 5.22591864e\PYGZhy{}01, 0.00000000e+00],
       [8.83086372e\PYGZhy{}01, 9.37484612e\PYGZhy{}01, 1.00000000e+00],
       [6.53216174e\PYGZhy{}02, 7.39886164e\PYGZhy{}02, 0.00000000e+00],
       [2.27984351e\PYGZhy{}01, 5.21808064e\PYGZhy{}01, 1.00000000e+00],
       [1.65607112e\PYGZhy{}01, 3.45269775e\PYGZhy{}01, 1.00000000e+00],
       [5.64813989e\PYGZhy{}02, 9.92833818e\PYGZhy{}01, 1.00000000e+00],
       [8.79814722e\PYGZhy{}01, 2.53955297e\PYGZhy{}01, 0.00000000e+00],
       [9.77485547e\PYGZhy{}01, 1.70025498e\PYGZhy{}01, 0.00000000e+00],
       [1.41836206e\PYGZhy{}01, 3.15041925e\PYGZhy{}01, 0.00000000e+00],
       [2.59220533e\PYGZhy{}01, 9.62980492e\PYGZhy{}01, 1.00000000e+00],
       [3.51497131e\PYGZhy{}01, 2.78906080e\PYGZhy{}01, 0.00000000e+00],
       [6.64580400e\PYGZhy{}01, 7.01842934e\PYGZhy{}02, 0.00000000e+00],
       [4.44121819e\PYGZhy{}01, 3.51691754e\PYGZhy{}01, 0.00000000e+00],
       [5.42772572e\PYGZhy{}01, 3.74177254e\PYGZhy{}01, 0.00000000e+00],
       [5.33559659e\PYGZhy{}01, 6.03796695e\PYGZhy{}02, 0.00000000e+00],
       [7.14464764e\PYGZhy{}01, 4.67565165e\PYGZhy{}01, 0.00000000e+00],
       [6.38031422e\PYGZhy{}01, 2.95004638e\PYGZhy{}01, 0.00000000e+00],
       [4.88271906e\PYGZhy{}02, 9.38922578e\PYGZhy{}01, 1.00000000e+00],
       [1.02843362e\PYGZhy{}01, 9.37126526e\PYGZhy{}01, 1.00000000e+00],
       [3.56534779e\PYGZhy{}01, 7.08709614e\PYGZhy{}01, 1.00000000e+00],
       [1.63519911e\PYGZhy{}01, 7.23977981e\PYGZhy{}01, 1.00000000e+00],
       [6.60446614e\PYGZhy{}01, 9.30841325e\PYGZhy{}01, 1.00000000e+00],
       [5.81182508e\PYGZhy{}01, 5.87915078e\PYGZhy{}01, 1.00000000e+00],
       [6.11213231e\PYGZhy{}01, 9.36195893e\PYGZhy{}01, 1.00000000e+00],
       [2.16768336e\PYGZhy{}01, 1.38934897e\PYGZhy{}01, 0.00000000e+00],
       [3.28105767e\PYGZhy{}01, 1.91262018e\PYGZhy{}01, 0.00000000e+00],
       [5.73652565e\PYGZhy{}01, 7.60061984e\PYGZhy{}01, 1.00000000e+00],
       [8.79005485e\PYGZhy{}02, 1.20505401e\PYGZhy{}02, 0.00000000e+00],
       [7.01190102e\PYGZhy{}01, 9.40271570e\PYGZhy{}01, 1.00000000e+00],
       [7.68977621e\PYGZhy{}01, 4.09961844e\PYGZhy{}01, 0.00000000e+00],
       [9.87505209e\PYGZhy{}01, 6.99815931e\PYGZhy{}01, 0.00000000e+00],
       [9.67779193e\PYGZhy{}03, 6.46274662e\PYGZhy{}01, 1.00000000e+00],
       [9.14770046e\PYGZhy{}01, 3.83878854e\PYGZhy{}01, 0.00000000e+00],
       [1.35396651e\PYGZhy{}01, 2.25502598e\PYGZhy{}01, 0.00000000e+00],
       [3.25598172e\PYGZhy{}01, 7.85606665e\PYGZhy{}03, 0.00000000e+00],
       [2.76528602e\PYGZhy{}01, 6.52431168e\PYGZhy{}01, 1.00000000e+00],
       [5.61591535e\PYGZhy{}01, 4.64326110e\PYGZhy{}01, 0.00000000e+00],
       [2.81257091e\PYGZhy{}01, 1.41430565e\PYGZhy{}01, 0.00000000e+00],
       [6.24085557e\PYGZhy{}01, 6.02068843e\PYGZhy{}01, 1.00000000e+00],
       [5.75462765e\PYGZhy{}02, 2.65035254e\PYGZhy{}01, 0.00000000e+00],
       [4.68613721e\PYGZhy{}01, 6.88933434e\PYGZhy{}01, 1.00000000e+00],
       [1.96038171e\PYGZhy{}01, 7.05489818e\PYGZhy{}01, 1.00000000e+00],
       [1.17127539e\PYGZhy{}01, 8.23033288e\PYGZhy{}01, 1.00000000e+00],
       [6.72151270e\PYGZhy{}01, 3.76131224e\PYGZhy{}01, 0.00000000e+00],
       [7.23790310e\PYGZhy{}02, 1.67986211e\PYGZhy{}01, 0.00000000e+00],
       [3.68925808e\PYGZhy{}01, 7.34899073e\PYGZhy{}01, 1.00000000e+00],
       [1.04107619e\PYGZhy{}01, 4.70991548e\PYGZhy{}01, 1.00000000e+00],
       [8.71298554e\PYGZhy{}01, 8.94217628e\PYGZhy{}01, 1.00000000e+00],
       [2.27461169e\PYGZhy{}01, 5.95966688e\PYGZhy{}01, 1.00000000e+00],
       [3.01397955e\PYGZhy{}01, 8.57056764e\PYGZhy{}01, 1.00000000e+00],
       [5.38521560e\PYGZhy{}02, 8.98289821e\PYGZhy{}02, 0.00000000e+00],
       [2.79326700e\PYGZhy{}01, 1.96546998e\PYGZhy{}01, 0.00000000e+00],
       [5.60660280e\PYGZhy{}01, 7.44485804e\PYGZhy{}01, 1.00000000e+00],
       [4.31773279e\PYGZhy{}01, 5.66443250e\PYGZhy{}02, 0.00000000e+00],
       [5.37837307e\PYGZhy{}02, 8.11545749e\PYGZhy{}01, 1.00000000e+00],
       [1.49346734e\PYGZhy{}01, 4.04671819e\PYGZhy{}01, 1.00000000e+00],
       [3.43854263e\PYGZhy{}01, 5.09228519e\PYGZhy{}01, 1.00000000e+00],
       [9.37027592e\PYGZhy{}01, 2.18104530e\PYGZhy{}01, 0.00000000e+00],
       [3.07357632e\PYGZhy{}02, 2.75480486e\PYGZhy{}01, 1.00000000e+00],
       [8.88030972e\PYGZhy{}01, 6.95808939e\PYGZhy{}01, 0.00000000e+00],
       [2.64822883e\PYGZhy{}01, 6.49500344e\PYGZhy{}01, 1.00000000e+00],
       [7.41011052e\PYGZhy{}01, 2.08787570e\PYGZhy{}02, 0.00000000e+00],
       [4.66054972e\PYGZhy{}01, 2.71924695e\PYGZhy{}02, 0.00000000e+00],
       [2.92732671e\PYGZhy{}01, 2.73976866e\PYGZhy{}01, 0.00000000e+00],
       [7.44176499e\PYGZhy{}01, 1.09548317e\PYGZhy{}01, 0.00000000e+00],
       [8.44894264e\PYGZhy{}01, 1.40473451e\PYGZhy{}01, 0.00000000e+00],
       [4.07356214e\PYGZhy{}01, 6.69652397e\PYGZhy{}01, 1.00000000e+00],
       [2.68427501e\PYGZhy{}01, 5.46381993e\PYGZhy{}01, 1.00000000e+00],
       [7.53492249e\PYGZhy{}01, 9.08693333e\PYGZhy{}01, 1.00000000e+00],
       [6.96070748e\PYGZhy{}01, 7.46383256e\PYGZhy{}01, 1.00000000e+00],
       [5.18458052e\PYGZhy{}01, 8.92999952e\PYGZhy{}01, 1.00000000e+00],
       [4.07917943e\PYGZhy{}01, 2.78845348e\PYGZhy{}01, 0.00000000e+00],
       [1.98397409e\PYGZhy{}01, 7.86913440e\PYGZhy{}01, 1.00000000e+00],
       [4.16230384e\PYGZhy{}01, 7.99952479e\PYGZhy{}01, 1.00000000e+00],
       [4.13486165e\PYGZhy{}01, 9.79455020e\PYGZhy{}01, 1.00000000e+00],
       [3.53564575e\PYGZhy{}01, 1.68300355e\PYGZhy{}01, 0.00000000e+00],
       [6.58037939e\PYGZhy{}01, 2.76240534e\PYGZhy{}01, 0.00000000e+00],
       [9.04815998e\PYGZhy{}03, 3.86544837e\PYGZhy{}01, 1.00000000e+00],
       [5.27455111e\PYGZhy{}01, 6.02606684e\PYGZhy{}01, 1.00000000e+00],
       [4.93421019e\PYGZhy{}01, 4.09192518e\PYGZhy{}01, 0.00000000e+00],
       [9.60430349e\PYGZhy{}01, 4.07517140e\PYGZhy{}01, 0.00000000e+00],
       [3.50077033e\PYGZhy{}01, 4.62910544e\PYGZhy{}04, 0.00000000e+00],
       [7.40289080e\PYGZhy{}01, 8.79375483e\PYGZhy{}02, 0.00000000e+00],
       [7.66813789e\PYGZhy{}01, 8.56678167e\PYGZhy{}01, 1.00000000e+00],
       [8.46424465e\PYGZhy{}01, 4.17965943e\PYGZhy{}01, 0.00000000e+00],
       [7.40121753e\PYGZhy{}01, 5.33473683e\PYGZhy{}01, 0.00000000e+00],
       [3.81687763e\PYGZhy{}01, 7.09661984e\PYGZhy{}01, 1.00000000e+00],
       [9.31298624e\PYGZhy{}01, 8.00767728e\PYGZhy{}01, 1.00000000e+00],
       [6.48551936e\PYGZhy{}01, 3.66151820e\PYGZhy{}01, 0.00000000e+00],
       [9.41612580e\PYGZhy{}01, 8.36248778e\PYGZhy{}01, 1.00000000e+00],
       [7.49208416e\PYGZhy{}01, 6.87896330e\PYGZhy{}01, 1.00000000e+00],
       [6.69896004e\PYGZhy{}01, 1.14646038e\PYGZhy{}01, 0.00000000e+00],
       [9.42681130e\PYGZhy{}01, 1.38038325e\PYGZhy{}03, 0.00000000e+00],
       [4.74073072e\PYGZhy{}01, 3.51599494e\PYGZhy{}01, 0.00000000e+00],
       [5.49614802e\PYGZhy{}02, 6.14350942e\PYGZhy{}01, 1.00000000e+00],
       [4.31399689e\PYGZhy{}01, 3.47906073e\PYGZhy{}02, 0.00000000e+00],
       [2.57511556e\PYGZhy{}02, 2.68226985e\PYGZhy{}01, 1.00000000e+00],
       [7.36364210e\PYGZhy{}01, 9.21662568e\PYGZhy{}01, 1.00000000e+00],
       [6.28482144e\PYGZhy{}01, 2.17778061e\PYGZhy{}01, 0.00000000e+00],
       [6.28728082e\PYGZhy{}01, 4.01647514e\PYGZhy{}01, 0.00000000e+00],
       [6.46835461e\PYGZhy{}01, 1.85864116e\PYGZhy{}01, 0.00000000e+00],
       [3.69749090e\PYGZhy{}01, 5.67486820e\PYGZhy{}01, 1.00000000e+00],
       [9.72027071e\PYGZhy{}01, 7.91242606e\PYGZhy{}01, 1.00000000e+00],
       [5.74023581e\PYGZhy{}01, 6.52252318e\PYGZhy{}01, 1.00000000e+00],
       [2.63226960e\PYGZhy{}01, 5.59692843e\PYGZhy{}01, 1.00000000e+00],
       [2.43296169e\PYGZhy{}01, 8.03720599e\PYGZhy{}01, 1.00000000e+00],
       [1.86737783e\PYGZhy{}02, 1.03393502e\PYGZhy{}01, 0.00000000e+00],
       [1.70963893e\PYGZhy{}01, 5.44984648e\PYGZhy{}01, 1.00000000e+00],
       [3.08646223e\PYGZhy{}01, 4.61021625e\PYGZhy{}04, 0.00000000e+00],
       [3.11928831e\PYGZhy{}01, 4.61607031e\PYGZhy{}01, 1.00000000e+00],
       [2.50573227e\PYGZhy{}01, 3.79165344e\PYGZhy{}01, 0.00000000e+00],
       [3.43787975e\PYGZhy{}01, 5.22515312e\PYGZhy{}01, 1.00000000e+00],
       [5.46515013e\PYGZhy{}01, 8.54968356e\PYGZhy{}01, 1.00000000e+00],
       [5.62483408e\PYGZhy{}01, 1.35878860e\PYGZhy{}03, 0.00000000e+00]])
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Potrzebujemy teraz ponownie użyć algorytmu perceptronu z rozdz. {\hyperref[\detokenize{docs/perceptron:lab-pa}]{\sphinxcrossref{\DUrole{std,std-ref}{Algorytm perceptronu}}}}. W naszym szczególnym przypadku działa on na próbce dwuwymiarowych danych etykietowanych. Dla wygody, pojedyncza runda algorytmu może zostać zebrana w funkcję w następujący sposób:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{teach\PYGZus{}perceptron}\PYG{p}{(}\PYG{n}{sample}\PYG{p}{,} \PYG{n}{eps}\PYG{p}{,} \PYG{n}{w\PYGZus{}in}\PYG{p}{,} \PYG{n}{f}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{step}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    Supervised learning for a single perceptron (single MCP neuron) }
\PYG{l+s+sd}{    for a sample of 2\PYGZhy{}dim. labeled data}
\PYG{l+s+sd}{       }
\PYG{l+s+sd}{    input:}
\PYG{l+s+sd}{    sample \PYGZhy{} array of two\PYGZhy{}dimensional labeled data points p}
\PYG{l+s+sd}{             p is an array in the format [x1,x2,label]}
\PYG{l+s+sd}{             label = 0 or 1}
\PYG{l+s+sd}{    eps    \PYGZhy{} learning speed}
\PYG{l+s+sd}{    w\PYGZus{}in   \PYGZhy{} initial weights in the format [[w0], [w1], [w2]]}
\PYG{l+s+sd}{    f      \PYGZhy{} activation function}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    return: updated weights in the format [[w0], [w1], [w2]]}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{p}{[}\PYG{p}{[}\PYG{n}{w0}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{n}{w1}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{n}{w2}\PYG{p}{]}\PYG{p}{]}\PYG{o}{=}\PYG{n}{w\PYGZus{}in}         \PYG{c+c1}{\PYGZsh{} define w0, w1, and w2}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{sample}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:}  \PYG{c+c1}{\PYGZsh{} loop over the whole sample}
        \PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{10}\PYG{p}{)}\PYG{p}{:}       \PYG{c+c1}{\PYGZsh{} repeat 10 times  }
            
            \PYG{n}{yo}\PYG{o}{=}\PYG{n}{f}\PYG{p}{(}\PYG{n}{w0}\PYG{o}{+}\PYG{n}{w1}\PYG{o}{*}\PYG{n}{sample}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{+}\PYG{n}{w2}\PYG{o}{*}\PYG{n}{sample}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} output from the neuron, f(x.w)}
            
            \PYG{c+c1}{\PYGZsh{} update of weights according to the perceptron algorithm formula}
            \PYG{n}{w0}\PYG{o}{=}\PYG{n}{w0}\PYG{o}{+}\PYG{n}{eps}\PYG{o}{*}\PYG{p}{(}\PYG{n}{sample}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{n}{yo}\PYG{p}{)}\PYG{o}{*}\PYG{l+m+mi}{1}
            \PYG{n}{w1}\PYG{o}{=}\PYG{n}{w1}\PYG{o}{+}\PYG{n}{eps}\PYG{o}{*}\PYG{p}{(}\PYG{n}{sample}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{n}{yo}\PYG{p}{)}\PYG{o}{*}\PYG{n}{sample}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}
            \PYG{n}{w2}\PYG{o}{=}\PYG{n}{w2}\PYG{o}{+}\PYG{n}{eps}\PYG{o}{*}\PYG{p}{(}\PYG{n}{sample}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{n}{yo}\PYG{p}{)}\PYG{o}{*}\PYG{n}{sample}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}
            
    \PYG{k}{return} \PYG{p}{[}\PYG{p}{[}\PYG{n}{w0}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{n}{w1}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{n}{w2}\PYG{p}{]}\PYG{p}{]}       \PYG{c+c1}{\PYGZsh{} updated weights}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
Następnie prześledzimy działanie algorytmu perceptronu, obserwując jak modyfikuje on wartości wprowadzonej powyżej funkcji błędu \(E(v_1,v_2)\). Zaczynamy od losowych wag, a następnie wykonujemy 10 rund zdefiniowanej powyżej funkcji \sphinxstylestrong{teach\_perceptron}, wypisując zaktualizowane wagi i odpowiadający im błąd:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{weights}\PYG{o}{=}\PYG{p}{[}\PYG{p}{[}\PYG{n}{func}\PYG{o}{.}\PYG{n}{rn}\PYG{p}{(}\PYG{p}{)}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{n}{func}\PYG{o}{.}\PYG{n}{rn}\PYG{p}{(}\PYG{p}{)}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{n}{func}\PYG{o}{.}\PYG{n}{rn}\PYG{p}{(}\PYG{p}{)}\PYG{p}{]}\PYG{p}{]} \PYG{c+c1}{\PYGZsh{} initial random weights}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{weights}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
[[\PYGZhy{}0.3475843530562248], [\PYGZhy{}0.14307973580699285], [\PYGZhy{}0.036348525023713885]]
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Optimum:}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{   w0  w1/w0  w2/w0 error}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}   \PYG{c+c1}{\PYGZsh{} header}

\PYG{n}{eps}\PYG{o}{=}\PYG{l+m+mf}{0.7}                 \PYG{c+c1}{\PYGZsh{} initial learning speed}
\PYG{k}{for} \PYG{n}{r} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{200}\PYG{p}{)}\PYG{p}{:}     \PYG{c+c1}{\PYGZsh{} rounds}
    \PYG{n}{eps}\PYG{o}{=}\PYG{l+m+mf}{0.9}\PYG{o}{*}\PYG{n}{eps}         \PYG{c+c1}{\PYGZsh{} decrease the learning speed}
    \PYG{n}{weights}\PYG{o}{=}\PYG{n}{teach\PYGZus{}perceptron}\PYG{p}{(}\PYG{n}{samp2}\PYG{p}{,}\PYG{n}{eps}\PYG{p}{,}\PYG{n}{weights}\PYG{p}{,}\PYG{n}{func}\PYG{o}{.}\PYG{n}{step}\PYG{p}{)} 
                        \PYG{c+c1}{\PYGZsh{} see the top of this chapter}
        
    \PYG{n}{w0\PYGZus{}o}\PYG{o}{=}\PYG{n}{weights}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}  \PYG{c+c1}{\PYGZsh{} updated weights and ratios}
    \PYG{n}{v1\PYGZus{}o}\PYG{o}{=}\PYG{n}{weights}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{/}\PYG{n}{weights}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}
    \PYG{n}{v2\PYGZus{}o}\PYG{o}{=}\PYG{n}{weights}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{/}\PYG{n}{weights}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}
    
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(}\PYG{n}{w0\PYGZus{}o}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{,}\PYG{n}{np}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(}\PYG{n}{v1\PYGZus{}o}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{,}\PYG{n}{np}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(}\PYG{n}{v2\PYGZus{}o}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{,}
          \PYG{n}{np}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(}\PYG{n}{error}\PYG{p}{(}\PYG{n}{w0\PYGZus{}o}\PYG{p}{,} \PYG{n}{w0\PYGZus{}o}\PYG{o}{*}\PYG{n}{v1\PYGZus{}o}\PYG{p}{,} \PYG{n}{w0\PYGZus{}o}\PYG{o}{*}\PYG{n}{v2\PYGZus{}o}\PYG{p}{,} \PYG{n}{samp2}\PYG{p}{,} \PYG{n}{func}\PYG{o}{.}\PYG{n}{step}\PYG{p}{)}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{p}{)}             
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
Optimum:
   w0  w1/w0  w2/w0 error
\PYGZhy{}1.158 2.094 \PYGZhy{}3.81 8.0
\PYGZhy{}1.158 1.933 \PYGZhy{}4.323 7.0
\PYGZhy{}1.158 2.233 \PYGZhy{}4.492 6.0
\PYGZhy{}1.617 1.812 \PYGZhy{}3.157 21.0
\PYGZhy{}1.617 1.84 \PYGZhy{}3.244 20.0
\PYGZhy{}1.617 1.873 \PYGZhy{}3.305 17.0
\PYGZhy{}1.617 1.744 \PYGZhy{}3.422 9.0
\PYGZhy{}1.617 1.759 \PYGZhy{}3.452 8.0
\PYGZhy{}1.617 1.772 \PYGZhy{}3.48 8.0
\PYGZhy{}1.617 1.853 \PYGZhy{}3.446 13.0
\PYGZhy{}1.617 1.849 \PYGZhy{}3.45 13.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\PYGZhy{}1.42 2.067 \PYGZhy{}3.988 0.0
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Zauważamy, że w kolejnych rundach błąd stopniowo maleje (w zależności od symulacji, może czasem nieco podskoczyć, jeśli szybkość uczenia się jest zbyt duża, ale nie stanowi to problemu, o ile koniec końców możemy zejść do minimum), osiągając ostatecznie wartość bardzo małą lub dokładnie 0 (w zależności od konkretnego przypadku symulacji). W związku z tym
algorytm perceptronu, jak już widzieliśmy w rozdziale {\hyperref[\detokenize{docs/perceptron:perc-lab}]{\sphinxcrossref{\DUrole{std,std-ref}{Perceptron}}}}, \sphinxstylestrong{minimalizuje błąd dla próbki treningowej}.

\sphinxAtStartPar
Pouczające jest spojrzenie na mapę konturową funkcji błędu \(E(v_1, v_2)\) w pobliżu optymalnych parametrów:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{fig}\PYG{p}{,} \PYG{n}{ax} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mf}{3.7}\PYG{p}{,}\PYG{l+m+mf}{3.7}\PYG{p}{)}\PYG{p}{,}\PYG{n}{dpi}\PYG{o}{=}\PYG{l+m+mi}{120}\PYG{p}{)}

\PYG{n}{delta} \PYG{o}{=} \PYG{l+m+mf}{0.02}  \PYG{c+c1}{\PYGZsh{} grid step in v1 and v2 for the contour map}
\PYG{n}{ran}\PYG{o}{=}\PYG{l+m+mf}{0.8}       \PYG{c+c1}{\PYGZsh{} plot range around (v1\PYGZus{}o, v2\PYGZus{}o)}

\PYG{n}{v1} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{n}{v1\PYGZus{}o}\PYG{o}{\PYGZhy{}}\PYG{n}{ran}\PYG{p}{,}\PYG{n}{v1\PYGZus{}o}\PYG{o}{+}\PYG{n}{ran}\PYG{p}{,} \PYG{n}{delta}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} grid for v1}
\PYG{n}{v2} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{n}{v2\PYGZus{}o}\PYG{o}{\PYGZhy{}}\PYG{n}{ran}\PYG{p}{,}\PYG{n}{v2\PYGZus{}o}\PYG{o}{+}\PYG{n}{ran}\PYG{p}{,} \PYG{n}{delta}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} grid for v2}
\PYG{n}{X}\PYG{p}{,} \PYG{n}{Y} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{meshgrid}\PYG{p}{(}\PYG{n}{v1}\PYG{p}{,} \PYG{n}{v2}\PYG{p}{)}               \PYG{c+c1}{\PYGZsh{} mesh for the contour plot}

\PYG{n}{Z}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{n}{error}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{o}{\PYGZhy{}}\PYG{n}{v1}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{,}\PYG{o}{\PYGZhy{}}\PYG{n}{v2}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{p}{,}\PYG{n}{samp2}\PYG{p}{,}\PYG{n}{func}\PYG{o}{.}\PYG{n}{step}\PYG{p}{)} 
             \PYG{c+c1}{\PYGZsh{} we use the scaling property of the error function here }
             \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{v1}\PYG{p}{)}\PYG{p}{)}\PYG{p}{]} \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{v2}\PYG{p}{)}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} values of E(v1,v2) }

\PYG{n}{CS} \PYG{o}{=} \PYG{n}{ax}\PYG{o}{.}\PYG{n}{contour}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,} \PYG{n}{Y}\PYG{p}{,} \PYG{n}{Z}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{5}\PYG{p}{,}\PYG{l+m+mi}{10}\PYG{p}{,}\PYG{l+m+mi}{15}\PYG{p}{,}\PYG{l+m+mi}{20}\PYG{p}{,}\PYG{l+m+mi}{25}\PYG{p}{,}\PYG{l+m+mi}{30}\PYG{p}{,}\PYG{l+m+mi}{35}\PYG{p}{,}\PYG{l+m+mi}{40}\PYG{p}{,}\PYG{l+m+mi}{45}\PYG{p}{,}\PYG{l+m+mi}{50}\PYG{p}{]}\PYG{p}{)}
                        \PYG{c+c1}{\PYGZsh{} explicit contour level values}
    
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{clabel}\PYG{p}{(}\PYG{n}{CS}\PYG{p}{,} \PYG{n}{inline}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{fmt}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+si}{\PYGZpc{}1.0f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{9}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} contour label format}

\PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Error function}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}aspect}\PYG{p}{(}\PYG{n}{aspect}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}

\PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZdl{}v\PYGZus{}1\PYGZdl{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZdl{}v\PYGZus{}2\PYGZdl{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}

\PYG{n}{ax}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{v1\PYGZus{}o}\PYG{p}{,} \PYG{n}{v2\PYGZus{}o}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{20}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{red}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{found minimum}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} our found optimal point}

\PYG{n}{ax}\PYG{o}{.}\PYG{n}{legend}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{backprop_19_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Uzyskane minimum znajduje się wewnątrz (lub blisko, w zależności od symulacji) wydłużonego obszaru w \(v_1\) i \(v_2\), gdzie błąd znika.


\section{Ciągła funkcja aktywacji}
\label{\detokenize{docs/backprop:ciagla-funkcja-aktywacji}}
\sphinxAtStartPar
Przyglądając się uważniej powyższej mapie konturowej, widzimy, że linie są „ząbkowane”. Dzieje się tak, ponieważ funkcja błędu, z oczywistego powodu, przyjmuje wartości całkowite. Jest zatem nieciągła, a zatem nieróżniczkowalna. Nieciągłości wynikają z nieciągłej funkcji aktywacji, mianowicie funkcji schodkowej. Mając na uwadze techniki, które poznamy niebawem, korzystne jest stosowanie funkcji aktywacji, która jest różniczkowalna. Historycznie tzw. \sphinxstylestrong{sigmoid}
\begin{equation*}
\begin{split} \sigma(s)=\frac{1}{1+e^{-s}}\end{split}
\end{equation*}
\sphinxAtStartPar
był wykorzystywany w wielu praktycznych zastosowaniach dla ANN.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} sigmoid, a.k.a. the logistic function, or simply (1+arctanh(\PYGZhy{}s/2))/2 }
\PYG{k}{def} \PYG{n+nf}{sig}\PYG{p}{(}\PYG{n}{s}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{return} \PYG{l+m+mi}{1}\PYG{o}{/}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{o}{+}\PYG{n}{np}\PYG{o}{.}\PYG{n}{exp}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{n}{s}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{backprop_24_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
Funkcja ta jest oczywiście różniczkowalna. Ponadto
\begin{equation*}
\begin{split} \sigma '(s) = \sigma (s) [1- \sigma (s)], \end{split}
\end{equation*}
\sphinxAtStartPar
co jest szczególna własnością sigmoidu.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} derivative of sigmoid}
\PYG{k}{def} \PYG{n+nf}{dsig}\PYG{p}{(}\PYG{n}{s}\PYG{p}{)}\PYG{p}{:}
     \PYG{k}{return} \PYG{n}{sig}\PYG{p}{(}\PYG{n}{s}\PYG{p}{)}\PYG{o}{*}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{o}{\PYGZhy{}}\PYG{n}{sig}\PYG{p}{(}\PYG{n}{s}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{backprop_27_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
Wprowadza się również sigmoid z „temperaturą” \(T \) (nomenklatura ta jest związana z podobnymi wyrażeniami dla funkcji termodynamicznych w fizyce):
\$\(\sigma(s;T)=\frac{1}{1+e^{-s/T}}.\)\$

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} sigmoid with temperature T}
\PYG{k}{def} \PYG{n+nf}{sig\PYGZus{}T}\PYG{p}{(}\PYG{n}{s}\PYG{p}{,}\PYG{n}{T}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{return} \PYG{l+m+mi}{1}\PYG{o}{/}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{o}{+}\PYG{n}{np}\PYG{o}{.}\PYG{n}{exp}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{n}{s}\PYG{o}{/}\PYG{n}{T}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{backprop_30_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
Dla coraz mniejszych \(T\) sigmoid zbliża się do poprzednio używanej funkcji schodkowej.

\sphinxAtStartPar
Zauważ, że argumentem sigmoidu jest iloraz
\begin{equation*}
\begin{split}
s/T = (w_0 + w_1 x_1 + w_2 x_2) / T = w_0 / T + w_1 / T \, x_1 + w_2 / T \, x_2 = \xi_0 + \xi_1 x_1 + \xi_2 x_2,
\end{split}
\end{equation*}
\sphinxAtStartPar
co oznacza, że zawsze możemy przyjąć \(T = 1\) bez utraty ogólności (\(T \) to „skala”). Jednak teraz mamy trzy niezależne argumenty \( \xi_0 \), \( \xi_1 \) i \(\xi_2\), więc nie można zredukować obecnej sytuacji do tylko dwóch niezależnych parametrów, jak miało to miejsce w poprzednim podrozdziale.

\sphinxAtStartPar
Powtórzymy teraz nasz przykład z klasyfikatorem, ale z funkcją aktywacji daną przez sigmoid. Funkcja błędu
\begin{equation*}
\begin{split}y_o^{(p)}=\sigma(w_0+w_1 x_1^{(p)} +w_2 x_2^{(p)}), \end{split}
\end{equation*}
\sphinxAtStartPar
staje się teraz
\begin{equation*}
\begin{split}E(w_0,w_1,w_2)=\sum_p \left [\sigma(w_0+w_1 x_1^{(p)} +w_2 x_2^{(p)})-y_t^{(p)} \right] ^2.\end{split}
\end{equation*}
\sphinxAtStartPar
Algorytm perceptronu z funkcją aktywacji sigmoidu wykonujemy 1000 razy, wypisując co 100 krok:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{weights}\PYG{o}{=}\PYG{p}{[}\PYG{p}{[}\PYG{n}{func}\PYG{o}{.}\PYG{n}{rn}\PYG{p}{(}\PYG{p}{)}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{n}{func}\PYG{o}{.}\PYG{n}{rn}\PYG{p}{(}\PYG{p}{)}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{n}{func}\PYG{o}{.}\PYG{n}{rn}\PYG{p}{(}\PYG{p}{)}\PYG{p}{]}\PYG{p}{]}      \PYG{c+c1}{\PYGZsh{} random weights from [\PYGZhy{}0.5,0.5]}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{   w0   w1/w0  w2/w0 error}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}   \PYG{c+c1}{\PYGZsh{} header}

\PYG{n}{eps}\PYG{o}{=}\PYG{l+m+mf}{0.7}                       \PYG{c+c1}{\PYGZsh{} initial learning speed}
\PYG{k}{for} \PYG{n}{r} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{10000}\PYG{p}{)}\PYG{p}{:}         \PYG{c+c1}{\PYGZsh{} rounds}
    \PYG{n}{eps}\PYG{o}{=}\PYG{l+m+mf}{0.9995}\PYG{o}{*}\PYG{n}{eps}            \PYG{c+c1}{\PYGZsh{} decrease learning speed}
    \PYG{n}{weights}\PYG{o}{=}\PYG{n}{teach\PYGZus{}perceptron}\PYG{p}{(}\PYG{n}{samp2}\PYG{p}{,}\PYG{n}{eps}\PYG{p}{,}\PYG{n}{weights}\PYG{p}{,}\PYG{n}{func}\PYG{o}{.}\PYG{n}{sig}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} update weights}
    \PYG{k}{if} \PYG{n}{r}\PYG{o}{\PYGZpc{}}\PYG{k}{100}==99:
        \PYG{n}{w0\PYGZus{}o}\PYG{o}{=}\PYG{n}{weights}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}               \PYG{c+c1}{\PYGZsh{} updated weights }
        \PYG{n}{w1\PYGZus{}o}\PYG{o}{=}\PYG{n}{weights}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]} 
        \PYG{n}{w2\PYGZus{}o}\PYG{o}{=}\PYG{n}{weights}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]} 
        \PYG{n}{v1\PYGZus{}o}\PYG{o}{=}\PYG{n}{w1\PYGZus{}o}\PYG{o}{/}\PYG{n}{w0\PYGZus{}o}                   \PYG{c+c1}{\PYGZsh{} ratios of weights}
        \PYG{n}{v2\PYGZus{}o}\PYG{o}{=}\PYG{n}{w2\PYGZus{}o}\PYG{o}{/}\PYG{n}{w0\PYGZus{}o}
        \PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(}\PYG{n}{w0\PYGZus{}o}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{,}\PYG{n}{np}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(}\PYG{n}{v1\PYGZus{}o}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{,}\PYG{n}{np}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(}\PYG{n}{v2\PYGZus{}o}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{,}
              \PYG{n}{np}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(}\PYG{n}{error}\PYG{p}{(}\PYG{n}{w0\PYGZus{}o}\PYG{p}{,} \PYG{n}{w0\PYGZus{}o}\PYG{o}{*}\PYG{n}{v1\PYGZus{}o}\PYG{p}{,} \PYG{n}{w0\PYGZus{}o}\PYG{o}{*}\PYG{n}{v2\PYGZus{}o}\PYG{p}{,} \PYG{n}{samp2}\PYG{p}{,} \PYG{n}{func}\PYG{o}{.}\PYG{n}{sig}\PYG{p}{)}\PYG{p}{,}\PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{)}                             
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
   w0   w1/w0  w2/w0 error
\PYGZhy{}19.537 2.072 \PYGZhy{}3.785 3.92218
\PYGZhy{}24.638 2.094 \PYGZhy{}3.842 2.90536
\PYGZhy{}28.263 2.098 \PYGZhy{}3.87 2.2508
\PYGZhy{}31.114 2.099 \PYGZhy{}3.887 1.82821
\PYGZhy{}33.47 2.098 \PYGZhy{}3.9 1.53985
\PYGZhy{}35.479 2.097 \PYGZhy{}3.909 1.33006
\PYGZhy{}37.228 2.096 \PYGZhy{}3.917 1.1687
\PYGZhy{}38.776 2.095 \PYGZhy{}3.924 1.03934
\PYGZhy{}40.161 2.095 \PYGZhy{}3.93 0.93264
\PYGZhy{}41.409 2.094 \PYGZhy{}3.935 0.84296
\PYGZhy{}42.542 2.094 \PYGZhy{}3.94 0.76666
\PYGZhy{}43.574 2.094 \PYGZhy{}3.945 0.70124
\PYGZhy{}44.518 2.094 \PYGZhy{}3.949 0.64485
\PYGZhy{}45.384 2.095 \PYGZhy{}3.953 0.59609
\PYGZhy{}46.18 2.095 \PYGZhy{}3.956 0.55383
\PYGZhy{}46.914 2.096 \PYGZhy{}3.96 0.51713
\PYGZhy{}47.592 2.097 \PYGZhy{}3.963 0.48523
\PYGZhy{}48.218 2.098 \PYGZhy{}3.967 0.45745
\PYGZhy{}48.799 2.099 \PYGZhy{}3.97 0.43326
\PYGZhy{}49.338 2.1 \PYGZhy{}3.973 0.41215
\PYGZhy{}49.839 2.101 \PYGZhy{}3.975 0.39371
\PYGZhy{}50.305 2.102 \PYGZhy{}3.978 0.3776
\PYGZhy{}50.739 2.103 \PYGZhy{}3.98 0.36349
\PYGZhy{}51.145 2.104 \PYGZhy{}3.983 0.35112
\PYGZhy{}51.523 2.105 \PYGZhy{}3.985 0.34026
\PYGZhy{}51.878 2.106 \PYGZhy{}3.987 0.33071
\PYGZhy{}52.209 2.107 \PYGZhy{}3.989 0.3223
\PYGZhy{}52.52 2.108 \PYGZhy{}3.991 0.31489
\PYGZhy{}52.812 2.109 \PYGZhy{}3.993 0.30834
\PYGZhy{}53.086 2.111 \PYGZhy{}3.995 0.30255
\PYGZhy{}53.343 2.112 \PYGZhy{}3.996 0.29741
\PYGZhy{}53.585 2.112 \PYGZhy{}3.998 0.29285
\PYGZhy{}53.813 2.113 \PYGZhy{}3.999 0.28879
\PYGZhy{}54.028 2.114 \PYGZhy{}4.001 0.28518
\PYGZhy{}54.23 2.115 \PYGZhy{}4.002 0.28195
\PYGZhy{}54.421 2.116 \PYGZhy{}4.003 0.27907
\PYGZhy{}54.601 2.117 \PYGZhy{}4.004 0.27649
\PYGZhy{}54.771 2.118 \PYGZhy{}4.006 0.27417
\PYGZhy{}54.932 2.118 \PYGZhy{}4.007 0.27209
\PYGZhy{}55.084 2.119 \PYGZhy{}4.008 0.27022
\PYGZhy{}55.227 2.12 \PYGZhy{}4.008 0.26853
\PYGZhy{}55.363 2.12 \PYGZhy{}4.009 0.267
\PYGZhy{}55.491 2.121 \PYGZhy{}4.01 0.26562
\PYGZhy{}55.612 2.121 \PYGZhy{}4.011 0.26437
\PYGZhy{}55.727 2.122 \PYGZhy{}4.012 0.26324
\PYGZhy{}55.836 2.123 \PYGZhy{}4.012 0.26221
\PYGZhy{}55.939 2.123 \PYGZhy{}4.013 0.26127
\PYGZhy{}56.037 2.123 \PYGZhy{}4.014 0.26042
\PYGZhy{}56.13 2.124 \PYGZhy{}4.014 0.25964
\PYGZhy{}56.217 2.124 \PYGZhy{}4.015 0.25893
\PYGZhy{}56.3 2.125 \PYGZhy{}4.015 0.25827
\PYGZhy{}56.379 2.125 \PYGZhy{}4.016 0.25768
\PYGZhy{}56.454 2.125 \PYGZhy{}4.016 0.25713
\PYGZhy{}56.525 2.126 \PYGZhy{}4.017 0.25662
\PYGZhy{}56.592 2.126 \PYGZhy{}4.017 0.25616
\PYGZhy{}56.656 2.126 \PYGZhy{}4.018 0.25573
\PYGZhy{}56.717 2.127 \PYGZhy{}4.018 0.25534
\PYGZhy{}56.774 2.127 \PYGZhy{}4.018 0.25498
\PYGZhy{}56.829 2.127 \PYGZhy{}4.019 0.25464
\PYGZhy{}56.881 2.128 \PYGZhy{}4.019 0.25433
\PYGZhy{}56.93 2.128 \PYGZhy{}4.019 0.25404
\PYGZhy{}56.977 2.128 \PYGZhy{}4.02 0.25378
\PYGZhy{}57.021 2.128 \PYGZhy{}4.02 0.25353
\PYGZhy{}57.063 2.128 \PYGZhy{}4.02 0.2533
\PYGZhy{}57.103 2.129 \PYGZhy{}4.02 0.25309
\PYGZhy{}57.141 2.129 \PYGZhy{}4.021 0.25289
\PYGZhy{}57.177 2.129 \PYGZhy{}4.021 0.2527
\PYGZhy{}57.211 2.129 \PYGZhy{}4.021 0.25253
\PYGZhy{}57.244 2.129 \PYGZhy{}4.021 0.25237
\PYGZhy{}57.275 2.13 \PYGZhy{}4.021 0.25222
\PYGZhy{}57.304 2.13 \PYGZhy{}4.022 0.25208
\PYGZhy{}57.332 2.13 \PYGZhy{}4.022 0.25195
\PYGZhy{}57.359 2.13 \PYGZhy{}4.022 0.25182
\PYGZhy{}57.384 2.13 \PYGZhy{}4.022 0.25171
\PYGZhy{}57.408 2.13 \PYGZhy{}4.022 0.2516
\PYGZhy{}57.431 2.13 \PYGZhy{}4.022 0.2515
\PYGZhy{}57.452 2.13 \PYGZhy{}4.022 0.25141
\PYGZhy{}57.473 2.13 \PYGZhy{}4.023 0.25132
\PYGZhy{}57.493 2.131 \PYGZhy{}4.023 0.25123
\PYGZhy{}57.511 2.131 \PYGZhy{}4.023 0.25115
\PYGZhy{}57.529 2.131 \PYGZhy{}4.023 0.25108
\PYGZhy{}57.546 2.131 \PYGZhy{}4.023 0.25101
\PYGZhy{}57.562 2.131 \PYGZhy{}4.023 0.25095
\PYGZhy{}57.577 2.131 \PYGZhy{}4.023 0.25088
\PYGZhy{}57.591 2.131 \PYGZhy{}4.023 0.25083
\PYGZhy{}57.605 2.131 \PYGZhy{}4.023 0.25077
\PYGZhy{}57.618 2.131 \PYGZhy{}4.023 0.25072
\PYGZhy{}57.63 2.131 \PYGZhy{}4.024 0.25067
\PYGZhy{}57.642 2.131 \PYGZhy{}4.024 0.25063
\PYGZhy{}57.653 2.131 \PYGZhy{}4.024 0.25058
\PYGZhy{}57.664 2.131 \PYGZhy{}4.024 0.25054
\PYGZhy{}57.674 2.131 \PYGZhy{}4.024 0.2505
\PYGZhy{}57.684 2.132 \PYGZhy{}4.024 0.25047
\PYGZhy{}57.693 2.132 \PYGZhy{}4.024 0.25043
\PYGZhy{}57.702 2.132 \PYGZhy{}4.024 0.2504
\PYGZhy{}57.71 2.132 \PYGZhy{}4.024 0.25037
\PYGZhy{}57.718 2.132 \PYGZhy{}4.024 0.25034
\PYGZhy{}57.726 2.132 \PYGZhy{}4.024 0.25031
\PYGZhy{}57.733 2.132 \PYGZhy{}4.024 0.25029
\PYGZhy{}57.739 2.132 \PYGZhy{}4.024 0.25026
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Obserwujemy, zgodnie z oczekiwaniami, stopniowy spadek błędu w miarę postępu symulacji. Ponieważ funkcja błędu ma teraz trzy niezależne argumenty, nie można jej narysować w dwóch wymiarach. Możemy jednak pokazać jej rzut, np. dla ustalonej wartości \( w_0 \), co robimy poniżej:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{fig}\PYG{p}{,} \PYG{n}{ax} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mf}{3.7}\PYG{p}{,}\PYG{l+m+mf}{3.7}\PYG{p}{)}\PYG{p}{,}\PYG{n}{dpi}\PYG{o}{=}\PYG{l+m+mi}{120}\PYG{p}{)}

\PYG{n}{delta} \PYG{o}{=} \PYG{l+m+mf}{0.5}
\PYG{n}{ran}\PYG{o}{=}\PYG{l+m+mi}{40} 
\PYG{n}{r1} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{n}{w1\PYGZus{}o}\PYG{o}{\PYGZhy{}}\PYG{n}{ran}\PYG{p}{,} \PYG{n}{w1\PYGZus{}o}\PYG{o}{+}\PYG{n}{ran}\PYG{p}{,} \PYG{n}{delta}\PYG{p}{)} 
\PYG{n}{r2} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{n}{w2\PYGZus{}o}\PYG{o}{\PYGZhy{}}\PYG{n}{ran}\PYG{p}{,} \PYG{n}{w2\PYGZus{}o}\PYG{o}{+}\PYG{n}{ran}\PYG{p}{,} \PYG{n}{delta}\PYG{p}{)} 
\PYG{n}{X}\PYG{p}{,} \PYG{n}{Y} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{meshgrid}\PYG{p}{(}\PYG{n}{r1}\PYG{p}{,} \PYG{n}{r2}\PYG{p}{)} 

\PYG{n}{Z}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{n}{error}\PYG{p}{(}\PYG{n}{w0\PYGZus{}o}\PYG{p}{,}\PYG{n}{r1}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{,}\PYG{n}{r2}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{p}{,}\PYG{n}{samp2}\PYG{p}{,}\PYG{n}{func}\PYG{o}{.}\PYG{n}{sig}\PYG{p}{)} 
             \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{r1}\PYG{p}{)}\PYG{p}{)}\PYG{p}{]} \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{r2}\PYG{p}{)}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}  

\PYG{n}{CS} \PYG{o}{=} \PYG{n}{ax}\PYG{o}{.}\PYG{n}{contour}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,} \PYG{n}{Y}\PYG{p}{,} \PYG{n}{Z}\PYG{p}{,}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{5}\PYG{p}{,}\PYG{l+m+mi}{10}\PYG{p}{,}\PYG{l+m+mi}{15}\PYG{p}{,}\PYG{l+m+mi}{20}\PYG{p}{,}\PYG{l+m+mi}{25}\PYG{p}{,}\PYG{l+m+mi}{30}\PYG{p}{,}\PYG{l+m+mi}{35}\PYG{p}{,}\PYG{l+m+mi}{40}\PYG{p}{,}\PYG{l+m+mi}{45}\PYG{p}{,}\PYG{l+m+mi}{50}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{clabel}\PYG{p}{(}\PYG{n}{CS}\PYG{p}{,} \PYG{n}{inline}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{fmt}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+si}{\PYGZpc{}1.0f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{9}\PYG{p}{)}

\PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Error function for \PYGZdl{}w\PYGZus{}0\PYGZdl{}=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{o}{+}\PYG{n+nb}{str}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(}\PYG{n}{w0\PYGZus{}o}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}aspect}\PYG{p}{(}\PYG{n}{aspect}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZdl{}w\PYGZus{}1\PYGZdl{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZdl{}w\PYGZus{}2\PYGZdl{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}

\PYG{n}{ax}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{w1\PYGZus{}o}\PYG{p}{,} \PYG{n}{w2\PYGZus{}o}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{20}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{red}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{found minimum}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} our found optimal point}

\PYG{n}{ax}\PYG{o}{.}\PYG{n}{legend}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{backprop_35_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\begin{sphinxadmonition}{note}{Informacja:}
\sphinxAtStartPar
W miarę jak wykonujemy coraz więcej iteracji, zauważamy, że wielkość wag rośnie, podczas gdy błąd naturalnie się zmniejsza. Powodem jest to, że nasza próbka danych jest separowalna, więc w przypadku użycia schodkowej funkcji aktywacji możliwe jest rozdzielenie próbki linią podziału i zejście z błędem aż do zera. W przypadku sigmoidu, zawsze istnieje pewien (niewielki) wkład do błędu, ponieważ wartości funkcji mieszczą się w sposób ciągły w przedziale (0,1). Jak omówiliśmy powyżej, w sigmoidzie, którego argumentem jest \( (w_0 + w_1 x_1 + w_2 x_2) / T\), zwiększanie wag jest równoznaczne ze zmniejszaniem temperatury \(T\). W moare postępu symulacji sigmoid zbliża się zatem do funkcji schodkowej, a błąd dąży do zera. Zachowanie to jest widoczne w powyższych symulacjach.
\end{sphinxadmonition}


\section{Najstromszy spadek}
\label{\detokenize{docs/backprop:najstromszy-spadek}}
\sphinxAtStartPar
Powodem dla powyższych symulacji było doprowadzenie czytelnika do wniosku, że zagadnienie optymalizacji wag można sprowadzić do ogólnego problemu minimalizacji funkcji wielu zmiennych. Jest to standardowy (choć na ogół trudny) problem w analizie matematycznej i metodach numerycznych. Problemy związane ze znalezieniem minimum funkcji wielu zmiennych są dobrze znane:
\begin{itemize}
\item {} 
\sphinxAtStartPar
mogą istnieć minima lokalne, dlatego znalezienie minimum globalnego może być bardzo trudne;

\item {} 
\sphinxAtStartPar
minimum może być w nieskończoności (czyli matematycznie nie istnieć);

\item {} 
\sphinxAtStartPar
Funkcja wokół minimum może być bardzo płaska, tj. jej gradient jest bardzo mały. Wówczas znajdowanie minimum z pomocą metod gradientowych jest bardzo powolne;

\end{itemize}

\sphinxAtStartPar
Ogólnie rzecz biorąc, minimalizacja numeryczna funkcji to sztuka! Opracowano tu wiele metod, a właściwy dobór do danego problemu ma kluczowe znaczenie dla sukcesu. Poniżej zastosujemy najprostszy wariant, tzw. metodę \sphinxstylestrong{najstromszego spadku}.

\sphinxAtStartPar
Dla różniczkowalnej funkcji wielu zmiennych \( F (z_1, z_2, ..., z_n) \), lokalnie najbardziej strome nachylenie jest okreslone przez minus gradient funkcji \( F \),
\$\(-\left (\frac{\partial F}{\partial z_1}, \frac{\partial F}{\partial z_2}, ..., 
\frac{\partial F}{\partial z_n} \right ), \)\$

\sphinxAtStartPar
gdzie pochodne cząstkowe definiuje się jako granice
\begin{equation*}
\begin{split}\frac{\partial F}{\partial z_1} = \lim _ {\Delta \to 0} \frac {F (z_1 + \Delta, z_2, ..., z_n) -F (z_1, z_2, ..., z_n)} { \Delta } \end{split}
\end{equation*}
\sphinxAtStartPar
i podobnie dla pozostałych \( z_i \).

\sphinxAtStartPar
Metoda znajdowania minimum funkcji poprzez najstromszy spadek zadana jest przez algorytm iteracyjny, w którym aktualizujemy współrzędne (wyszukiwanego minimum) w każdym kroku iteracji \(m\) (górny wskaźnik) w nastepujacy sposób:
\begin{equation*}
\begin{split}z_{i}^{(m+1)} = z_i^{(m)} - \epsilon  \, \frac{\partial F}{\partial z_i}. \end{split}
\end{equation*}
\sphinxAtStartPar
W naszym zagadnieniu potrzebujemy zminimalzować funcję błedu
\begin{equation*}
\begin{split}E(w_0,w_1,w_2)= \sum_p [y_o^{(p)}-y_t^{(p)}]^2=\sum_p [\sigma(s^{(p)})-y_t^{(p)}]^2=\sum_p [\sigma(w_0  x_0^{(p)}+w_1 x_1^{(p)} +w_2 x_2^{(p)})-y_t^{(p)}]^2. \end{split}
\end{equation*}
\sphinxAtStartPar
Aby obliczyć pochodne, stosujemy \sphinxstylestrong{twierdzenie o pochodnej funkcji złożonej}.

\begin{sphinxadmonition}{note}{Tw. o pochodnej funkcji złożonej}

\sphinxAtStartPar
Dla funkcji złożonej

\sphinxAtStartPar
\([f(g(x))]' = f'(g(x)) g'(x)\).

\sphinxAtStartPar
Dla złożenia większej liczby funkcji \([f(g(h(x)))]' = f'(g(h(x))) \,g'(h(x)) \,h'(x)\) itp.
\end{sphinxadmonition}

\sphinxAtStartPar
Prowadzi to do wzoru
\begin{equation*}
\begin{split} \frac{\partial E}{\partial w_i} = \sum_p 2[\sigma(s^{(p)})-y_t^{(p)}]\, \sigma'(s^{(p)}) \,x_i^{(p)} = \sum_p 2[\sigma(s^{(p)})-y_t^{(p)}]\, \sigma(s^{(p)})\, [1-\sigma(s^{(p)})] \,x_i^{(p)}\end{split}
\end{equation*}
\sphinxAtStartPar
(pochodna funkcji kwadratowej \( \times \) pochodna sigmoidu \( \times \) pochodna \( s ^ {(p)} \)), gdzie w ostatniej równości użyliśmy specjalnej własności pochodnej sigmoidu. Metoda najstromszego spadku aktualizuje zatem wagi w następujący sposób:
\begin{equation*}
\begin{split}w_i \to w_i - \varepsilon (y_o^{(p)} -y_t^{(p)}) y_o^{(p)} (1-y_o^{(p)}) x_i.\end{split}
\end{equation*}
\sphinxAtStartPar
Zauważmy, że aktualizacja zawsze występuje, ponieważ odpowiedź \( y_o^ {(p)} \) nigdy nie jest ściśle równa 0 lub 1, podczas gdy
prawdziwa wartość (etykieta) \( y_t ^ {(p)} \) wynosi 0 lub 1.

\sphinxAtStartPar
Ponieważ \( y_o ^ {(p)} (1-y_o ^ {(p)}) = \sigma (s ^ {(p)}) [1- \sigma (s ^ {(p)})] \) jest istotnie różne od zera tylko w okolicy \( s ^ {(p)} = 0\) (patrz wcześniejszy wykres pochodnej sigmoidu), znacząca aktualizacja następuje tylko w pobliżu progu. To cecha jest odpowiednia, ponieważ problemy z błędną klasyfikacją zdarzają się właśnie w pobliżu linii podziału.

\begin{sphinxadmonition}{note}{Informacja:}
\sphinxAtStartPar
Dla porównania, wcześniejszy algorytm perceptronu jest strukturalnie bardzo podobny,
\begin{equation*}
\begin{split}w_i \to w_i - \varepsilon \,(y_o^{(p)} - y_t^{(p)}) \, x_i,\end{split}
\end{equation*}
\sphinxAtStartPar
ale tutaj aktualizacja następuje dla wszystkich punktów próbki, a nie tylko tych w pobliżu linii podziału.
\end{sphinxadmonition}

\sphinxAtStartPar
Kod algorytmu uczenia naszego perceptronu metodą najstromszyego spadku jest następujący:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{teach\PYGZus{}sd}\PYG{p}{(}\PYG{n}{sample}\PYG{p}{,} \PYG{n}{eps}\PYG{p}{,} \PYG{n}{w\PYGZus{}in}\PYG{p}{)}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} Steepest descent for the perceptron}
    
    \PYG{p}{[}\PYG{p}{[}\PYG{n}{w0}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{n}{w1}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{n}{w2}\PYG{p}{]}\PYG{p}{]}\PYG{o}{=}\PYG{n}{w\PYGZus{}in}              \PYG{c+c1}{\PYGZsh{} initial weights}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{sample}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:}       \PYG{c+c1}{\PYGZsh{} loop over the data sample}
        \PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{10}\PYG{p}{)}\PYG{p}{:}            \PYG{c+c1}{\PYGZsh{} repeat 10 times }
            
            \PYG{n}{yo}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{sig}\PYG{p}{(}\PYG{n}{w0}\PYG{o}{+}\PYG{n}{w1}\PYG{o}{*}\PYG{n}{sample}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{+}\PYG{n}{w2}\PYG{o}{*}\PYG{n}{sample}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} obtained answer for pont i}

            \PYG{n}{w0}\PYG{o}{=}\PYG{n}{w0}\PYG{o}{+}\PYG{n}{eps}\PYG{o}{*}\PYG{p}{(}\PYG{n}{sample}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{n}{yo}\PYG{p}{)}\PYG{o}{*}\PYG{n}{yo}\PYG{o}{*}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{o}{\PYGZhy{}}\PYG{n}{yo}\PYG{p}{)}\PYG{o}{*}\PYG{l+m+mi}{1}            \PYG{c+c1}{\PYGZsh{} update of weights}
            \PYG{n}{w1}\PYG{o}{=}\PYG{n}{w1}\PYG{o}{+}\PYG{n}{eps}\PYG{o}{*}\PYG{p}{(}\PYG{n}{sample}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{n}{yo}\PYG{p}{)}\PYG{o}{*}\PYG{n}{yo}\PYG{o}{*}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{o}{\PYGZhy{}}\PYG{n}{yo}\PYG{p}{)}\PYG{o}{*}\PYG{n}{sample}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}
            \PYG{n}{w2}\PYG{o}{=}\PYG{n}{w2}\PYG{o}{+}\PYG{n}{eps}\PYG{o}{*}\PYG{p}{(}\PYG{n}{sample}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{n}{yo}\PYG{p}{)}\PYG{o}{*}\PYG{n}{yo}\PYG{o}{*}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{o}{\PYGZhy{}}\PYG{n}{yo}\PYG{p}{)}\PYG{o}{*}\PYG{n}{sample}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}
    \PYG{k}{return} \PYG{p}{[}\PYG{p}{[}\PYG{n}{w0}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{n}{w1}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{n}{w2}\PYG{p}{]}\PYG{p}{]}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
Jego wydajność jest podobna do oryginalnego algorytmu perceptronu badanego powyżej:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{weights}\PYG{o}{=}\PYG{p}{[}\PYG{p}{[}\PYG{n}{func}\PYG{o}{.}\PYG{n}{rn}\PYG{p}{(}\PYG{p}{)}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{n}{func}\PYG{o}{.}\PYG{n}{rn}\PYG{p}{(}\PYG{p}{)}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{n}{func}\PYG{o}{.}\PYG{n}{rn}\PYG{p}{(}\PYG{p}{)}\PYG{p}{]}\PYG{p}{]}      \PYG{c+c1}{\PYGZsh{} random weights from [\PYGZhy{}0.5,0.5]}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{   w0   w1/w0  w2/w0 error}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}   \PYG{c+c1}{\PYGZsh{} header}

\PYG{n}{eps}\PYG{o}{=}\PYG{l+m+mf}{0.7}                       \PYG{c+c1}{\PYGZsh{} initial learning speed}
\PYG{k}{for} \PYG{n}{r} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{1000}\PYG{p}{)}\PYG{p}{:}         \PYG{c+c1}{\PYGZsh{} rounds}
    \PYG{n}{eps}\PYG{o}{=}\PYG{l+m+mf}{0.9995}\PYG{o}{*}\PYG{n}{eps}            \PYG{c+c1}{\PYGZsh{} decrease learning speed}
    \PYG{n}{weights}\PYG{o}{=}\PYG{n}{teach\PYGZus{}sd}\PYG{p}{(}\PYG{n}{samp2}\PYG{p}{,}\PYG{n}{eps}\PYG{p}{,}\PYG{n}{weights}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} update weights}
    \PYG{k}{if} \PYG{n}{r}\PYG{o}{\PYGZpc{}}\PYG{k}{100}==99:
        \PYG{n}{w0\PYGZus{}o}\PYG{o}{=}\PYG{n}{weights}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}               \PYG{c+c1}{\PYGZsh{} updated weights }
        \PYG{n}{w1\PYGZus{}o}\PYG{o}{=}\PYG{n}{weights}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]} 
        \PYG{n}{w2\PYGZus{}o}\PYG{o}{=}\PYG{n}{weights}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]} 
        \PYG{n}{v1\PYGZus{}o}\PYG{o}{=}\PYG{n}{w1\PYGZus{}o}\PYG{o}{/}\PYG{n}{w0\PYGZus{}o}
        \PYG{n}{v2\PYGZus{}o}\PYG{o}{=}\PYG{n}{w2\PYGZus{}o}\PYG{o}{/}\PYG{n}{w0\PYGZus{}o}
        \PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(}\PYG{n}{w0\PYGZus{}o}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{,}\PYG{n}{np}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(}\PYG{n}{v1\PYGZus{}o}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{,}\PYG{n}{np}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(}\PYG{n}{v2\PYGZus{}o}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{,}
              \PYG{n}{np}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(}\PYG{n}{error}\PYG{p}{(}\PYG{n}{w0\PYGZus{}o}\PYG{p}{,} \PYG{n}{w0\PYGZus{}o}\PYG{o}{*}\PYG{n}{v1\PYGZus{}o}\PYG{p}{,} \PYG{n}{w0\PYGZus{}o}\PYG{o}{*}\PYG{n}{v2\PYGZus{}o}\PYG{p}{,} \PYG{n}{samp2}\PYG{p}{,} \PYG{n}{func}\PYG{o}{.}\PYG{n}{sig}\PYG{p}{)}\PYG{p}{,}\PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{)}                                          
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
   w0   w1/w0  w2/w0 error
\PYGZhy{}8.874 2.137 \PYGZhy{}3.991 2.54579
\PYGZhy{}11.282 2.137 \PYGZhy{}3.978 2.18976
\PYGZhy{}13.014 2.136 \PYGZhy{}3.978 1.94467
\PYGZhy{}14.391 2.133 \PYGZhy{}3.98 1.75955
\PYGZhy{}15.538 2.131 \PYGZhy{}3.982 1.61477
\PYGZhy{}16.52 2.128 \PYGZhy{}3.985 1.49897
\PYGZhy{}17.377 2.126 \PYGZhy{}3.987 1.4047
\PYGZhy{}18.135 2.124 \PYGZhy{}3.99 1.32683
\PYGZhy{}18.813 2.123 \PYGZhy{}3.992 1.26167
\PYGZhy{}19.423 2.122 \PYGZhy{}3.994 1.20652
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Podsumowując dotychczasowy materiał, wykazaliśmy, że można skutecznie uczyć jednowarstwy perceptron (pojedynczy neuronu MCP) za pomocą metody najstromszego spadku, minimalizując funkcję błędu generowaną przez badaną próbkę. W następnym podrozdziale uogólnimy ten pomysł na dowolny wielowarstwową sieć typu feed\sphinxhyphen{}forward.


\section{Algorytm propagacji wstecznej (backprop)}
\label{\detokenize{docs/backprop:algorytm-propagacji-wstecznej-backprop}}\label{\detokenize{docs/backprop:bpa-lab}}
\sphinxAtStartPar
Materiał tego podrozdziału jest absolutnie \sphinxstylestrong{kluczowy} dla zrozumienia idei uczenia sieci neuronowych poprzez uczenie nadzorowane. Jednocześnie dla czytelnika mniej zaznajomionego z analizą matematyczną może być dość trudny, ponieważ pojawiają się wyprowadzenia i wzory z bogatą notacją. Nie udało się jednak znaleźć sposobu na przedstawienie materiału w prostszy sposób niż poniżej, z jednoczesnym zachowaniem niezbędnego rygoru.

\begin{sphinxadmonition}{note}{Informacja:}
\sphinxAtStartPar
Formuły, które wyprowadzamy tutaj krok po kroku, stanowią słynny \sphinxstylestrong{algorytm wstecznej propagacji (backprop)} {[}\hyperlink{cite.docs/conclusion:id12}{BH69}{]} dla aktualizacji wag perceptronu wielowarstwowego. Wykorzystujemy tylko dwa podstawowe fakty:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{tw. o pochodnej funkcji złożonej} do obliczania pochodnej, oraz

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{metodę najstromszego spadku}, wyjaśnioną w poprzednim podrozdziale.

\end{itemize}
\end{sphinxadmonition}

\sphinxAtStartPar
Rozważmy perceptron z dowolną liczbą warstw neuronowych, \(l\). Neurony w warstwach pośrednich \(j=1,\dots,l-1\) są ponumerowane odpowiednimi wskaźnikami \(\alpha_j=0,\dots,n_j\), gdzie 0 oznacza węzeł progowy. W warstwie wyjściowej, nie zawierającej węzła progowego, wskaźnik przyjmuje wartości \(\alpha_l=1,\dots,n_l\). Na przykład sieć z wykresu poniżej ma
\begin{equation*}
\begin{split}l=4, \; \; \alpha_1=0,\dots,4, \;\; \alpha_2=0,\dots,5, \;\; \alpha_3=0,\dots,3, \;\; \alpha_4=1,\dots,2,\end{split}
\end{equation*}
\sphinxAtStartPar
ze wskaźnikami w każdej warstwie liczonymi od dołu.

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{backprop_50_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
Funkcja błędu to suma po punktach próbki treningowej oraz dodatkowo po węzłach w warstwie wyjściowej:
\begin{equation*}
\begin{split}
E(\{w\})=\sum_p \sum_{\alpha_l=1}^{n_l} \left[ y_{o,{\alpha_l}}^{(p)}(\{w\})-y_{t,{\alpha_l}}^{(p)}\right]^2,
\end{split}
\end{equation*}
\sphinxAtStartPar
gdzie \( \{w \} \) reprezentują wszystkie wagi sieci.
Pojedynczy wkład punktu \(p\) do \(E\), oznaczony jako \(e\), to
suma po wszystkich neuronach w warstwie wyjściowej:
\begin{equation*}
\begin{split}
e(\{w\})= \sum_{{\alpha_l}=1}^{n_l}\left[ y_{o,{\alpha_l}}-y_{t,{\alpha_l}}\right]^2. 
\end{split}
\end{equation*}
\sphinxAtStartPar
Dla zwięzłości, opuściliśmy górny wskaźnik \((p)\).
Dla neuronu \(\alpha_j\) w warstwie \(j\) sygnałem wejściowym jest
\begin{equation*}
\begin{split}
s_{\alpha_j}^{j}=\sum_{\alpha_{j-1}=0}^{n_{j-1}} x_{\alpha_{j-1}}^{j-1} w_{\alpha_{j-1} \alpha_j}^{j}.
\end{split}
\end{equation*}
\sphinxAtStartPar
Sygnały w warstwie wyjściowej mają postać
\begin{equation*}
\begin{split}
y_{o,{\alpha_l}}=f\left( s_{\alpha_l}^{l} \right)
\end{split}
\end{equation*}
\sphinxAtStartPar
natomiast sygnały wyjściowe w warstwach pośrednich \(j=1,\dots,l-1\) to
\begin{equation*}
\begin{split}
x_{\alpha_j}^{j}=f \left ( s_{\alpha_j}^{j}\right ),\;\;\;\alpha_{j}=1,\dots,n_j, \;\; \; {\rm i} \;\;\; x_0^{j}=1,
\end{split}
\end{equation*}
\sphinxAtStartPar
z węzłem progowym mającym wartość 1.

\sphinxAtStartPar
Kolejne podstawienia powyższych formuł do \(e\) są następujące:

\sphinxAtStartPar
\(e = \sum_{{\alpha_l}=1}^{n_l}\left( y_{o,{\alpha_l}}-y_{t,{\alpha_l}}\right)^2\)

\sphinxAtStartPar
\(=\sum_{{\alpha_l}=1}^{n_l} \left( f \left (\sum_{\alpha_{l-1}=0}^{n_{l-1}} x_{\alpha_{l-1}}^{l-1} w_{\alpha_{l-1} {\alpha_l}}^{l} \right )-y_{t,{\alpha_l}} \right)^2\)

\sphinxAtStartPar
\(=\sum_{{\alpha_l}=1}^{n_l} \left( 
f \left (\sum_{\alpha_{l-1}=1}^{n_{l-1}} f \left( \sum_{\alpha_{l-2}=0}^{n_{l-2}} x_{\alpha_{l-2}}^{l-2} w_{\alpha_{l-2} \alpha_{l-1}}^{l-1}\right) w_{\alpha_{l-1} {\alpha_l}}^{l} + x_0^{l-1} w_{0 \gamma}^{l} \right)-y_{t,{\alpha_l}} \right)^2\)

\sphinxAtStartPar
\(=\sum_{{\alpha_l}=1}^{n_l} \left( 
f \left (\sum_{\alpha_{l-1}=1}^{n_{l-1}} f\left( 
\sum_{\alpha_{l-2}=1}^{n_{l-2}} f\left( \sum_{\alpha_{l-3}=0}^{n_{l-3}} x_{\alpha_{l-3}}^{l-3} w_{\alpha_{l-3} \alpha_{l-2}}^{l-2}\right) w_{\alpha_{l-2} \alpha_{l-1}}^{l-1} + 
x_{0}^{l-2} w_{0 \alpha_{l-1}}^{l-1}
 \right)  w_{\alpha_{l-1} {\alpha_l}}^{l} + x_0^{l-1} w_{0 {\alpha_l}}^{l} \right)-y_{t,{\alpha_l}} \right)^2\)

\sphinxAtStartPar
\(=\sum_{{\alpha_l}=1}^{n_l} \left( 
f \left (\sum_{\alpha_{l-1}=1}^{n_{l-1}} f\left( 
\dots f\left( \sum_{\alpha_{0}=0}^{n_{0}} x_{\alpha_{0}}^{0} w_{\alpha_{0} \alpha_{1}}^{1}\right) w_{\alpha_{1} \alpha_{2}}^{2} + 
x_{0}^{1} w_{0 \alpha_{2}}^{2} \dots
 \right)  w_{\alpha_{l-1} {\alpha_l}}^{l} + x_0^{l-1} w_{0 {\alpha_l}}^{l} \right)-y_{t,{\alpha_l}} \right)^2\)

\sphinxAtStartPar
Obliczając kolejne pochodne względem wag idąc wstecz, tj. od \(j=l\) do 1, otrzymujemy (patrz ćwiczenia)
\begin{equation*}
\begin{split}
\frac{\partial e}{\partial w^j_{\alpha_{j-1} \alpha_j}} = x_{\alpha_{j-1}}^{j-1} D_{\alpha_j}^{j} , \;\;\; \alpha_{j-1}=0,\dots,n_{j-1}, \;\; \alpha_{j}=1,\dots,n_{j},
\end{split}
\end{equation*}
\sphinxAtStartPar
gdzie

\sphinxAtStartPar
\(D_{\alpha_l}^{l}=2 (y_{o,\alpha_l}-y_{t,\alpha_l})\, f'(s_{\alpha_l}^{l})\),

\sphinxAtStartPar
\(D_{\alpha_j}^{j}= \sum_{\alpha_{j+1}} D_{\alpha_{j+1}}^{j+1}\, w_{\alpha_j \alpha_{j+1}}^{j+1} \, f'(s_{\alpha_j}^{j}), ~~~~ j=l-1,l-2,\dots,1\).

\sphinxAtStartPar
Ostatnie wyrażenie to rekurencja wstecz. Zauważamy, że aby uzyskać \(D^j\), potrzebujemy \(D^{j+1}\), które uzyskaliśmy już w poprzednim kroku, oraz sygnał \(s^j\), który znamy z propagacji sygnału do przodu. Ta rekurencja prowadzi do uproszczenia obliczania pochodnych i aktualizacji wag.

\sphinxAtStartPar
Przy najstromszym spadku wagi są aktualizowane jako
\begin{equation*}
\begin{split} w^j_{\alpha_{j-1} \alpha_j} \to  w^j_{\alpha_{j-1} \alpha_j} -\varepsilon x_{\alpha_{j-1}}^{j-1} D_{\alpha_j}^{j}, \end{split}
\end{equation*}
\sphinxAtStartPar
W przypadku sigmoidu możemy użyć
\begin{equation*}
\begin{split}
\sigma'(s_A^{(i)})=\sigma'(s_A^{(i)}) (1-\sigma'(s_A^{(i)})) =x_A^{(i)}(1-x_A^{(i)}).
\end{split}
\end{equation*}
\begin{sphinxadmonition}{note}{Informacja:}
\sphinxAtStartPar
Powyższe formuły wyjaśniają nazwę \sphinxstylestrong{propagacja wsteczna}, ponieważ w aktualizacji wag zaczynamy od ostatniej warstwy, a następnie posuwamy się rekurencyjnie do początku sieci. Na każdym kroku potrzebujemy tylko sygnału w danej warstwie i właściwości kolejnej warstwy! Te cechy wynikają z
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
charakteru feed\sphinxhyphen{}forward sieci oraz

\item {} 
\sphinxAtStartPar
tw. o pochodnej funkcji złożonej.

\end{enumerate}
\end{sphinxadmonition}

\begin{sphinxadmonition}{important}{Ważne:}
\sphinxAtStartPar
Praktyczne znaczenie cofania się warstwa po warstwie polega na tym, że w jednym kroku aktualizuje się znacznie mniej wag: tylko te, które wchodzą do danej warstwy, a nie wszystkie naraz. Ma to znaczenie dla zbieżności metody najstromszego spadku, zwłaszcza dla sieci głębokich (o wielu warswach).
\end{sphinxadmonition}

\sphinxAtStartPar
Jeżeli funkcje aktywacji są różne w różnych warstwach (oznaczamy je \(f_j\) dla warstwy \(j\)), to zachodzi oczywista modyfikacja:

\sphinxAtStartPar
\(D_{\alpha_l}^{l}=2 (y_{o,\alpha_l}-y_{t,\alpha_l})\, f_l'(s_{\alpha_l}^{l})\),

\sphinxAtStartPar
\(D_{\alpha_j}^{j}= \sum_{\alpha_{j+1}} D_{\alpha_{j+1}}^{j+1}\, w_{\alpha_j \alpha_{j+1}}^{j+1} \, f_j'(s_{\alpha_j}^{j}), ~~~~ j=l-1,l-2,\dots,1\).

\sphinxAtStartPar
Nie jest to rzadkie, ponieważ w wielu zastosowaniach wybiera się różne funkcje aktywacji dla warstw pośrednich i warswy wyjściowej.


\subsection{Kod dla algorytmu backprop}
\label{\detokenize{docs/backprop:kod-dla-algorytmu-backprop}}
\sphinxAtStartPar
Następnie przedstawimy prosty kod realizujący algorytm backprop. Jest to bezpośrednia implementacja wyprowadzonych powyżej formuł. W kodzie zachowujemy jak najwięcej notacji z powyższego wyprowadzenia.

\sphinxAtStartPar
Kod ma tylko 12 linijek, nie licząc komentarzy!

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{back\PYGZus{}prop}\PYG{p}{(}\PYG{n}{fe}\PYG{p}{,}\PYG{n}{la}\PYG{p}{,} \PYG{n}{p}\PYG{p}{,} \PYG{n}{ar}\PYG{p}{,} \PYG{n}{we}\PYG{p}{,} \PYG{n}{eps}\PYG{p}{,}\PYG{n}{f}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{sig}\PYG{p}{,}\PYG{n}{df}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{dsig}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    fe \PYGZhy{} array of features}
\PYG{l+s+sd}{    la \PYGZhy{} array of labels}
\PYG{l+s+sd}{    p  \PYGZhy{} index of the used data point}
\PYG{l+s+sd}{    ar \PYGZhy{} array of numbers of nodes in subsequent layers}
\PYG{l+s+sd}{    we \PYGZhy{} disctionary of weights}
\PYG{l+s+sd}{    eps \PYGZhy{} learning speed }
\PYG{l+s+sd}{    f   \PYGZhy{} activation function}
\PYG{l+s+sd}{    df  \PYGZhy{} derivaive of f}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
 
    \PYG{n}{l}\PYG{o}{=}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{)}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1} \PYG{c+c1}{\PYGZsh{} number of neuron layers (= index of the output layer)}
    \PYG{n}{nl}\PYG{o}{=}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{l}\PYG{p}{]}    \PYG{c+c1}{\PYGZsh{} number of neurons in the otput layer  }
   
    \PYG{n}{x}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{feed\PYGZus{}forward}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{,}\PYG{n}{we}\PYG{p}{,}\PYG{n}{fe}\PYG{p}{[}\PYG{n}{p}\PYG{p}{]}\PYG{p}{,}\PYG{n}{ff}\PYG{o}{=}\PYG{n}{f}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} feed\PYGZhy{}forward of point p}
   
    \PYG{c+c1}{\PYGZsh{} formulas from the derivation in a one\PYGZhy{}to\PYGZhy{}one notation:}
    
    \PYG{n}{D}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{p}{\PYGZcb{}}                 
    \PYG{n}{D}\PYG{o}{.}\PYG{n}{update}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{n}{l}\PYG{p}{:} \PYG{p}{[}\PYG{l+m+mi}{2}\PYG{o}{*}\PYG{p}{(}\PYG{n}{x}\PYG{p}{[}\PYG{n}{l}\PYG{p}{]}\PYG{p}{[}\PYG{n}{gam}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{n}{la}\PYG{p}{[}\PYG{n}{p}\PYG{p}{]}\PYG{p}{[}\PYG{n}{gam}\PYG{p}{]}\PYG{p}{)}\PYG{o}{*}
                    \PYG{n}{df}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{[}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{we}\PYG{p}{[}\PYG{n}{l}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}\PYG{p}{[}\PYG{n}{gam}\PYG{p}{]} \PYG{k}{for} \PYG{n}{gam} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{nl}\PYG{p}{)}\PYG{p}{]}\PYG{p}{\PYGZcb{}}\PYG{p}{)}   
    \PYG{n}{we}\PYG{p}{[}\PYG{n}{l}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{o}{=}\PYG{n}{eps}\PYG{o}{*}\PYG{n}{np}\PYG{o}{.}\PYG{n}{outer}\PYG{p}{(}\PYG{n}{x}\PYG{p}{[}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{D}\PYG{p}{[}\PYG{n}{l}\PYG{p}{]}\PYG{p}{)} 
    
    \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{reversed}\PYG{p}{(}\PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{n}{l}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:}           
        \PYG{n}{u}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{delete}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{we}\PYG{p}{[}\PYG{n}{j}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{D}\PYG{p}{[}\PYG{n}{j}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{)} 
        \PYG{n}{v}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{[}\PYG{n}{j}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{we}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{p}{)}          
        \PYG{n}{D}\PYG{o}{.}\PYG{n}{update}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{n}{j}\PYG{p}{:} \PYG{p}{[}\PYG{n}{u}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{o}{*}\PYG{n}{df}\PYG{p}{(}\PYG{n}{v}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{)} \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{u}\PYG{p}{)}\PYG{p}{)}\PYG{p}{]}\PYG{p}{\PYGZcb{}}\PYG{p}{)} 
        \PYG{n}{we}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{o}{=}\PYG{n}{eps}\PYG{o}{*}\PYG{n}{np}\PYG{o}{.}\PYG{n}{outer}\PYG{p}{(}\PYG{n}{x}\PYG{p}{[}\PYG{n}{j}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{D}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{p}{)}      
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}

\section{Przykład z kołem}
\label{\detokenize{docs/backprop:przyklad-z-kolem}}\label{\detokenize{docs/backprop:circ-lab}}
\sphinxAtStartPar
Kod ilustrujemy na przykładzie klasyfikatora binarnego punktów wewnątrz okręgu.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{cir}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{x1}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)}                  \PYG{c+c1}{\PYGZsh{} coordinate 1}
    \PYG{n}{x2}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)}                  \PYG{c+c1}{\PYGZsh{} coordinate 2}
    \PYG{k}{if}\PYG{p}{(}\PYG{p}{(}\PYG{n}{x1}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.5}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{o}{+}\PYG{p}{(}\PYG{n}{x2}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.5}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2} \PYG{o}{\PYGZlt{}} \PYG{l+m+mf}{0.4}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{:}  \PYG{c+c1}{\PYGZsh{} inside circle, radius 0.4, center (0.5,0.5)}
        \PYG{k}{return} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{x1}\PYG{p}{,}\PYG{n}{x2}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}
    \PYG{k}{else}\PYG{p}{:}                                  \PYG{c+c1}{\PYGZsh{} outside}
        \PYG{k}{return} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{x1}\PYG{p}{,}\PYG{n}{x2}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
Do przyszłego użytku \sphinxstylestrong{(nowa konwencja)} podzielimy próbkę na oddzielne tablice \sphinxstylestrong{cech} (dwie współrzędne) i \sphinxstylestrong{etykiet} (1, jeśli punkt znajduje się wewnątrz okręgu, 0 w przeciwnym razie):

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{sample\PYGZus{}c}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{cir}\PYG{p}{(}\PYG{p}{)} \PYG{k}{for} \PYG{n}{\PYGZus{}} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{3000}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} sample}
\PYG{n}{features\PYGZus{}c}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{delete}\PYG{p}{(}\PYG{n}{sample\PYGZus{}c}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{labels\PYGZus{}c}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{delete}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{delete}\PYG{p}{(}\PYG{n}{sample\PYGZus{}c}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mf}{2.3}\PYG{p}{,}\PYG{l+m+mf}{2.3}\PYG{p}{)}\PYG{p}{,}\PYG{n}{dpi}\PYG{o}{=}\PYG{l+m+mi}{120}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlim}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{.1}\PYG{p}{,}\PYG{l+m+mf}{1.1}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylim}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{.1}\PYG{p}{,}\PYG{l+m+mf}{1.1}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{sample\PYGZus{}c}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}\PYG{n}{sample\PYGZus{}c}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{n}{sample\PYGZus{}c}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,}
            \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{n}{cmap}\PYG{o}{=}\PYG{n}{mpl}\PYG{o}{.}\PYG{n}{cm}\PYG{o}{.}\PYG{n}{cool}\PYG{p}{,}\PYG{n}{norm}\PYG{o}{=}\PYG{n}{mpl}\PYG{o}{.}\PYG{n}{colors}\PYG{o}{.}\PYG{n}{Normalize}\PYG{p}{(}\PYG{n}{vmin}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{vmax}\PYG{o}{=}\PYG{l+m+mf}{.9}\PYG{p}{)}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZdl{}x\PYGZus{}1\PYGZdl{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZdl{}x\PYGZus{}2\PYGZdl{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{backprop_63_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Dobieramy następującą architekturę i początkowe parametry:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{arch\PYGZus{}c}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{4}\PYG{p}{,}\PYG{l+m+mi}{4}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}                  \PYG{c+c1}{\PYGZsh{} architecture}
\PYG{n}{weights}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{set\PYGZus{}ran\PYGZus{}w}\PYG{p}{(}\PYG{n}{arch\PYGZus{}c}\PYG{p}{,}\PYG{l+m+mi}{4}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} scaled random initial weights in [\PYGZhy{}2,2]}
\PYG{n}{eps}\PYG{o}{=}\PYG{l+m+mf}{.7}                            \PYG{c+c1}{\PYGZsh{} initial learning speed }
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{n}{draw}\PYG{o}{.}\PYG{n}{plot\PYGZus{}net}\PYG{p}{(}\PYG{n}{arch\PYGZus{}c}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{backprop_66_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Symulacja zabiera kilka minut.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{1000}\PYG{p}{)}\PYG{p}{:}   \PYG{c+c1}{\PYGZsh{} rounds}
    \PYG{n}{eps}\PYG{o}{=}\PYG{l+m+mf}{.995}\PYG{o}{*}\PYG{n}{eps}        \PYG{c+c1}{\PYGZsh{} decrease learning speed}
    \PYG{k}{if} \PYG{n}{k}\PYG{o}{\PYGZpc{}}\PYG{k}{100}==99: print(k+1,\PYGZsq{} \PYGZsq{},end=\PYGZsq{}\PYGZsq{})             \PYGZsh{} print progress        
    \PYG{k}{for} \PYG{n}{p} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{features\PYGZus{}c}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:}                \PYG{c+c1}{\PYGZsh{} loop over points}
        \PYG{n}{func}\PYG{o}{.}\PYG{n}{back\PYGZus{}prop}\PYG{p}{(}\PYG{n}{features\PYGZus{}c}\PYG{p}{,}\PYG{n}{labels\PYGZus{}c}\PYG{p}{,}\PYG{n}{p}\PYG{p}{,}\PYG{n}{arch\PYGZus{}c}\PYG{p}{,}\PYG{n}{weights}\PYG{p}{,}\PYG{n}{eps}\PYG{p}{,}
                       \PYG{n}{f}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{sig}\PYG{p}{,}\PYG{n}{df}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{dsig}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} backprop}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
100  200  300  400  500  600  700  800  900  1000  
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Zmniejszenie szybkości uczenia się w każdej rundzie daje końcową wartość \(\varepsilon\), która powinna być niewielka, ale nie za mała:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{eps}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
0.004657778005182377
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
(zbyt mała wartość aktualizowałaby wagi w znikomy sposób, więc dalsze rundy byłyby bezużyteczne).

\sphinxAtStartPar
Podczas gdy faza nauki była dość długa, testowanie przebiega bardzo szybko:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{test}\PYG{o}{=}\PYG{p}{[}\PYG{p}{]} 

\PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{3000}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{po}\PYG{o}{=}\PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)}\PYG{p}{]} 
    \PYG{n}{xt}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{feed\PYGZus{}forward}\PYG{p}{(}\PYG{n}{arch\PYGZus{}c}\PYG{p}{,}\PYG{n}{weights}\PYG{p}{,}\PYG{n}{po}\PYG{p}{,}\PYG{n}{ff}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{sig}\PYG{p}{)}   
    \PYG{n}{test}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{p}{[}\PYG{n}{po}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}\PYG{n}{po}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{np}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(}\PYG{n}{xt}\PYG{p}{[}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{arch\PYGZus{}c}\PYG{p}{)}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}

\PYG{n}{tt}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{n}{test}\PYG{p}{)}

\PYG{n}{fig}\PYG{o}{=}\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mf}{2.3}\PYG{p}{,}\PYG{l+m+mf}{2.3}\PYG{p}{)}\PYG{p}{,}\PYG{n}{dpi}\PYG{o}{=}\PYG{l+m+mi}{120}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} drawing the circle}
\PYG{n}{ax}\PYG{o}{=}\PYG{n}{fig}\PYG{o}{.}\PYG{n}{add\PYGZus{}subplot}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{circ}\PYG{o}{=}\PYG{n}{plt}\PYG{o}{.}\PYG{n}{Circle}\PYG{p}{(}\PYG{p}{(}\PYG{l+m+mf}{0.5}\PYG{p}{,}\PYG{l+m+mf}{0.5}\PYG{p}{)}\PYG{p}{,} \PYG{n}{radius}\PYG{o}{=}\PYG{l+m+mf}{.4}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gray}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fill}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{add\PYGZus{}patch}\PYG{p}{(}\PYG{n}{circ}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlim}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{.1}\PYG{p}{,}\PYG{l+m+mf}{1.1}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylim}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{.1}\PYG{p}{,}\PYG{l+m+mf}{1.1}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{tt}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}\PYG{n}{tt}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{n}{tt}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,}
            \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{n}{cmap}\PYG{o}{=}\PYG{n}{mpl}\PYG{o}{.}\PYG{n}{cm}\PYG{o}{.}\PYG{n}{cool}\PYG{p}{,}\PYG{n}{norm}\PYG{o}{=}\PYG{n}{mpl}\PYG{o}{.}\PYG{n}{colors}\PYG{o}{.}\PYG{n}{Normalize}\PYG{p}{(}\PYG{n}{vmin}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{vmax}\PYG{o}{=}\PYG{l+m+mf}{.9}\PYG{p}{)}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZdl{}x\PYGZus{}1\PYGZdl{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZdl{}x\PYGZus{}2\PYGZdl{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{backprop_73_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Wytrenowana sieć wygląda następująco:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{fnet}\PYG{o}{=}\PYG{n}{draw}\PYG{o}{.}\PYG{n}{plot\PYGZus{}net\PYGZus{}w}\PYG{p}{(}\PYG{n}{arch\PYGZus{}c}\PYG{p}{,}\PYG{n}{weights}\PYG{p}{,}\PYG{l+m+mf}{.1}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxadmonition}{note}{Informacja:}
\sphinxAtStartPar
To fascynujące, że nauczyliśmy sieć rozpoznawać, czy punkt znajduje się w okręgu, a nie ma ona żadnego pojęcia o geometrii, odległości euklidesowej, równaniu okręgu itp. Sieć właśnie nauczyła się „empirycznie”, jak postępować, za pomocą próbki szkoleniowej!
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{Informacja:}
\sphinxAtStartPar
Wynik przedstawiony na rysunku jest całkiem niezły, może z wyjątkiem, jak zwykle, punktów blisko granicy. Biorąc pod uwagę naszą dyskusję w rozdz. {\hyperref[\detokenize{docs/more_layers:more-lab}]{\sphinxcrossref{\DUrole{std,std-ref}{Więcej warstw}}}}, w którym wyznaczyliśmy wagi sieci z trzema warstwami neuronów na podstawie rozważań geometrycznych, jakość prezentowanego wyniku jest oszałamiająca. Nie widzimy żadnych prostych boków wielokąta, ale ładnie zaokrągloną granicę. Dalsza poprawa wyniku wymagałaby większej liczebności próbki szkoleniowej i dłuższego treningu, co jest czasochłonne.
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{Lokalne minima}

\sphinxAtStartPar
Wspomnieliśmy wcześniej o pojawianiu się minimów lokalnych w optymalizacji wielowymiarowej jako o potencjalnym problemie. Na poniższym rysunku pokazujemy trzy różne wyniki kodu backprop dla naszego klasyfikatora punktów w okręgu. Zauważamy, że każdy z nich ma radykalnie inny zestaw optymalnych wag, podczas gdy spawdzenie na próbce testowej jest, przynajmniej na oko, równie dobre dla każdego przypadku. To pokazuje, że optymalizacja backprop prowadzi, zgodnie z przewidywaniami, do różnych minimów lokalnych. Jednak każde z nich działa wystarczająco i równie dobrze. To jest właśnie powód, dla którego algorytm backprop można wykorzystać w praktycznych problemach: istnieją miliony lokalnych minimów, ale to naprawdę nie ma znaczenia!
\end{sphinxadmonition}

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{backprop_78_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}

\section{Ogólne uwagi}
\label{\detokenize{docs/backprop:ogolne-uwagi}}
\sphinxAtStartPar
Należy poczynić kilka istotnych i ogólnych obserwacji:

\begin{sphinxadmonition}{note}{Informacja:}\begin{itemize}
\item {} 
\sphinxAtStartPar
Uczenie nadzorowane zajmuje bardzo dużo czasu, ale użycie wytrenowanej sieci trwa mgnienie oka. Asymetria wynika z prostego faktu, że optymalizacja wieloparametrowa wymaga bardzo wielu wywołań funkcji (tutaj \sphinxstylestrong{feed\sphinxhyphen{}forward}) i obliczneia pochodnych w wielu rundach (użyliśmy 1000 rund dla przykładu okręgu), ale użycie sieci dla przypadku jednego punktu wymaga tylko jednego wywołania funkcji.

\item {} 
\sphinxAtStartPar
Klasyfikator wyszkolony algorytmem backprop może działać niedokładnie dla punktów w pobliżu linii granicznych. Środkiem zaradczym jest dłuższe trenowanie i/lub zwiększenie
liczebności próbki szkoleniowej, w szczególności w pobliżu granicy.

\item {} 
\sphinxAtStartPar
Jednak zbyt długa nauka na tej samej próbce treningowej nie ma sensu, ponieważ w pewnym momencie dokładność przestaje się poprawiać.

\item {} 
\sphinxAtStartPar
Lokalne minima są powszecjne, ale w żadnym wypadku nie stanowi to przeszkody w stosowaniu algorytmu. To ważna praktyczna cecha.

\item {} 
\sphinxAtStartPar
Można stosować różne ulepszenia metody najstromszego spadku lub zupełnie inne metody minimalizacji (patrz ćwiczenia). Mogą one znacznie zwiększyć wydajność algorytmu.

\item {} 
\sphinxAtStartPar
Cofając się z aktualizacją wag w kolejnych warstwach, można wprowadzić współczynnik zwiększający uaktualnianie (patrz ćwiczenia). To pomaga w wydajności.

\item {} 
\sphinxAtStartPar
Wreszcie, inne funkcje aktywacji mogą być używane do poprawy wydajności (patrz kolejne wykłady).

\end{itemize}
\end{sphinxadmonition}


\section{Ćwiczenia}
\label{\detokenize{docs/backprop:cwiczenia}}
\begin{sphinxadmonition}{note}{\protect\(~\protect\)}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Udowodnij (analitycznie), obliczając pochodną, że \( \sigma '(s) = \sigma (s) [1- \sigma (s)]\). Pokaż, że sigmoid jest \sphinxstylestrong{jedyną} funkcją z tą właściwością.

\item {} 
\sphinxAtStartPar
Wyprowadź jawnie wzory algorytmu backprop dla sieci z jedną i dwiema warstwami pośrednimi. Zwróć uwagę na pojawiającą się prawidłowość (powtarzalność) i udowodnij ogólne wzory z wykładu dla dowolnej liczby warstw pośrednich.

\item {} 
\sphinxAtStartPar
Zmodyfikuj przykład z wykładu dla klasyfikatora punktów w okręgu dla:
\begin{itemize}
\item {} 
\sphinxAtStartPar
półkola;

\item {} 
\sphinxAtStartPar
dwóch rozłącznych okręgów;

\item {} 
\sphinxAtStartPar
pierścienia;

\item {} 
\sphinxAtStartPar
dowolnego z twoich ulubionych kształtów.

\end{itemize}

\item {} 
\sphinxAtStartPar
Powtórz 3, eksperymentując z liczbą warstw i neuronów, ale pamiętaj, że duża ich liczba wydłuża czas obliczeń i niekoniecznie poprawia wynik. Uszereguj każdy przypadek według liczby błędnie sklasyfikowanych punktów w próbce testowej. Znajdź optymalną/praktyczną architekturę dla każdego z rozważanych obszarów.

\item {} 
\sphinxAtStartPar
Jeśli sieć ma dużo neuronów i połączeń, przez każdą synapsę przepływa mało sygnału, stąd sieć jest odporna na niewielkie przypadkowe uszkodzenia. Tak dzieje się w naszym mózgu, który jest nieustannie „uszkadzany” (promienie kosmiczne, alkohol,…). Poza tym taką sieć po zniszczeniu można (już przy mniejszej liczbie połączeń) dodatkowo doszkolić. Weź wytrenowaną sieć z problemu 3. i usuń jedno z jej \sphinxstylestrong{słabych} połączeń (najpierw znajdź je, sprawdzając wagi), zmieniając odpowiednią wagę na 0. Przetestuj taką uszkodzoną sieć na próbce testowej i wyciągnij wnioski.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Skalowanie wag w propagacji wstecznej.}
Wadą zastosowania sigmoidu w algorytmie backprop jest bardzo powolna aktualizacja wag w warstwach odległych od warstwy wyjściowej (im bliżej początku sieci, tym wolniej). Remedium jest tutaj przeskalowanie wag, gdzie szybkość uczenia się warstw, licząc od tyłu, jest sukcesywnie zwiększana o pewien współczynnik. Pamiętamy, że kolejne pochodne wnoszą do szybkości aktualizacji współczynniki postaci \( \sigma '(s) = \sigma (s) [1- \sigma (s)] = y (1-y) \), gdzie \( y \) wynosi w zakresie \( (0, 1) \). Zatem wartość \( y (1-y \) nie może przekraczać 1/4, a w kolejnych warstwach (licząc od tyłu) czynnika \( [y (1-y] ^ n \le 1/4 ^ n\)).
Aby zapobiec temu „kurczeniu się”, wskaźnik uczenia się można przemnażać przez współczynniki kompensacyjne \(4 ^ n: 4, 16, 64, 256, ... \). Kolejny argument heurystyczny {[}\hyperlink{cite.docs/conclusion:id13}{RIV91}{]} sugeruje jeszcze szybciej rosnące czynniki  postaci \(6^n\):\(6,36,216,1296,...\)
\begin{itemize}
\item {} 
\sphinxAtStartPar
Wprowadź powyższe dwie receptury do kodu backprop.

\item {} 
\sphinxAtStartPar
Sprawdź, czy rzeczywiście poprawiają wydajność algorytmu dla głębszych sieci, na przykład dla klasyfikatora punktów okręgu itp.

\item {} 
\sphinxAtStartPar
W celu oceny wydajności wykonaj pomiar czasu wykonania (np. za pomocą pakietu biblioteki Python \sphinxstylestrong{time}).

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Najstromsze spadek.}
Zastosowana w wykładzie metoda najstromszego spadku do wyznaczania minimum funkcji wielu zmiennych zależy od gradientu lokalnego. Istnieją znacznie lepsze podejścia, które zapewniają szybszą zbieżność do (lokalnego) minimum. Jednym z nich jest przepis \sphinxhref{https://en.wikipedia.org/wiki/Gradient\_descent}{Barzilai\sphinxhyphen{}Borwein} wyjaśniony poniżej. Zaimplementuj tę metodę w algorytmie wstecznej propagacji. Wektory \(x\) w przestrzeni \(n\)\sphinxhyphen{}wymiarowej są aktualizowane w kolejnych iteracjach jako \( x^{(m + 1)} = x^{(m)} - \gamma_m \nabla F (x^{(m)} )\),
gdzie \(m\) numeruje iterację, a szybkość uczenia się zależy od zachowania w dwóch (bieżącym i poprzednim) punktach:

\end{enumerate}
\begin{equation*}
\begin{split} \gamma _ {m} = \frac {\left | \left (x^{(m)}-x^{(m-1)} \right) \cdot
\left [\nabla F (x^{(m)}) - \nabla F (x^{(m-1)}) \right] \right |}
{\left \| \nabla F (x^{(m)}) - \nabla F (x^{(m-1)}) \right \| ^ {2}}.
\end{split}
\end{equation*}\end{sphinxadmonition}


\chapter{Interpolacja}
\label{\detokenize{docs/interpol:interpolacja}}\label{\detokenize{docs/interpol::doc}}

\section{Symulowane dane}
\label{\detokenize{docs/interpol:symulowane-dane}}
\sphinxAtStartPar
Do tej pory zajmowaliśmy się \sphinxstylestrong{klasyfikacją}, czyli rozpoznawaniem przez sieci, czy dany obiekt (w naszym przykładzie punkt na płaszczyźnie) ma określone cechy. Teraz przechodzimy do innego praktycznego zastosowania, a mianowicie do \sphinxstylestrong{interpolacji} funkcji. To zastosowanie ANN stało się bardzo popularne w analizie danych naukowych. Zilustrujemy tę metodę na prostym przykładzie, który wyjaśni podstawową ideę i pokaże, jak ona działa.

\sphinxAtStartPar
Wyobraźmy sobie, że dysponujemy pewnymi danymi eksperymentalnymi. W tym przypadku symulujemy je w sztuczny sposób, np.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{fi}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{return} \PYG{l+m+mf}{0.2}\PYG{o}{+}\PYG{l+m+mf}{0.8}\PYG{o}{*}\PYG{n}{np}\PYG{o}{.}\PYG{n}{sin}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}\PYG{o}{+}\PYG{l+m+mf}{0.5}\PYG{o}{*}\PYG{n}{x}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{3} \PYG{c+c1}{\PYGZsh{} a function}

\PYG{k}{def} \PYG{n+nf}{data}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:} 
    \PYG{n}{x} \PYG{o}{=} \PYG{l+m+mf}{7.}\PYG{o}{*}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{rand}\PYG{p}{(}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} random x coordinate}
    \PYG{n}{y} \PYG{o}{=} \PYG{n}{fi}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}\PYG{o}{+}\PYG{l+m+mf}{0.4}\PYG{o}{*}\PYG{n}{func}\PYG{o}{.}\PYG{n}{rn}\PYG{p}{(}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} y coordinate = the function value + noise from [\PYGZhy{}0.2,0.2]}
    \PYG{k}{return} \PYG{p}{[}\PYG{n}{x}\PYG{p}{,}\PYG{n}{y}\PYG{p}{]}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
Powinniśmy teraz myśleć w kategoriach uczenia nadzorowanego: \(x\) to „cecha”, a \(y\) to „etykieta”.

\sphinxAtStartPar
Tablicujemy nasze zaszumione punkty danych i wykreślamy je wraz z funkcją \sphinxstylestrong{fi(x)}, wokół której się wahają. Jest to imitacja pomiaru eksperymentalnego, który zawsze obarczony jest pewnym błędem, tutaj naśladowanym przez losowy szum.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{tab}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{data}\PYG{p}{(}\PYG{p}{)} \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{200}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}    \PYG{c+c1}{\PYGZsh{} data sample}
\PYG{n}{features}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{delete}\PYG{p}{(}\PYG{n}{tab}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{)}                   \PYG{c+c1}{\PYGZsh{} x coordinate}
\PYG{n}{labels}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{delete}\PYG{p}{(}\PYG{n}{tab}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{)}                     \PYG{c+c1}{\PYGZsh{} y coordinate}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{interpol_8_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
W języku ANN mamy zatem próbkę treningową składającą się z punktów o danych wejściowych (cechach) \(x\) i „prawdziwych” danych wyjściowych (etykietach) \(y\). Tak jak poprzednio, minimalizujemy funkcję błędu odpowiedniej sieci neuronowej,
\begin{equation*}
\begin{split}E(\{w \}) = \sum_p (y_o^{(p)} - y^{(p)})^2. \end{split}
\end{equation*}
\sphinxAtStartPar
Ponieważ generowane \(y_o\) jest pewną (zależną od wag) funkcją \(x\), metoda ta jest odmianą \sphinxstylestrong{dopasowania najmniejszych kwadratów}, powszechnie stosowaną w analizie danych. Różnica polega na tym, że w standardowej metodzie najmniejszych kwadratów funkcja modelu, którą dopasowujemy do danych, ma pewną prostą postać analityczną (np. \( f(x) = A + B x\)), podczas gdy teraz jest to pewna „zakamuflowana” funkcja zależna od wag, dostarczona przez sieć neuronową.


\section{Interpolacja z pomocą ANN}
\label{\detokenize{docs/interpol:interpolacja-z-pomoca-ann}}
\sphinxAtStartPar
Aby zrozumieć podstawową ideę, rozważmy sieć z tylko dwoma neuronami w warstwie pośredniej, z sigmoidalną funkcją aktywacji:

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{interpol_12_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
Sygnały docierające do dwóch neuronów w warstwie środkowej to, w notacji z rozdz. {\hyperref[\detokenize{docs/more_layers:more-lab}]{\sphinxcrossref{\DUrole{std,std-ref}{Więcej warstw}}}},
\begin{equation*}
\begin{split} s_1^{1}=w_{01}^{1}+w_{11}^{1} x, \end{split}
\end{equation*}\begin{equation*}
\begin{split} s_2^{1}=w_{02}^{1}+w_{12}^{1} x, \end{split}
\end{equation*}
\sphinxAtStartPar
a sygnały wychodzące to odpowiednio,
\begin{equation*}
\begin{split} \sigma \left( w_{01}^{1}+w_{11}^{1} x \right), \end{split}
\end{equation*}\begin{equation*}
\begin{split} \sigma \left( w_{02}^{1}+w_{12}^{1} x \right). \end{split}
\end{equation*}
\sphinxAtStartPar
Zatem połączony sygnał wchodzący do neuronu wyjściowego ma postać
\begin{equation*}
\begin{split} s_1^{1}=w_{01}^{2}+ w_{11}^{2}\sigma \left( w_{01}^{1}+w_{11}^{1} x \right)
+  w_{21}^{2}\sigma \left( w_{02}^{1}+w_{12}^{1} x \right). \end{split}
\end{equation*}
\sphinxAtStartPar
Przyjmując, dla ilustracji, przykładowe wartości wag
\begin{equation*}
\begin{split} w_{01}^{2}=0, w_{11}^{2}=1, w_{21}^{2}=-1, w_{21}^{2},
w_{11}^{1}=w_{12}^{1}=1, \, w_{01}^{1}=-x_1, \, w_{02}^{1}=-x_2, \end{split}
\end{equation*}
\sphinxAtStartPar
gdzie \(x_1\) i \(x_2\) to notacja skrótowa, otrzymujemy
\begin{equation*}
\begin{split} s_1^{1}=\sigma(x-x_1)-\sigma(x-x_2). \end{split}
\end{equation*}
\sphinxAtStartPar
Funkcja ta jest przedstawiona na poniższym wykresie, gdzie \(x_1=-1\) i \(x_2=4\).
Dąży ona do 0 w \(-\infty\), potem rośnie wraz z \(x\), osiągając maksimum w punkcie
\((x_1+x_2)/2\), a następnie maleje, dążąc do 0 przy \(+\infty\). W punktach \(x=x_1\) i \(x=x_2\) jej wartości wynoszą około 0.5, można więc powiedzieć, że przedział znaczących wartości funkcji zawiera się między \(x_1\) i \(x_2\).

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{interpol_14_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
Jest to prosty, ale ważny wniosek:
Jesteśmy w stanie utworzyć, za pomocą pary neuronów z sigmoidami, sygnał „garbowy”, zlokalizowany wokół danej wartości, tutaj \( (x_1 + x_2) / 2 = 2\), i o danym rozrzucie rzędu \(|x_2-x_1|\). Zmieniając wagi, możemy modyfikować jej kształt, szerokość i wysokość.

\sphinxAtStartPar
Można teraz pomyśleć w następujący sposób: Wyobraźmy sobie, że mamy do dyspozycji wiele neuronów w warstwie pośredniej. Możemy je łączyć w pary, tworząc garby „specjalizujące się” w określonych regionach współrzędnych. Następnie, dostosowując wysokości garbów, możemy łatwo aproksymować daną funkcję.

\sphinxAtStartPar
W rzeczywistej procedurze dopasowania nie musimy „łączyć neuronów w pary”, lecz dokonać łącznego dopasowania wszystkich parametrów jednocześnie, tak jak to miało miejsce w przypadku klasyfikatorów. Poniższy przykład przedstawia kompozycję 8 sigmoidów,
\begin{equation*}
\begin{split}
f = \sigma(z+3)-\sigma(z+1)+2 \sigma(z)-2\sigma(z-4)+
      \sigma(z-2)-\sigma(z-8)-1.3 \sigma(z-8)-1.3\sigma(z-10). 
\end{split}
\end{equation*}
\sphinxAtStartPar
Na rysunku funkcje składowe (cienkie linie oznaczające pojedyncze garby) sumują się do funkcji o dość skomplikowanym kształcie, oznaczonej grubą linią.

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{interpol_16_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\begin{sphinxadmonition}{note}{Informacja:}
\sphinxAtStartPar
Jeśli dopasowana funkcja jest regularna, można ją aproksymować za pomocą kombinacji liniowej sigmoidów. W przypadku większej liczby sigmoidów można uzyskać lepszą dokładność.
\end{sphinxadmonition}

\sphinxAtStartPar
Istnieje istotna różnica między ANN używanymi do aproksymacji funkcji w porównaniu z omawianymi wcześniej klasyfikatorami binarnymi. Tam odpowiedzi były równe 0 lub 1, więc w warstwie wyjściowej stosowaliśmy skokową funkcję aktywacji, a raczej jej gładką odmianę sigmoidalną. W przypadku aproksymacji funkcji odpowiedzi stanowią zazwyczaj kontinuum w zakresie wartości funkcji. Z tego powodu w warstwie wyjściowej używamy po prostu funkcji \sphinxstylestrong{identycznościowej}, czyli przepuszczamy przez nią bez zmian przychodzący sygnał. Oczywiście sigmoidy pozostają w warstwach pośrednich. Wówczas wzory używane do algorytmu \sphinxstylestrong{backprop} z sekcji {\hyperref[\detokenize{docs/backprop:bpa-lab}]{\sphinxcrossref{\DUrole{std,std-ref}{Algorytm propagacji wstecznej (backprop)}}}} mają w warstwie wyjściowej \(f_l(s)=s\).

\begin{sphinxadmonition}{note}{Warstwa wyjściowa dla aproksymacji funkcji}

\sphinxAtStartPar
W sieciach ANN używanych do aproksymacji funkcji, funkcja aktywacji w warstwie wyjściowej jest \sphinxstylestrong{identycznościowa}.
\end{sphinxadmonition}


\subsection{Alggorytm backprop dla funkcji jednowymiarowych}
\label{\detokenize{docs/interpol:alggorytm-backprop-dla-funkcji-jednowymiarowych}}
\sphinxAtStartPar
Weźmy architekturę

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{arch}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{6}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
i losowe wagi

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{weights}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{set\PYGZus{}ran\PYGZus{}w}\PYG{p}{(}\PYG{n}{arch}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
Jak właśnie wspomniano, wartość wyjściowa nie zawiera się teraz w przedziale od 0 do 1, co widać poniżej.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{x}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{feed\PYGZus{}forward\PYGZus{}o}\PYG{p}{(}\PYG{n}{arch}\PYG{p}{,} \PYG{n}{weights}\PYG{p}{,}\PYG{n}{features}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{ff}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{sig}\PYG{p}{,}\PYG{n}{ffo}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{lin}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{n}{draw}\PYG{o}{.}\PYG{n}{plot\PYGZus{}net\PYGZus{}w\PYGZus{}x}\PYG{p}{(}\PYG{n}{arch}\PYG{p}{,} \PYG{n}{weights}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{n}{x}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{interpol_26_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
W module biblioteki \sphinxstylestrong{func} mamy funkcję dla algorytmu backprop, która pozwala na zastosowanie jednej funkcji aktywacji w warstwach pośrednich (przyjmujemy sigmoidę) i innej w warstwie wyjściowej (przyjmujemy funkcję identycznościową). Trening jest przeprowadzany w dwóch etapach: w pierwszych 30 rundach pobieramy punkty z próbki treningowej w losowej kolejności, a następnie w kolejnych 1500 rundach przecodzimy kolejno przez wszystkie punkty, zmniejszając również szybkość uczenia \sphinxstylestrong{eps}. Strategia ta jest jedną z wielu możliwych, ale w tym przypadku dobrze spełnia swoje zadanie.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{eps}\PYG{o}{=}\PYG{l+m+mf}{0.02}                           \PYG{c+c1}{\PYGZsh{} initial learning speed}
\PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{30}\PYG{p}{)}\PYG{p}{:}                \PYG{c+c1}{\PYGZsh{} rounds}
    \PYG{k}{for} \PYG{n}{p} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{features}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} loop over the data sample points}
        \PYG{n}{pp}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randint}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{features}\PYG{p}{)}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} random point}
        \PYG{n}{func}\PYG{o}{.}\PYG{n}{back\PYGZus{}prop\PYGZus{}o}\PYG{p}{(}\PYG{n}{features}\PYG{p}{,}\PYG{n}{labels}\PYG{p}{,}\PYG{n}{pp}\PYG{p}{,}\PYG{n}{arch}\PYG{p}{,}\PYG{n}{weights}\PYG{p}{,}\PYG{n}{eps}\PYG{p}{,}
                         \PYG{n}{f}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{sig}\PYG{p}{,}\PYG{n}{df}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{dsig}\PYG{p}{,}\PYG{n}{fo}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{lin}\PYG{p}{,}\PYG{n}{dfo}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{dlin}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{1500}\PYG{p}{)}\PYG{p}{:}               \PYG{c+c1}{\PYGZsh{} rounds}
    \PYG{n}{eps}\PYG{o}{=}\PYG{l+m+mf}{0.999}\PYG{o}{*}\PYG{n}{eps}                  \PYG{c+c1}{\PYGZsh{} dicrease of the learning speed}
    \PYG{k}{for} \PYG{n}{p} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{features}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} loop over points taken in sequence}
        \PYG{n}{func}\PYG{o}{.}\PYG{n}{back\PYGZus{}prop\PYGZus{}o}\PYG{p}{(}\PYG{n}{features}\PYG{p}{,}\PYG{n}{labels}\PYG{p}{,}\PYG{n}{p}\PYG{p}{,}\PYG{n}{arch}\PYG{p}{,}\PYG{n}{weights}\PYG{p}{,}\PYG{n}{eps}\PYG{p}{,}
                         \PYG{n}{f}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{sig}\PYG{p}{,}\PYG{n}{df}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{dsig}\PYG{p}{,}\PYG{n}{fo}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{lin}\PYG{p}{,}\PYG{n}{dfo}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{dlin}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{interpol_31_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
Zauważmy, że otrzymana czerwona krzywa jest bardzo bliska funkcji użytej do wygenerowania próbki danych (czarna linia). Świadczy to o tym, że aproksymacja działa poprawnie. Konstrukcja miary ilościowej (sumy najmniejszych kwadratów) jest tematem ćwiczenia.

\begin{sphinxadmonition}{note}{Informacja:}
\sphinxAtStartPar
Funkcja aktywacji w warstwie wyjściowej może być dowolną gładką funkcją o wartościach zawierających wartości interpolowanej funkcji, niekoniecznie liniową.
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{Więcej wymiarów}

\sphinxAtStartPar
Aby interpolować funkcje dwóch lub więcej argumentów, należy użyć sieci ANN z co najmniej trzema warstwami neuronów.
\end{sphinxadmonition}

\sphinxAtStartPar
Możemy to rozumieć następująco {[}\hyperlink{cite.docs/conclusion:id7}{MullerRS12}{]}: dwa neurony w pierwszej warstwie neuronowej mogą tworzyć garb we współrzędnej \(x_1\), dwa inne \sphinxhyphen{} garb we współrzędnej \(x_2\), i tak dalej dla wszystkich pozostałych wymiarów. Tworząc koniunkcję tych \(n\) garbów w drugiej warstwie neuronów, otrzymujemy funkcję bazową specjalizującą się w obszarze wokół pewnego punktu w wielowymiarowej przestrzeni wejściowej. A zatem odpowiednio duża liczba takich funkcji bazowych może być użyta do aproksymacji w \(n\) wymiarach, w pełnej analogii do przypadku jednowymiarowego.

\begin{sphinxadmonition}{tip}{Wskazówka:}
\sphinxAtStartPar
Liczba neuronów potrzebnych w procedurze aproksymacji odzwierciedla zachowanie interpolowanej funkcji. Jeśli funkcja ulega licznym znacznym wahaniom, potrzeba więcej neuronów. W jednym wymiarze jest ich zwykle co najmniej dwa razy więcej niż liczba ekstremów funkcji.
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{Nadmierne dopasowanie (overfitting)}

\sphinxAtStartPar
Aby uniknąć tak zwanego \sphinxstylestrong{problemu nadmiernego dopasowania}, danych użytych do aproksymacji musi być znacznie więcej niż parametrów sieci. W przeciwnym razie moglibyśmy dopasować bardzo dokładnie dane treningowe za pomocą funkcji „wahającej się od punktu do punktu”. Jednocześnie, działanie takiej sieci na danych testowych byłoby bardzo kiepskie.
\end{sphinxadmonition}


\section{Ćwiczenia}
\label{\detokenize{docs/interpol:cwiczenia}}
\begin{sphinxadmonition}{note}{\protect\(~\protect\)}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Dopasuj punkty danych wygenerowane przez Twoją ulubioną funkcję (jednej zmiennej) z szumem. Pobaw się architekturą sieci i wyciągnij wnioski.

\item {} 
\sphinxAtStartPar
Oblicz sumę kwadratów odległości między wartościami punktów danych treningowych a odpowiadającą im funkcją aproksymującą i wykorzystaj ją jako miarę jakości dopasowania. Sprawdź, jak liczba neuronów w sieci wpływa na wynik.

\item {} 
\sphinxAtStartPar
Użyj sieci o większej liczbie warstw (co najmniej 3 warstwy neuronów) do dopasowania punktów danych wygenerowanych za pomocą ulubionej funkcji dwóch zmiennych. Wykonaj dwuwymiarowe wykresy konturowe dla tej funkcji oraz dla funkcji uzyskanej z sieci neuronowej i porównaj wyniki (oczywiście powinny być podobne, jeśli wszystko działa dobrze).

\end{enumerate}
\end{sphinxadmonition}


\chapter{Rectification}
\label{\detokenize{docs/rectification:rectification}}\label{\detokenize{docs/rectification::doc}}
\sphinxAtStartPar
In the previous chapter we made a hump function from two sigmoids, which would form a basis function for approximation. We may now ask a follow\sphinxhyphen{}up question: can we make the sigmoid itself a linear combination (or simply difference) of some other functions. Then we could use these functions for activation of neurons in place of the sigmoid. The answer is yes. For instance, the \sphinxstylestrong{Rectified Linear Unit (ReLU)} function
\begin{equation*}
\begin{split}
{\rm ReLU}(x) = \left \{ \begin{array}{l} x {\rm ~~~ for~} x \ge 0 \\
                                          0 {\rm ~~~ for~} x < 0 \end{array}    \right . = {\rm max}(x,0)
\end{split}
\end{equation*}
\sphinxAtStartPar
does (approximately) the job. The somewhat awkward name comes from electonics, where a „rectifying” (straightening up) unit is used to cut off negative values of an electric signal. The plot of ReLU looks as follows:

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{rectification_6_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
Taking a difference of two ReLU functions with shifted arguments yields, for example,

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{rectification_8_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
which looks pretty much as a sigmoid, apart from the sharp corners. One can make things smooth by taking a different function, the \sphinxstylestrong{softplus},
\begin{equation*}
\begin{split}
{\rm softplus}(x)=\log \left( 1+e^x \right ),
\end{split}
\end{equation*}
\sphinxAtStartPar
which looks like

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{rectification_10_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
A difference of two \sphinxstylestrong{softplus} functions yields a result very similar to the sigmoid.

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{rectification_12_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\begin{sphinxadmonition}{note}{Informacja:}
\sphinxAtStartPar
One may use the ReLU of softplus, or a plethora of other similar functions, for the activation.
\end{sphinxadmonition}

\sphinxAtStartPar
Why one should actually do this will be dicussed later.


\section{Interpolation with ReLU}
\label{\detokenize{docs/rectification:interpolation-with-relu}}
\sphinxAtStartPar
We can approximate our simulated data with an ANN with ReLU acivation in the intermediate layers (and the identity function is the output layer, as in the previous section). The functions are taken from the module \sphinxstylestrong{func}.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{fff}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{relu}    \PYG{c+c1}{\PYGZsh{} short\PYGZhy{}hand notation}
\PYG{n}{dfff}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{drelu}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
The network must now have more neurons, as the sigmoid „splits” into two ReLU functions:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{arch}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{30}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}                   \PYG{c+c1}{\PYGZsh{} architecture}
\PYG{n}{weights}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{set\PYGZus{}ran\PYGZus{}w}\PYG{p}{(}\PYG{n}{arch}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} initialize weights randomly in [\PYGZhy{}2.5,2.5]}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
We carry the simulations exactly as in the previous case. Experience says one should stat with small learning speeds. Two sets of rounds (as in the previous chapter)

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{eps}\PYG{o}{=}\PYG{l+m+mf}{0.0003}         \PYG{c+c1}{\PYGZsh{} small learning speed}
\PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{30}\PYG{p}{)}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} rounds}
    \PYG{k}{for} \PYG{n}{p} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{features}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:}          \PYG{c+c1}{\PYGZsh{} loop over the data sample points}
        \PYG{n}{pp}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randint}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{features}\PYG{p}{)}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} random point}
        \PYG{n}{func}\PYG{o}{.}\PYG{n}{back\PYGZus{}prop\PYGZus{}o}\PYG{p}{(}\PYG{n}{features}\PYG{p}{,}\PYG{n}{labels}\PYG{p}{,}\PYG{n}{pp}\PYG{p}{,}\PYG{n}{arch}\PYG{p}{,}\PYG{n}{weights}\PYG{p}{,}\PYG{n}{eps}\PYG{p}{,}
                         \PYG{n}{f}\PYG{o}{=}\PYG{n}{fff}\PYG{p}{,}\PYG{n}{df}\PYG{o}{=}\PYG{n}{dfff}\PYG{p}{,}\PYG{n}{fo}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{lin}\PYG{p}{,}\PYG{n}{dfo}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{dlin}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} teaching}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{600}\PYG{p}{)}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} rounds}
    \PYG{n}{eps}\PYG{o}{=}\PYG{n}{eps}\PYG{o}{*}\PYG{l+m+mf}{.995}
    \PYG{k}{for} \PYG{n}{p} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{features}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} points in sequence}
        \PYG{n}{func}\PYG{o}{.}\PYG{n}{back\PYGZus{}prop\PYGZus{}o}\PYG{p}{(}\PYG{n}{features}\PYG{p}{,}\PYG{n}{labels}\PYG{p}{,}\PYG{n}{p}\PYG{p}{,}\PYG{n}{arch}\PYG{p}{,}\PYG{n}{weights}\PYG{p}{,}\PYG{n}{eps}\PYG{p}{,}
                         \PYG{n}{f}\PYG{o}{=}\PYG{n}{fff}\PYG{p}{,}\PYG{n}{df}\PYG{o}{=}\PYG{n}{dfff}\PYG{p}{,}\PYG{n}{fo}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{lin}\PYG{p}{,}\PYG{n}{dfo}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{dlin}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} teaching}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
yield the result

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{rectification_23_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
We obtain again a quite satisfactory result (red line), noticing that the plot of the fitting function is a sequence of straight lines, simply reflecting the features of the ReLU activation function.


\section{Classifiers with rectification}
\label{\detokenize{docs/rectification:classifiers-with-rectification}}
\sphinxAtStartPar
There are technical reasons in favor of using \sphinxhref{https://en.wikipedia.org/wiki/Rectifier\_(neural\_networks)}{rectified functions} rather than sigmoid\sphinxhyphen{}like ones in backprop. The derivatives of the sigmoid are very close to zero apart for the narrow region near the threshold. This makes updating the weights unlikely, especially when going many layers back, as then very small numbers multiply yielding essentially no update (this is known as the \sphinxstylestrong{vanishing gradient problem}). With rectified functions, the range where the derivative is large is big (for ReLU it holds for all positive coordinates), hence the problem is cured. For that reason, rectified functions are used in deep ANNs, where there are many layers, impossible to train when the activation function is of a sigmoid type.

\begin{sphinxadmonition}{note}{Informacja:}
\sphinxAtStartPar
Application of rectified activation functions was one of the key tricks that allowed a breakthrough in deep ANNs around 2011.
\end{sphinxadmonition}

\sphinxAtStartPar
On the other hand, with ReLU it may happen that some weights are set to such values that many neurons become inactive, i.e. never fire for any input, and so are effectively eliminated. This is known as the „dead neuron” or „dead body” problem, which arises especially when the learning speed parameter is too high. A way to reduce the problem is to use an activation function which does not have at all a range with zero derivative, such as the \sphinxhref{https://en.wikipedia.org/wiki/Activation\_function}{Leaky ReLU}. Here we take it in the form
\begin{equation*}
\begin{split}
{\rm Leaky~ReLU}(x) = \left \{ \begin{array}{ll} x &{\rm ~~~ for~} x \ge 0 \\
                                          0.1 \, x &{\rm ~~~ for~} x < 0 \end{array}    \right . .
\end{split}
\end{equation*}
\sphinxAtStartPar
For illustration, we repeat our example from section {\hyperref[\detokenize{docs/backprop:circ-lab}]{\sphinxcrossref{\DUrole{std,std-ref}{Przykład z kołem}}}} with the classification of points in the circle, now with Leaky ReLU.

\sphinxAtStartPar
We take the following architecture and initial parameters:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{arch\PYGZus{}c}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{20}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}                   \PYG{c+c1}{\PYGZsh{} architecture}
\PYG{n}{weights}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{set\PYGZus{}ran\PYGZus{}w}\PYG{p}{(}\PYG{n}{arch\PYGZus{}c}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} scaled random initial weights in [\PYGZhy{}1.5,1.5]}
\PYG{n}{eps}\PYG{o}{=}\PYG{l+m+mf}{.01}                           \PYG{c+c1}{\PYGZsh{} initial learning speed }
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
and run the algorithm in two stages: with Leaky ReLU, and then with ReLU.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{300}\PYG{p}{)}\PYG{p}{:}    \PYG{c+c1}{\PYGZsh{} rounds}
    \PYG{n}{eps}\PYG{o}{=}\PYG{l+m+mf}{.9999}\PYG{o}{*}\PYG{n}{eps}       \PYG{c+c1}{\PYGZsh{} decrease the learning speed}
    \PYG{k}{if} \PYG{n}{k}\PYG{o}{\PYGZpc{}}\PYG{k}{100}==99: print(k+1,\PYGZsq{} \PYGZsq{},end=\PYGZsq{}\PYGZsq{})             \PYGZsh{} print progress        
    \PYG{k}{for} \PYG{n}{p} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{features\PYGZus{}c}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:}                \PYG{c+c1}{\PYGZsh{} loop over points}
        \PYG{n}{func}\PYG{o}{.}\PYG{n}{back\PYGZus{}prop\PYGZus{}o}\PYG{p}{(}\PYG{n}{features\PYGZus{}c}\PYG{p}{,}\PYG{n}{labels\PYGZus{}c}\PYG{p}{,}\PYG{n}{p}\PYG{p}{,}\PYG{n}{arch\PYGZus{}c}\PYG{p}{,}\PYG{n}{weights}\PYG{p}{,}\PYG{n}{eps}\PYG{p}{,}
            \PYG{n}{f}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{lrelu}\PYG{p}{,}\PYG{n}{df}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{dlrelu}\PYG{p}{,}\PYG{n}{fo}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{sig}\PYG{p}{,}\PYG{n}{dfo}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{dsig}\PYG{p}{)} 
                    \PYG{c+c1}{\PYGZsh{} backprop with leaky ReLU}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
100  200  300  
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{700}\PYG{p}{)}\PYG{p}{:}    \PYG{c+c1}{\PYGZsh{} rounds}
    \PYG{n}{eps}\PYG{o}{=}\PYG{l+m+mf}{.9999}\PYG{o}{*}\PYG{n}{eps}       \PYG{c+c1}{\PYGZsh{} decrease the learning speed}
    \PYG{k}{if} \PYG{n}{k}\PYG{o}{\PYGZpc{}}\PYG{k}{100}==99: print(k+1,\PYGZsq{} \PYGZsq{},end=\PYGZsq{}\PYGZsq{})             \PYGZsh{} print progress        
    \PYG{k}{for} \PYG{n}{p} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{features\PYGZus{}c}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:}                \PYG{c+c1}{\PYGZsh{} loop over points}
        \PYG{n}{func}\PYG{o}{.}\PYG{n}{back\PYGZus{}prop\PYGZus{}o}\PYG{p}{(}\PYG{n}{features\PYGZus{}c}\PYG{p}{,}\PYG{n}{labels\PYGZus{}c}\PYG{p}{,}\PYG{n}{p}\PYG{p}{,}\PYG{n}{arch\PYGZus{}c}\PYG{p}{,}\PYG{n}{weights}\PYG{p}{,}\PYG{n}{eps}\PYG{p}{,}
            \PYG{n}{f}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{relu}\PYG{p}{,}\PYG{n}{df}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{drelu}\PYG{p}{,}\PYG{n}{fo}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{sig}\PYG{p}{,}\PYG{n}{dfo}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{dsig}\PYG{p}{)} 
                    \PYG{c+c1}{\PYGZsh{} backprop with ReLU}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
100  200  300  400  500  600  700  
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
The result is quite satisfactory, showing that the method works. With the present architecture and activation functions, not surprisingly, in the plot below we can notice traces of a polygon approximating the circle.

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{rectification_37_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}

\section{Exercises}
\label{\detokenize{docs/rectification:exercises}}
\begin{sphinxadmonition}{note}{\protect\(~\protect\)}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Use various rectified activation functions for the binary classifiers and test them on various shapes (in analogy to the example with the circle above).

\item {} 
\sphinxAtStartPar
Convince yourself that starting backprop (with ReLU) with a too large initial learning speed leads to a „dead neuron” problem and a failure of the algorithm.

\end{enumerate}
\end{sphinxadmonition}


\chapter{Unsupervised learning}
\label{\detokenize{docs/unsupervised:unsupervised-learning}}\label{\detokenize{docs/unsupervised:un-lab}}\label{\detokenize{docs/unsupervised::doc}}
\begin{sphinxadmonition}{note}{Motto}

\sphinxAtStartPar
\sphinxstyleemphasis{teachers! leave those kids alone!}

\sphinxAtStartPar
          (Pink Floyd, Another Brick In The Wall)
\end{sphinxadmonition}

\sphinxAtStartPar
Supervised learning, discussed in previous lectures, needs a teacher or a training sample with labels, where we know \sphinxstylestrong{a priori} characteristics of the data (e.g., as in one of our examples, whether a given point is inside or outside the circle).

\sphinxAtStartPar
However, this is quite a special situation, because most often the data that we encounter do not have preassigned labels and „are what they are”. Also, from the neurobiological or methodological point of view, we learn many facts and activities „on an ongoing basis”, classifying and then recognizing them, whilst the process goes on without any external supervision or labels floating around.

\sphinxAtStartPar
Imagine an alien botanist who enters a meadow and encounters various species of flowers. He has no idea what they are and what to expect at all, as he has no prior knowledge on earthly matters. After finding the first flower, he records its features: color, size, number of petals, scent, etc. He goes on, finds a different flower, records its features, and so on and on with subsequent flowers. At some point, however, he finds a flower that he already had met. More precisely, its features are close, though not identical (the size may easily differ somewhat, so the color, etc.), to the previous instance. Hence he concludes that it belongs to the same category. The exploration goes on, and new flowers either start a new category, of join one already present. At the end of his quest, he has a catalog of flowers and now he can assign names (labels) to each species: corn poppy, bluebottle, mullein,…  These labels, or names, are useful in sharing the knowledge with others, as they summarize, so to speak, the features of the flower. Note, however, that these labels have actually never been used in the meadow exploration (learning) process.

\sphinxAtStartPar
Formally, the described problem of \sphinxstylestrong{unsupervised learning} is related to data classification (division into categories, or \sphinxstylestrong{clusters}, i.e. subsets of the sample where the suitably defined distances between individual data are small, smaller than the assumed distances between clusters). Colloquially speaking, we are looking for similarities between individual data points and try to divide the sample into groups of similar objects.


\section{Clusters of points}
\label{\detokenize{docs/unsupervised:clusters-of-points}}
\sphinxAtStartPar
Here is our simplified version of the alien botanist exploration.

\sphinxAtStartPar
Consider points on the plane that are randomly generated. Their distribution is not homogeneous, but is concentrated in four clusters: A, B, C, and D. For example, we can set appropriate limits for the coordinates \(x_1\) and \(x_2\) when randomly generating points of a given category. We use the numpy \sphinxstylestrong{random.uniform(a,b)} function, giving a uniformly distributed number between a and b:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{pA}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{return} \PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{uniform}\PYG{p}{(}\PYG{l+m+mf}{.75}\PYG{p}{,} \PYG{l+m+mf}{.95}\PYG{p}{)}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{uniform}\PYG{p}{(}\PYG{l+m+mf}{.7}\PYG{p}{,} \PYG{l+m+mf}{.9}\PYG{p}{)}\PYG{p}{]} 

\PYG{k}{def} \PYG{n+nf}{pB}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{return} \PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{uniform}\PYG{p}{(}\PYG{l+m+mf}{.4}\PYG{p}{,} \PYG{l+m+mf}{.6}\PYG{p}{)}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{uniform}\PYG{p}{(}\PYG{l+m+mf}{.6}\PYG{p}{,} \PYG{l+m+mf}{.75}\PYG{p}{)}\PYG{p}{]} 

\PYG{k}{def} \PYG{n+nf}{pC}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{return} \PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{uniform}\PYG{p}{(}\PYG{l+m+mf}{.1}\PYG{p}{,} \PYG{l+m+mf}{.3}\PYG{p}{)}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{uniform}\PYG{p}{(}\PYG{l+m+mf}{.4}\PYG{p}{,} \PYG{l+m+mf}{.5}\PYG{p}{)}\PYG{p}{]} 

\PYG{k}{def} \PYG{n+nf}{pD}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{return} \PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{uniform}\PYG{p}{(}\PYG{l+m+mf}{.7}\PYG{p}{,} \PYG{l+m+mf}{.9}\PYG{p}{)}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{uniform}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mf}{.2}\PYG{p}{)}\PYG{p}{]} 
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
Let us create data samples with a few points from each category:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{samA}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{pA}\PYG{p}{(}\PYG{p}{)} \PYG{k}{for} \PYG{n}{\PYGZus{}} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{10}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{samB}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{pB}\PYG{p}{(}\PYG{p}{)} \PYG{k}{for} \PYG{n}{\PYGZus{}} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{7}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{samC}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{pC}\PYG{p}{(}\PYG{p}{)} \PYG{k}{for} \PYG{n}{\PYGZus{}} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{9}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{samD}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{pD}\PYG{p}{(}\PYG{p}{)} \PYG{k}{for} \PYG{n}{\PYGZus{}} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{11}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
Our data looks like this:

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{unsupervised_11_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
If we show the above picture to someone, This the person will undoubtedly state that there are four clusters. But what algorithm is being used to determine this? We will construct such an algorithm shortly and will be able to carry out clusterization. For the moment, let us jump ahead and assume we \sphinxstylestrong{know} what the clusters are. Clearly, in our example the clusters are well defined, i.e. visibly separated from each other.

\sphinxAtStartPar
One can represent clusters with \sphinxstylestrong{representative points} that lie somewhere within the cluster. For example, one could take an item belonging to a given cluster as its representative, or for each cluster one can evaluate the mean position of its points and use it as a representative point:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{rA}\PYG{o}{=}\PYG{p}{[}\PYG{n}{st}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{samA}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,}\PYG{n}{st}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{samA}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{]}
\PYG{n}{rB}\PYG{o}{=}\PYG{p}{[}\PYG{n}{st}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{samB}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,}\PYG{n}{st}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{samB}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{]}
\PYG{n}{rC}\PYG{o}{=}\PYG{p}{[}\PYG{n}{st}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{samC}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,}\PYG{n}{st}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{samC}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{]}
\PYG{n}{rD}\PYG{o}{=}\PYG{p}{[}\PYG{n}{st}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{samD}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,}\PYG{n}{st}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{samD}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{]}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
(we have used the \sphinxstylestrong{statistics} module to evaluate the mean). We append thus defined characteristic points to our graphics. For visual convenience, we assign a color for each category (after having the clusters, we may assign labels, and the color here serves precisely this purpose).

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{col}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{red}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{blue}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{green}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{magenta}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mf}{2.3}\PYG{p}{,}\PYG{l+m+mf}{2.3}\PYG{p}{)}\PYG{p}{,}\PYG{n}{dpi}\PYG{o}{=}\PYG{l+m+mi}{120}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Clusters with representative points}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)} 
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlim}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{.1}\PYG{p}{,}\PYG{l+m+mf}{1.1}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylim}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{.1}\PYG{p}{,}\PYG{l+m+mf}{1.1}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{samA}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}\PYG{n}{samA}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{n}{col}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{samB}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}\PYG{n}{samB}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{n}{col}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{samC}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}\PYG{n}{samC}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{n}{col}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{samD}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}\PYG{n}{samD}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{n}{col}\PYG{p}{[}\PYG{l+m+mi}{3}\PYG{p}{]}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{rA}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}\PYG{n}{rA}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{n}{col}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{90}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{0.5}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{rB}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}\PYG{n}{rB}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{n}{col}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{90}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{0.5}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{rC}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}\PYG{n}{rC}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{n}{col}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{90}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{0.5}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{rD}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}\PYG{n}{rD}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{n}{col}\PYG{p}{[}\PYG{l+m+mi}{3}\PYG{p}{]}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{90}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{0.5}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZdl{}x\PYGZus{}1\PYGZdl{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZdl{}x\PYGZus{}2\PYGZdl{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{unsupervised_16_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}

\section{Voronoi areas}
\label{\detokenize{docs/unsupervised:voronoi-areas}}\label{\detokenize{docs/unsupervised:vor-lab}}
\sphinxAtStartPar
Having the situation as in the figure above, i.e. with the representative points determined, we can divide the entire plane into areas according to the following Voronoi criterion, which is a simple geometric notion:

\begin{sphinxadmonition}{note}{Voronoi areas}

\sphinxAtStartPar
Consider a metric space in which there are a number of representative points (the Voronoi points) \(R\). For a given point \(P\) one determines the distances to all \(R\). If there is a strict minimum among these distances (the closest point \(R_m\)), then by definition \(P\) belongs to the Voronoi area of \(R_m\). If there is no strict minimum, then \(P\) belongs to a boundary between some Voronoi regions. The construction divides the whole space into Voronoi areas and their boundaries.
\end{sphinxadmonition}

\sphinxAtStartPar
Returning to our example, let us then define the color of a point P in our square as the color of the nearest representative point. To do it, we first need (the square of) the distance function (here Euclidean) between two points in 2\sphinxhyphen{}dim. space:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{eucl}\PYG{p}{(}\PYG{n}{p1}\PYG{p}{,}\PYG{n}{p2}\PYG{p}{)}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} square of the Euclidean distance}
    \PYG{k}{return} \PYG{p}{(}\PYG{n}{p1}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{n}{p2}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{o}{+}\PYG{p}{(}\PYG{n}{p1}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{n}{p2}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
Then, with \sphinxstylestrong{np.argmin}, we find the nearest representative point and determine its color:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{col\PYGZus{}char}\PYG{p}{(}\PYG{n}{p}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{dist}\PYG{o}{=}\PYG{p}{[}\PYG{n}{eucl}\PYG{p}{(}\PYG{n}{p}\PYG{p}{,}\PYG{n}{rA}\PYG{p}{)}\PYG{p}{,}\PYG{n}{eucl}\PYG{p}{(}\PYG{n}{p}\PYG{p}{,}\PYG{n}{rB}\PYG{p}{)}\PYG{p}{,}\PYG{n}{eucl}\PYG{p}{(}\PYG{n}{p}\PYG{p}{,}\PYG{n}{rC}\PYG{p}{)}\PYG{p}{,}\PYG{n}{eucl}\PYG{p}{(}\PYG{n}{p}\PYG{p}{,}\PYG{n}{rD}\PYG{p}{)}\PYG{p}{]} \PYG{c+c1}{\PYGZsh{} array of distances}
    \PYG{n}{ind\PYGZus{}min} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{argmin}\PYG{p}{(}\PYG{n}{dist}\PYG{p}{)}                          \PYG{c+c1}{\PYGZsh{} index of the nearest point}
    \PYG{k}{return} \PYG{n}{col}\PYG{p}{[}\PYG{n}{ind\PYGZus{}min}\PYG{p}{]}                                \PYG{c+c1}{\PYGZsh{} color of the nearest point}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
for instance

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{col\PYGZus{}char}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mf}{.5}\PYG{p}{,}\PYG{l+m+mf}{.5}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZsq{}blue\PYGZsq{}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
The result of running this coloring for points in our square (we take here a sufficiently dense sample of 70\textbackslash{}tims 70\$ points) is its following division into the Voronoi areas:

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{unsupervised_26_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
It is easy to prove that the boundaries between neighboring areas are straight lines.

\begin{sphinxadmonition}{note}{Informacja:}
\sphinxAtStartPar
A practical message here is that once we have determined the characteristic points, we can use Voronoi’s criterion for a classification of data.
\end{sphinxadmonition}


\section{Naive clusterization}
\label{\detokenize{docs/unsupervised:naive-clusterization}}
\sphinxAtStartPar
Now we go back to the alien botanist’s problem:  imagine we have our sample, but we know nothing about how its points were generated (we do not have any labels A, B, C, D, nor colors of the points). Moreover, the data is mixed, i.e., the data points appear in a random order. So we merge our points with \sphinxstylestrong{np.concatenate}:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{alls}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{concatenate}\PYG{p}{(}\PYG{p}{(}\PYG{n}{samA}\PYG{p}{,} \PYG{n}{samB}\PYG{p}{,} \PYG{n}{samC}\PYG{p}{,} \PYG{n}{samD}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
and shuffle them with \sphinxstylestrong{np.random.shuffle}:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{shuffle}\PYG{p}{(}\PYG{n}{alls}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
The data visualization looks as in the first plot of this chapter.

\sphinxAtStartPar
We now want to somehow create representative points, but a priori we don’t know where they should be, or even how many of them there are. Very different strategies are possible here. Their common feature is that the position of the representative points is updated as the sample data is processed.

\sphinxAtStartPar
Let us start with just one representative point, \(\vec{R}\). Not very ambitious, but in the end we will at least know some mean characteristics of the sample. The initial position is \( R=(R_1, R_2) \), a two dimensional vector in \([0,1]\times [0,1]\). After reading a data point \(P\) with coordinates \( (x_1 ^ P, x_2 ^ P) \), \(R\) changes as follows:
\begin{equation*}
\begin{split} (R_1, R_2) \to (R_1, R_2) + \varepsilon (x_1 ^P-R_1, x_2 ^P-R_2), \end{split}
\end{equation*}
\sphinxAtStartPar
or in the vector notation
\begin{equation*}
\begin{split} \vec {R} \to \vec {R} + \varepsilon (\vec {x}^P - \vec {R}). \end{split}
\end{equation*}
\sphinxAtStartPar
The step is repeated for all the points of the sample, and then many such rounds may be carried out. As in the previous chapters, \( \varepsilon \) is the learning rate that (preferably) decreases
as the algorithm proceeds. The above formula realizes the „snapping” of the point \(\vec{R}\) by the data point \(\vec{P}\).

\sphinxAtStartPar
The following code implements the above prescription:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{R}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} initial location}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{initial location:}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(}\PYG{n}{R}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{round   location}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{n}{eps}\PYG{o}{=}\PYG{l+m+mf}{.5}                         \PYG{c+c1}{\PYGZsh{} initial learning speed}

\PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{50}\PYG{p}{)}\PYG{p}{:}            \PYG{c+c1}{\PYGZsh{} rounds}
    \PYG{n}{eps}\PYG{o}{=}\PYG{l+m+mf}{0.85}\PYG{o}{*}\PYG{n}{eps}               \PYG{c+c1}{\PYGZsh{} decrease the learning speed }
    \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{shuffle}\PYG{p}{(}\PYG{n}{alls}\PYG{p}{)}    \PYG{c+c1}{\PYGZsh{} reshuffle the sample}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{alls}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} loop over points of the whole sample}
        \PYG{n}{R}\PYG{o}{+}\PYG{o}{=}\PYG{n}{eps}\PYG{o}{*}\PYG{p}{(}\PYG{n}{alls}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{n}{R}\PYG{p}{)}     \PYG{c+c1}{\PYGZsh{} update/learning}
    \PYG{k}{if} \PYG{n}{j}\PYG{o}{\PYGZpc{}}\PYG{k}{5}==4: print(j+1, \PYGZdq{}    \PYGZdq{},np.round(R,3))  \PYGZsh{} print every 5th step
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
initial location:
[0.42  0.084]
round   location
5      [0.653 0.655]
10      [0.658 0.385]
15      [0.603 0.462]
20      [0.594 0.484]
25      [0.598 0.486]
30      [0.598 0.487]
35      [0.599 0.488]
40      [0.599 0.488]
45      [0.599 0.488]
50      [0.599 0.488]
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
We can see that the position of the characteristic point converges. Actually, it becomes very close to the mean location of all the points of the sample,

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{R\PYGZus{}mean}\PYG{o}{=}\PYG{p}{[}\PYG{n}{st}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{alls}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,}\PYG{n}{st}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{alls}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{]}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(}\PYG{n}{R\PYGZus{}mean}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
[0.599 0.488]
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
We have decided a priori to have just one category, and here is our plot of the result for the characteristic point, indicated with a gray blob:

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{unsupervised_39_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
One is, of course, not satisfied with the above (things are not classified with one category), so let us try to generalize the algorithm for the case of several (\( n_R> \) 1) representative points.
\begin{itemize}
\item {} 
\sphinxAtStartPar
We initialize randomly representative vectors \( \vec{R}^i \), \(i = 1, \dots, n_R \).

\item {} 
\sphinxAtStartPar
Round: We take the sample points P one by one and update only the \sphinxstylestrong{closest} representative point \(R^m\) to the point P in a given step:

\end{itemize}
\begin{equation*}
\begin{split} \vec{R}^m \to \vec{R}^m + \varepsilon (\vec{x} - \vec{R}^m). \end{split}
\end{equation*}\begin{itemize}
\item {} 
\sphinxAtStartPar
The position of the other representative points remains unchanged. This strategy is called \sphinxstylestrong{winner\sphinxhyphen{}take\sphinxhyphen{}all}.

\item {} 
\sphinxAtStartPar
We repeat the rounds, reducing the learning speed \( \varepsilon \) each time, until we are happy with the result.

\end{itemize}

\begin{sphinxadmonition}{important}{Ważne:}
\sphinxAtStartPar
The \sphinxstylestrong{winner\sphinxhyphen{}take\sphinxhyphen{}all} strategy is an important concept in the ANN training. The competing neurons in a layer fight for the „reward”, and the one that wins, takes it all (its weighs get updated), while the losers get nothing.
\end{sphinxadmonition}

\sphinxAtStartPar
Let us then consider two representative points that we initialize randomly:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{R1}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{R2}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
Next, we carry out the above algorithm. For each data point we find the nearest representative point out of the two, and update only the winner:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{initial locations:}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(}\PYG{n}{R1}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(}\PYG{n}{R2}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{rounds  locations}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{n}{eps}\PYG{o}{=}\PYG{l+m+mf}{.5}

\PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{40}\PYG{p}{)}\PYG{p}{:}             
    \PYG{n}{eps}\PYG{o}{=}\PYG{l+m+mf}{0.85}\PYG{o}{*}\PYG{n}{eps}
    \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{shuffle}\PYG{p}{(}\PYG{n}{alls}\PYG{p}{)} 
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{alls}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{p}\PYG{o}{=}\PYG{n}{alls}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}   
        \PYG{n}{dist}\PYG{o}{=}\PYG{p}{[}\PYG{n}{func}\PYG{o}{.}\PYG{n}{eucl}\PYG{p}{(}\PYG{n}{p}\PYG{p}{,}\PYG{n}{R1}\PYG{p}{)}\PYG{p}{,} \PYG{n}{func}\PYG{o}{.}\PYG{n}{eucl}\PYG{p}{(}\PYG{n}{p}\PYG{p}{,}\PYG{n}{R2}\PYG{p}{)}\PYG{p}{]} \PYG{c+c1}{\PYGZsh{} squares of distances}
        \PYG{n}{ind\PYGZus{}min} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{argmin}\PYG{p}{(}\PYG{n}{dist}\PYG{p}{)}               \PYG{c+c1}{\PYGZsh{} index of the minimum}
        \PYG{k}{if} \PYG{n}{ind\PYGZus{}min}\PYG{o}{==}\PYG{l+m+mi}{0}\PYG{p}{:}         \PYG{c+c1}{\PYGZsh{} if R1 closer to the new data point}
            \PYG{n}{R1}\PYG{o}{+}\PYG{o}{=}\PYG{n}{eps}\PYG{o}{*}\PYG{p}{(}\PYG{n}{p}\PYG{o}{\PYGZhy{}}\PYG{n}{R1}\PYG{p}{)}     \PYG{c+c1}{\PYGZsh{} update R1                }
        \PYG{k}{else}\PYG{p}{:}                  \PYG{c+c1}{\PYGZsh{} if R2 closer ... }
            \PYG{n}{R2}\PYG{o}{+}\PYG{o}{=}\PYG{n}{eps}\PYG{o}{*}\PYG{p}{(}\PYG{n}{p}\PYG{o}{\PYGZhy{}}\PYG{n}{R2}\PYG{p}{)}     \PYG{c+c1}{\PYGZsh{} update R2       }

    \PYG{k}{if} \PYG{n}{j}\PYG{o}{\PYGZpc{}}\PYG{k}{5}==4: print(j+1,\PYGZdq{}    \PYGZdq{}, np.round(R1,3), np.round(R2,3))  
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
initial locations:
[0.489 0.494] [0.814 0.666]
rounds  locations
5      [0.749 0.097] [0.409 0.608]
10      [0.764 0.085] [0.469 0.641]
15      [0.765 0.084] [0.526 0.657]
20      [0.766 0.083] [0.523 0.657]
25      [0.766 0.083] [0.525 0.659]
30      [0.766 0.083] [0.527 0.66 ]
35      [0.766 0.082] [0.527 0.66 ]
40      [0.767 0.082] [0.527 0.66 ]
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
The result is this:

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{unsupervised_47_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
One of the characteristic points „specializes” in the lower right cluster, and the other in the remaining points.

\sphinxAtStartPar
Next, we continue, completely analogously, with four representative points.

\sphinxAtStartPar
The result for two different initial conditions of the characteristic points is
shown in \hyperref[\detokenize{docs/unsupervised:p-fig}]{Rys.\@ \ref{\detokenize{docs/unsupervised:p-fig}}}.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=500\sphinxpxdimen]{{cl4_2}.png}
\caption{Left: proper characteristic points. Right: one „dead body”.}\label{\detokenize{docs/unsupervised:p-fig}}\end{figure}

\sphinxAtStartPar
We notice that the procedure does not always give the „correct”/expected answer. Quite often one of the representative points is not updated at all and becomes the so\sphinxhyphen{}called \sphinxstylestrong{dead body}. This is because the other representative points always win, i.e. one of them is always closer to each data point of the sample than the „corpse”. Certainly, this is an unsatisfactory situation.

\sphinxAtStartPar
When we set up five characteristic points, depending on the random initialization, several situation may occur, as shown in \hyperref[\detokenize{docs/unsupervised:id1}]{Rys.\@ \ref{\detokenize{docs/unsupervised:id1}}}. Sometimes a cluster is split into two smaller ones, sometimes dead bodies occur.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=870\sphinxpxdimen]{{cl5}.jpg}
\caption{From left to right: 5 characteristic points with one cluster split into two, with another cluster split into two, one dead body, and two dead bodies.}\label{\detokenize{docs/unsupervised:id1}}\end{figure}

\sphinxAtStartPar
Enforcing more representative points leads to the formation of dead bodies even more often. Of course, we may disregard them, but the example shows that the current strategy is highly problematic and we need something better.


\section{Clustering scale}
\label{\detokenize{docs/unsupervised:clustering-scale}}
\sphinxAtStartPar
In the previous section we were trying to guess from the outset how many clusters there are in the data. This led to problems, as usually we do not even know how many clusters there are. Actually, up to now we have not defined what precisely a cluster is, and were using some intuition only. This intuition told us that the points in the same cluster must be close to one another, or close to a characteristic point, but how close? Actually, the definition must involve a \sphinxstylestrong{scale} (a characteristic distance) telling us „how close is close”. For instance, in our example we may take a scale of about 0.2, where there are 4 clusters, but we may take a smaller scale and resolve the bigger clusters into smaller ones, as in two left panels of \hyperref[\detokenize{docs/unsupervised:id1}]{Rys.\@ \ref{\detokenize{docs/unsupervised:id1}}}.

\begin{sphinxadmonition}{note}{Definition of cluster}

\sphinxAtStartPar
A cluster of scale \(d\) associated with a characteristic point \(R\) is a set of data points \(P\), whose distance from \(R\) is less than \(d\), whereas the distance from other characteristic points is \(\ge d\). The characteristic points must be selected in such a way that each data point belongs to a cluster, and no characteristic point is a dead body (i.e., its cluster must contain at least one data point).
\end{sphinxadmonition}

\sphinxAtStartPar
Various strategies can be used to implement this prescription. We use here the \sphinxstylestrong{dynamical clusterization}, where a new cluster/representative point is created whenever an encountered data point is farther than \(d\) from any characteristic point defined up to now.

\begin{sphinxadmonition}{note}{Dynamical clusterization}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Set the clustering scale \(d\) and the initial learning speed \(\varepsilon\). Shuffle the sample.

\item {} 
\sphinxAtStartPar
Read the first data point \(P_1\) and set the first characteristic point \(R^1=P_1\). Add it to an array \(R\) of all characteristic points. Mark \(P_1\) as belonging to cluster \(1\).

\item {} 
\sphinxAtStartPar
Read the next data points \(P\). If the distance of \(P\) to the \sphinxstylestrong{closest} characteristic point, \(R^m\), is \(\le d\), then
\begin{itemize}
\item {} 
\sphinxAtStartPar
mark \(P\) as belonging to cluster \(m\).

\item {} 
\sphinxAtStartPar
move \(R^m\) towards \(P\) with the learning speed \(\varepsilon\).Otherwise, add to \(R\) a new characteristic point at the location of point \(P\).

\end{itemize}

\item {} 
\sphinxAtStartPar
Repeat from \(2.\) until all the data points are processed.

\item {} 
\sphinxAtStartPar
Repeat from \(2.\) in a number of rounds, decreasing each time \(\varepsilon\). The result is a division of the sample into a number of clusters, and the location of corresponding characteristic points. The result may depend on the random reshuffling, hence does not have to be the same when the procedure is repeated.

\end{enumerate}
\end{sphinxadmonition}

\sphinxAtStartPar
A Python implementation, finding dynamically the representative points, is following:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{d}\PYG{o}{=}\PYG{l+m+mf}{0.2}   \PYG{c+c1}{\PYGZsh{} clustering scale}
\PYG{n}{eps}\PYG{o}{=}\PYG{l+m+mf}{0.5} \PYG{c+c1}{\PYGZsh{} initial learning speed}

\PYG{k}{for} \PYG{n}{r} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{20}\PYG{p}{)}\PYG{p}{:}               \PYG{c+c1}{\PYGZsh{} rounds}
    \PYG{n}{eps}\PYG{o}{=}\PYG{l+m+mf}{0.85}\PYG{o}{*}\PYG{n}{eps}                  \PYG{c+c1}{\PYGZsh{} decrease the learning speed }
    \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{shuffle}\PYG{p}{(}\PYG{n}{alls}\PYG{p}{)}       \PYG{c+c1}{\PYGZsh{} shuffle the sample}
    \PYG{k}{if} \PYG{n}{r}\PYG{o}{==}\PYG{l+m+mi}{0}\PYG{p}{:}                      \PYG{c+c1}{\PYGZsh{} in the first round}
        \PYG{n}{R}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{alls}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}     \PYG{c+c1}{\PYGZsh{} R \PYGZhy{} array of representative points}
                                  \PYG{c+c1}{\PYGZsh{} initialized to the first data point}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{alls}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:}    \PYG{c+c1}{\PYGZsh{} loop over the sample points}
        \PYG{n}{p}\PYG{o}{=}\PYG{n}{alls}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}                 \PYG{c+c1}{\PYGZsh{} new data point}
        \PYG{n}{dist}\PYG{o}{=}\PYG{p}{[}\PYG{n}{func}\PYG{o}{.}\PYG{n}{eucl}\PYG{p}{(}\PYG{n}{p}\PYG{p}{,}\PYG{n}{R}\PYG{p}{[}\PYG{n}{k}\PYG{p}{]}\PYG{p}{)} \PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{R}\PYG{p}{)}\PYG{p}{)}\PYG{p}{]} 
         \PYG{c+c1}{\PYGZsh{} array of squares of distances of p from the current repr. points in R}
        \PYG{n}{ind\PYGZus{}min} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{argmin}\PYG{p}{(}\PYG{n}{dist}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} index of the closest repr. point}
        \PYG{k}{if} \PYG{n}{dist}\PYG{p}{[}\PYG{n}{ind\PYGZus{}min}\PYG{p}{]} \PYG{o}{\PYGZgt{}} \PYG{n}{d}\PYG{o}{*}\PYG{n}{d}\PYG{p}{:}   \PYG{c+c1}{\PYGZsh{} if its distance square \PYGZgt{} d*d}
                                  \PYG{c+c1}{\PYGZsh{} dynamical creation of a new category}
            \PYG{n}{R}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{R}\PYG{p}{,} \PYG{p}{[}\PYG{n}{p}\PYG{p}{]}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}    \PYG{c+c1}{\PYGZsh{} add new repr. point to R}
        \PYG{k}{else}\PYG{p}{:}   
            \PYG{n}{R}\PYG{p}{[}\PYG{n}{ind\PYGZus{}min}\PYG{p}{]}\PYG{o}{+}\PYG{o}{=}\PYG{n}{eps}\PYG{o}{*}\PYG{p}{(}\PYG{n}{p}\PYG{o}{\PYGZhy{}}\PYG{n}{R}\PYG{p}{[}\PYG{n}{ind\PYGZus{}min}\PYG{p}{]}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} otherwise, apdate the \PYGZdq{}old\PYGZdq{} repr. point}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Number of representative points: }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{R}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
Number of representative points:  4
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
The outcome of the algorithm for various values of the clustering scale \(d\) is shown in \hyperref[\detokenize{docs/unsupervised:dyn-fig}]{Rys.\@ \ref{\detokenize{docs/unsupervised:dyn-fig}}}. At very low values of \(d\), smaller than the minimum separation between the points, there are as many clusters as the data points. Then, as we increase \(d\), the number of clusters decreases. At very large \(d\), of the order of the span of the whole sample, there is only one cluster.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=770\sphinxpxdimen]{{cd}.jpg}
\caption{Dynamical clustering for various values of the scale \(d\).}\label{\detokenize{docs/unsupervised:dyn-fig}}\end{figure}

\sphinxAtStartPar
Certainly, an algorithm will not tell us which clustering scale to use. The proper value depends on the nature of the problem. Recall our botanist. If he used a very small \(d\), he would get as many categories as there are flowers in the meadow, as all flowers, even of the same species, are slightly different from one another. That would be useless. On the other extreme, if his \(d\) is too large, then the classification is too crude. Something in between is just right!

\begin{sphinxadmonition}{note}{Labels}

\sphinxAtStartPar
After forming the clusters, we may assign them \sphinxstylestrong{labels} for convenience. They are not used in the learning (cluster formation) process.
\end{sphinxadmonition}

\sphinxAtStartPar
Having determined the clusters, we have a \sphinxstylestrong{classifier}. We may use it in a two\sphinxhyphen{}fold way:
\begin{itemize}
\item {} 
\sphinxAtStartPar
continue the dynamical update as new data are encountered, or

\item {} 
\sphinxAtStartPar
„close” it, and see where the new data falls in.

\end{itemize}

\sphinxAtStartPar
In the first case, we assign a corresponding cluster label to the new data point (our botanist knows what new flower he found), or initiate a new category if the point does not belong to any of the existing clusters. This is just a continuation of the dynamical algorithm described above for new incoming data

\sphinxAtStartPar
In the latter case (we bought the ready and closed botanist’s catalog), a data point may
\begin{itemize}
\item {} 
\sphinxAtStartPar
belong to a cluster (we know its label),

\item {} 
\sphinxAtStartPar
fall outside of any cluster, then we just do not know what it is, or

\item {} 
\sphinxAtStartPar
fall into an overlapping region of two or more clusters (cf. \hyperref[\detokenize{docs/unsupervised:dyn-fig}]{Rys.\@ \ref{\detokenize{docs/unsupervised:dyn-fig}}}, where we only get „partial” or ambiguous classification.

\end{itemize}

\sphinxAtStartPar
Alternatively, we can use the Voronoi areas classification to get rid of the ambiguity.


\subsection{Interpretation via steepest descent}
\label{\detokenize{docs/unsupervised:interpretation-via-steepest-descent}}
\sphinxAtStartPar
Let us denote a given cluster with \(C_i\), \(i = 1, ..., n\), where \( n \) is the total number of clusters. The sum of the squared distances of data points in \( C_i \) to its representative point \( R ^ i \) is
\begin{equation*}
\begin{split}
\sum_{P \in C_i} | \vec{R}^i- \vec{x}^P|^2.
\end{split}
\end{equation*}
\sphinxAtStartPar
Summing up over all clusters, we obtain a function analogous to the previously discussed error function:
\begin{equation*}
\begin{split}E (\{R \}) = \sum_{i = 1}^ n \sum_ {P \in C_i} |\vec{R}^i- \vec{x}^P |^2 .\end{split}
\end{equation*}
\sphinxAtStartPar
Its derivative with respect to \( \vec{R}_i \) is
\begin{equation*}
\begin{split} \frac{\partial E (\{R \})}{\partial \vec{R}^i}
= 2 \sum_{P \in C_i} (\vec{R}^i- \vec{x}^P). \end{split}
\end{equation*}
\sphinxAtStartPar
The steepest descent method results \sphinxstylestrong{exactly} in the recipe used in the
dynamic clusterization algorithm presented above, i.e.
\begin{equation*}
\begin{split} \vec{R} \to \vec{R} - \varepsilon (\vec{R} - \vec {x}^P). \end{split}
\end{equation*}
\sphinxAtStartPar
To summarize, the algorithm used here actually involves the steepest descent method for the function \( E (\{R \})\), as discussed in the previous lectures.

\begin{sphinxadmonition}{note}{Informacja:}
\sphinxAtStartPar
Note, however, that the minimization used in the present algorithms also takes into account different combinatorial divisions of points into clusters. In particular, a given data point may change its cluster assignment during the execution of the algorithm. This happens when its closest representative point changes.
\end{sphinxadmonition}


\section{Interpretation via neural networks}
\label{\detokenize{docs/unsupervised:interpretation-via-neural-networks}}\label{\detokenize{docs/unsupervised:inn-sec}}
\sphinxAtStartPar
We shall now interpret the unsupervised learning algorithm used above with the winner\sphinxhyphen{}take\sphinxhyphen{}all strategy in the neural network language. We have the following sample network:

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{unsupervised_74_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
It consists of four neurons in the intermediate neuron layer, each corresponding to one characteristic point \(\vec{R}^i\). The weights are the coordinates of \(\vec{R}^i\). There is one node in the output layer. We note significant differences from the perceptron discussed earlier.
\begin{itemize}
\item {} 
\sphinxAtStartPar
There are no threshold nodes.

\item {} 
\sphinxAtStartPar
In the intermediate layer, the signal equals the distance squared of the input from the corresponding characteristic point. It is not a weighted sum.

\item {} 
\sphinxAtStartPar
The node in the last layer (MIN) indicates in which neuron of the intermediate layer the signal is the smallest, i.e., where we have the shortest distance. Hence it works as a control unit selecting the minimum.

\end{itemize}

\sphinxAtStartPar
During (unsupervised) learning, an input point P „attracts” the closest characteristic point, whose weights are updated towards the coordinates of P.

\sphinxAtStartPar
The application of the above network classifies the point with coordinates \((x_1, x_2)\), assigning it the index of the closest representative point of a given category (here it is the number 1, 2, 3, or 4).


\subsection{Representation with spherical coordinates}
\label{\detokenize{docs/unsupervised:representation-with-spherical-coordinates}}
\sphinxAtStartPar
Even with our vast „mathematical liberty”, calling the above system a neural network would be quite abusive, as it seems very far away from any neurobiological pattern. In particular, the use of a (non\sphinxhyphen{}linear) signal of the form \(\left(\vec{R}^i-\vec{x}\right)^2\) contrasts with the perceptron, where the signal entering the neurons is a (linear) weighted sum of inputs, i.e.
\begin{equation*}
\begin{split} s ^ i = x_1 w_1 ^ i + x_2 w_2 ^ i + ... + w_1 ^ m x_m = \vec {x} \cdot \vec {w} ^ i. \end{split}
\end{equation*}
\sphinxAtStartPar
We can alter our problem with a simple geometric construction/trick to make it more similar to the perceptron principle. For this purpose we introduce a (spurious) third coordinate defined as
\begin{equation*}
\begin{split} x_3 = \sqrt {r ^ 2-x_1 ^ 2-x_2 ^ 2}, \end{split}
\end{equation*}
\sphinxAtStartPar
where \( r \) is chosen such that for all data points \( r ^ 2 \ge x_1 ^ 2 + x_2 ^ 2 \).
From construction, \( \vec {x} \cdot \vec {x} = x_1 ^ 2 + x_2 ^ 2 + x_3 ^ 2 = r ^ 2 \), so the data points lie on the hemisphere (\( x_3 \ge 0 \)) of radius \( r \). Similarly, for the representative points we introduce:
\begin{equation*}
\begin{split} w_1 ^ i = R_1 ^ i,  \; w_2 ^ i = R_2 ^ i,  \; 
w_3 ^ i = \sqrt {r ^ 2-(R_1 ^i)^2 -(R_2 ^i)^2}. \end{split}
\end{equation*}
\sphinxAtStartPar
It is geometrically obvious that two points in a plane are close to each other if and only if their extensions to the hemisphere are close. We support this statement with a simple calculation:

\sphinxAtStartPar
The dot product of two points \( \vec {x} \) and \( \vec {y} \) on a hemisphere can be written as
\begin{equation*}
\begin{split} \vec {x} \cdot \vec {y} = x_1 y_1 + x_2 y_2 + \sqrt {r ^ 2-x_1 ^ 2-x_2 ^ 2} \sqrt {r ^ 2-y_1 ^ 2-y_2 ^ 2}. \end{split}
\end{equation*}
\sphinxAtStartPar
For simplicity, let us consider a situation when \( x_1 ^ 2 + x_2 ^ 2 \ll r ^ 2 \) and \( y_1 ^ 2 + y_2 ^ 2 \ll r ^ 2 \), i.e. both points lie near the pole of the hemisphere. Using your knowledge of mathematical analysis
\begin{equation*}
\begin{split} \sqrt{r^2-a^2} \simeq r - \frac{a^2}{2r},  \;\;\;a \ll r, \end{split}
\end{equation*}
\sphinxAtStartPar
hence

\sphinxAtStartPar
\(\vec{x} \cdot \vec{y} \simeq x_1 y_1 + x_2 y_2 + \left( r -\frac{x_1^2+x_2^2}{2r} \right) \left( r -\frac{y_1^2+y_2^2}{2r} \right) \\ 
\;\;\;\simeq r^2 - \frac{1}{2} (x_1^2+x_2^2 +y_1^2+y_2^2) + x_1 y_1+x_2 y_2 \\ 
\;\;\; = r^2 - \frac{1}{2}[ (x_1-x_2)^2 +(y_1-y_2)^2]\).

\sphinxAtStartPar
It equals (for points close to the pole) the constant \( r ^ 2 \) minus half the square of the distance between the points \( (x_1, x_2) \) and \( (y_1, y_2) \) on the plane! It then follows that instead of finding a minimum distance for points on the plane, as in the previous algorithm, we can find a maximum scalar product for their 3\sphinxhyphen{}dim. extensions to a hemisphere.

\sphinxAtStartPar
With the extension of the data to a hemisphere, the appropriate neural network can be viewed as follows:

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{unsupervised_80_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
Thanks to our efforts, the signal in the intermediate layer is now just a dot product of the input and the weights, as it should be in an artificial neuron. The unit in the last layer (MAX) indicates where the dot product is largest.

\sphinxAtStartPar
This MAX unit is still problematic to interpret within our present framework. Actually, it is possible, but requires going beyond feed\sphinxhyphen{}forward type networks. When the neurons in the layer can communicate (recurrent \sphinxhref{https://en.wikipedia.org/wiki/Hopfield\_network}{Hopfield networks}), they can compete, and with proper feed\sphinxhyphen{}back it is possible to enforce the winner\sphinxhyphen{}take\sphinxhyphen{}all mechanism. We discuss these aspects in section \{ref\}`lat\sphinxhyphen{}lab».

\begin{sphinxadmonition}{note}{Hebbian rule}

\sphinxAtStartPar
On the conceptual side, we touch upon a very important and intuitive principle in biological neural networks, known as the \sphinxhref{https://en.wikipedia.org/wiki/Hebbian\_theory}{Hebbian rule}. Essentially, it applies the truth „What is being used, gets stronger” to synaptic connections. A repeated use of a connection makes it stronger.
\end{sphinxadmonition}

\sphinxAtStartPar
In our formulation, if a signal passes through a given connection, its weight changes accordingly, while other connections remain the same. The process takes place in an unsupervised manner and its implementation is biologically well motivated.

\begin{sphinxadmonition}{note}{Informacja:}
\sphinxAtStartPar
On the other hand, it is difficult to find a biological justification for the backprop supervised learning, where all weights are updated, also in layers very distant from the output. According to many researchers, it is rather a mathematical concept (but nevertheless extremely useful).
\end{sphinxadmonition}


\subsection{Scalar product maximization}
\label{\detokenize{docs/unsupervised:scalar-product-maximization}}
\sphinxAtStartPar
Now the algorithm becomes as as follows:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Extend the points from the sample with the third coordinate, \( x_3 = \sqrt {r ^ 2-x_1 ^ 2-x_2 ^ 2} \), choosing appropriately large \( r \), such that \( r ^ 2> x_1 ^ 2 + x_2 ^ 2 \) for all sample points.

\item {} 
\sphinxAtStartPar
Initialize the weights such that \( \vec {w} _i \cdot \vec {w} _i = r ^ 2 \).

\end{itemize}

\sphinxAtStartPar
Then loop over the data points:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Find the neuron in the intermediate layer for which the dot product \( x \cdot \vec {w} _i \) is the largest. Change the weights of this neuron according to the recipe

\end{itemize}
\begin{equation*}
\begin{split} \vec {w} ^ i \to \vec {w} ^ i + \varepsilon (\vec {x} - \vec {w} ^ i). \end{split}
\end{equation*}\begin{itemize}
\item {} 
\sphinxAtStartPar
Renormalize the updated weight vector \( \vec {w_i} \) such that \( \vec {w} _i \cdot \vec {w} _i = r ^ 2 \):

\end{itemize}
\begin{equation*}
\begin{split} \vec {w} ^ i \to \vec {w} ^ i \frac {r} {\sqrt {\vec {w} _i \cdot \vec {w} _i}}. \end{split}
\end{equation*}
\sphinxAtStartPar
The remaining steps of the algorithm, such as determining the initial positions of the representative points, their dynamic creation as they encounter successive data points, etc., remain exactly as in the previously discussed procedure.

\sphinxAtStartPar
The generalization for \( n \) dimensions is obvious: we enter an additional coordinate
\begin{equation*}
\begin{split} x_ {n + 1} = \sqrt {r ^ 2 - x_1 ^ 2 -...- x_n ^ 2},\end{split}
\end{equation*}
\sphinxAtStartPar
hence we have a point on the hyper\sphinxhyphen{}hemisphere \( x_1 ^ 2 + \dots + x_n ^ 2 + x_ {n + 1} ^ 2 = r ^ 2 \),  \(x_ {n + 1} >0\).

\sphinxAtStartPar
In Python:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{d}\PYG{o}{=}\PYG{l+m+mf}{0.25}
\PYG{n}{eps}\PYG{o}{=}\PYG{l+m+mf}{.5}

\PYG{n}{rad}\PYG{o}{=}\PYG{l+m+mi}{2}    \PYG{c+c1}{\PYGZsh{} radius of the hypersphere}

\PYG{k}{for} \PYG{n}{r} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{25}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{eps}\PYG{o}{=}\PYG{l+m+mf}{0.85}\PYG{o}{*}\PYG{n}{eps}
    \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{shuffle}\PYG{p}{(}\PYG{n}{alls}\PYG{p}{)}
    \PYG{k}{if} \PYG{n}{r}\PYG{o}{==}\PYG{l+m+mi}{0}\PYG{p}{:}
        \PYG{n}{p}\PYG{o}{=}\PYG{n}{alls}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}
        \PYG{n}{R}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{p}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}\PYG{n}{p}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{np}\PYG{o}{.}\PYG{n}{sqrt}\PYG{p}{(}\PYG{n}{rad}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2} \PYG{o}{\PYGZhy{}} \PYG{n}{p}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2} \PYG{o}{\PYGZhy{}} \PYG{n}{p}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}
                                        \PYG{c+c1}{\PYGZsh{} extension of R to the hypersphere}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{alls}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{p}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{alls}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{alls}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} 
                    \PYG{n}{np}\PYG{o}{.}\PYG{n}{sqrt}\PYG{p}{(}\PYG{n}{rad}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2} \PYG{o}{\PYGZhy{}} \PYG{n}{alls}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2} \PYG{o}{\PYGZhy{}} \PYG{n}{alls}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}
                                        \PYG{c+c1}{\PYGZsh{} extension of p to the hypersphere}
        \PYG{n}{dist}\PYG{o}{=}\PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{p}\PYG{p}{,}\PYG{n}{R}\PYG{p}{[}\PYG{n}{k}\PYG{p}{]}\PYG{p}{)} \PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{R}\PYG{p}{)}\PYG{p}{)}\PYG{p}{]}      \PYG{c+c1}{\PYGZsh{} array of dot products}
        \PYG{n}{ind\PYGZus{}max} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{argmax}\PYG{p}{(}\PYG{n}{dist}\PYG{p}{)}                         \PYG{c+c1}{\PYGZsh{} maximum}
        \PYG{k}{if} \PYG{n}{dist}\PYG{p}{[}\PYG{n}{ind\PYGZus{}max}\PYG{p}{]} \PYG{o}{\PYGZlt{}} \PYG{n}{rad}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2} \PYG{o}{\PYGZhy{}} \PYG{n}{d}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{:}
             \PYG{n}{R}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{R}\PYG{p}{,} \PYG{p}{[}\PYG{n}{p}\PYG{p}{]}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
        \PYG{k}{else}\PYG{p}{:}   
            \PYG{n}{R}\PYG{p}{[}\PYG{n}{ind\PYGZus{}max}\PYG{p}{]}\PYG{o}{+}\PYG{o}{=}\PYG{n}{eps}\PYG{o}{*}\PYG{p}{(}\PYG{n}{p}\PYG{o}{\PYGZhy{}}\PYG{n}{R}\PYG{p}{[}\PYG{n}{ind\PYGZus{}max}\PYG{p}{]}\PYG{p}{)}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Number of representative points: }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{R}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
Number of representative points:  4
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{unsupervised_87_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
We can promptly see that the dot product maximization algorithm yields an almost exactly the same result as the distance squared minimization (cf. \hyperref[\detokenize{docs/unsupervised:dyn-fig}]{Rys.\@ \ref{\detokenize{docs/unsupervised:dyn-fig}}}.


\section{Exercises}
\label{\detokenize{docs/unsupervised:exercises}}
\begin{sphinxadmonition}{note}{\protect\(~\protect\)}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
The city (Manhattan) metric is defined as
\( d (\vec {x}, \vec {y}) = | x_1-y_1 | + | x_2 - y_2 | \) for points \( \vec {x} \) and \( \vec {y} \).
Repeat the simulations of this chapter using this metric. Draw conclusions.

\item {} 
\sphinxAtStartPar
Run the classification algorithms for more categories in the data sample (generate your own sample).

\item {} 
\sphinxAtStartPar
Extend the dynamic clusterization algorithm to a three\sphinxhyphen{}dimensional input space.

\end{enumerate}
\end{sphinxadmonition}


\chapter{Self Organizing Maps}
\label{\detokenize{docs/som:self-organizing-maps}}\label{\detokenize{docs/som::doc}}
\sphinxAtStartPar
A very important and ingenious application of unsupervised learning are the so\sphinxhyphen{}called \sphinxstylestrong{Kohonen networks} (\sphinxhref{https://en.wikipedia.org/wiki/Teuvo\_Kohonen}{Teuvo Kohonen}, a class of \sphinxstylestrong{self\sphinxhyphen{}organizing mappings (SOM)}. Consider firs a mapping \(f\) between a \sphinxstylestrong{discrete} \(k\)\sphinxhyphen{}dimensional set (we call it a \sphinxstylestrong{grid} in this chapter) of neurons and \(n\)\sphinxhyphen{}dimensional input data \(D\) (continuous or discrete),
\begin{equation*}
\begin{split}
f: N \to D
\end{split}
\end{equation*}
\sphinxAtStartPar
(note that \sphinxstylestrong{this is not a Kohonen mapping yet!}).
Since \(N\) is discrete, each neuron carries an index consisting of \(k\) natural numbers, denoted as \(\bar {i} = (i_1, i_2, ..., i_k)\). Typically, the dimensions in Kohonen’s networks satisfy \(n \ge k\). When \(n > k\), one talks about \sphinxstylestrong{reduction of dimensionality}, as then the input space \(D\) has more dimensions than the dimensionaiy of the grid of neurons \(N\).

\sphinxAtStartPar
Two examples of such networks are visualized in \hyperref[\detokenize{docs/som:koh-fig}]{Rys.\@ \ref{\detokenize{docs/som:koh-fig}}}. The left panel shows a 2\sphinxhyphen{}dim. input space \(D\), and a one dimensional grid on neurons labeled with \(i\). The input point \((x_1,x_2)\) enters all the neurons in the grid, and one of the neurons (the one with best\sphinxhyphen{}suited weights) becomes the \sphinxstylestrong{winner} (red dot). The gray oval indicates the \sphinxstylestrong{neighborhood} of the winner, to be defined accurately in the following.

\sphinxAtStartPar
The right panel shows an analogous situation for the case of a 3\sphinxhyphen{}dim. input and 2\sphinxhyphen{}dim. grid of neurons, now labeled with a double index \(\bar {i} = (i_1, i_2)\). Here, for clarity, we only indicate the edges entering the winner, but they also enter all the other neurons in the grid, similarly to the left panel.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=500\sphinxpxdimen]{{koha}.png}
\caption{Example of Kohonen’s networks. Left: 1\sphinxhyphen{}dim. grid of neurons \(N\) and 2\sphinxhyphen{}dim. input space \(D\). Right: 2\sphinxhyphen{}dim. grid of neurons \(N\) and 3\sphinxhyphen{}dim. input space \(D\). The red dot indicates the winner, and the gray oval marks its neighborhood.}\label{\detokenize{docs/som:koh-fig}}\end{figure}

\sphinxAtStartPar
Next, one defines the neuron \sphinxstylestrong{proximity function}, \(\phi (\bar {i}, \bar {j})\), which assigns, to a pair of neurons, a real number depending on their relative position in the grid. This function must decrease with the distance between the neuron indices. A popular choice is a Gaussian,
\begin{equation*}
\begin{split} \phi(\bar{i}, \bar{j})=\exp\left [ -\frac{(i_1-j_1)^2+...+(i_k-j_k)^2}{2 \delta^2} \right ] ,\end{split}
\end{equation*}
\sphinxAtStartPar
where \(\delta\) is the \sphinxstylestrong{neighborhood radius}. For a 1\sphinxhyphen{}dim. grid we have \( \phi(i,j)=\exp\left [ -\frac{(i-j)^2}{2 \delta^2} \right ]\).


\section{Kohonen’s algorithm}
\label{\detokenize{docs/som:kohonen-s-algorithm}}
\sphinxAtStartPar
The set up for Kohonen’s algorithm is similar to the unsupervised learning discussed in the previous chapter. Each neuron \(\bar{i}\) obtains weights \(f\left(\bar{i}\right)\), which are elements of \(D\), i.e. form \(n\)\sphinxhyphen{}dimensional vectors. One may simply think of this procedure as placing the neurons in some locations in \(D\).

\sphinxAtStartPar
When an input point \(P\) from \(D\) is fed into the network, one looks for the closest neuron, which becomes the \sphinxstylestrong{winner}, exactly as in the unsupervised learning algorithm from section {\hyperref[\detokenize{docs/unsupervised:inn-sec}]{\sphinxcrossref{\DUrole{std,std-ref}{Interpretation via neural networks}}}}. However, now comes a \sphinxstylestrong{crucial difference}: Not only the winner is attracted (updated) a bit towards \(P\), but also its neighbors, to a lesser and lesser extent the farther they are from the winner, as quantified by the proximity function.

\begin{sphinxadmonition}{note}{Winner\sphinxhyphen{}take\sphinxhyphen{}most strategy}

\sphinxAtStartPar
Kohonen’s algorithm involves the „winner take most” strategy, where not only the winner neuron is updated (as in the winner\sphinxhyphen{}take\sphinxhyphen{}all case), but also its neighbors. The neighbors update is strongest for the nearest neighbors, and gradually weakens with the distance from the winner, as given by the proximity function.
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{Kohnen’s algorithm}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Initialize (for instance randomly) \(n\)\sphinxhyphen{}dimensional weight vectors \(w_i\), \(i-1,\dots,m\) for all the \(m\) neurons in the grid. Set an an initial neighborhood radius \( \delta \) and an initial learning speed \( \varepsilon \).

\item {} 
\sphinxAtStartPar
Choose (for instance, randomly) a data point \(P\) with coordinates \(x\) from the input space (possibly with an appropriate probability distribution).

\item {} 
\sphinxAtStartPar
Find the neuron (the winner) for which the distance from \(P\) is the smallest. Denote its index as \( \bar {l} \).

\item {} 
\sphinxAtStartPar
The weights of the winner and its neighbors are updated according to the \sphinxstylestrong{winner\sphinxhyphen{}take\sphinxhyphen{}most} recipe:

\end{enumerate}
\begin{equation*}
\begin{split}w_{\bar{i}} \to w_{\bar{i}} + \varepsilon \phi(\bar{i}, \bar{l})(x - w_{\bar{i}}), \hspace{1cm} i=1, \dots , m. 
\end{split}
\end{equation*}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Loop from \(1.\) for a specified number of points.

\item {} 
\sphinxAtStartPar
Repeat from \(1.\) in rounds, until a satisfactory result is obtained or a stopping criterion is reached. In each round  \sphinxstylestrong{reduce} \( \varepsilon \) and \( \delta \) according to a chosen policy.

\end{enumerate}
\end{sphinxadmonition}

\begin{sphinxadmonition}{important}{Ważne:}
\sphinxAtStartPar
The way the reduction of \( \varepsilon \) and \( \delta \) is done is very important for the desired outcome of the algorithm (see exercises).
\end{sphinxadmonition}


\subsection{2\sphinxhyphen{}dim. data and 1\sphinxhyphen{}dim. neuron grid}
\label{\detokenize{docs/som:dim-data-and-1-dim-neuron-grid}}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{num}\PYG{o}{=}\PYG{l+m+mi}{100} \PYG{c+c1}{\PYGZsh{} number of neurons}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
and the Gaussian proximity function

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{phi}\PYG{p}{(}\PYG{n}{i}\PYG{p}{,}\PYG{n}{k}\PYG{p}{,}\PYG{n}{d}\PYG{p}{)}\PYG{p}{:}                       \PYG{c+c1}{\PYGZsh{} proximity function}
    \PYG{k}{return} \PYG{n}{np}\PYG{o}{.}\PYG{n}{exp}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{i}\PYG{o}{\PYGZhy{}}\PYG{n}{k}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{o}{/}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{o}{*}\PYG{n}{d}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} Gaussian}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
This function looks as follows around the middle neuron (\(k=50\)) and for the width parameter \(\delta=5\):

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{som_17_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
As a feature of a Gaussian, at \(|k-i|=\delta\) the function drops to \(~60\%\) of the central value, and at \(|k-i|=3\delta\) to \(~1\%\), a tiny fraction. Hence \(\delta\) controls the size of the neighborhood of the winner. The neurons farther away from the winner than, say, \(3\delta\) are practically left uncharged.

\sphinxAtStartPar
We initiate the network by by placing the grid inside the circle, with a random location of each neuron. As said, this amounts to assigning weights to the neuron equal to its location. An auxiliary line is drawn to guide the eye sequentially along the neuron indices: \(1,2,3,\dots m\). The line has no other meaning.

\sphinxAtStartPar
The weights (neuron locations) are stored in array \sphinxstylestrong{W}:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{W}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{func}\PYG{o}{.}\PYG{n}{point\PYGZus{}c}\PYG{p}{(}\PYG{p}{)} \PYG{k}{for} \PYG{n}{\PYGZus{}} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{num}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} random initialization of weights}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
As a result of the initial randomness, the neurons are, of course, „chaotically” distributed:

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{som_21_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
Next, we initialize the parameters \sphinxstylestrong{eps} amd \sphinxstylestrong{delta} and run the algorithm. Its structure is analogous to the previously discussed codes and is a straightforward implementation of the steps spelled out in the previous section. For that reason, we only provide the comments in the code.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{eps}\PYG{o}{=}\PYG{l+m+mf}{.5}   \PYG{c+c1}{\PYGZsh{} initial learning speed }
\PYG{n}{de} \PYG{o}{=} \PYG{l+m+mi}{10}  \PYG{c+c1}{\PYGZsh{} initial neighborhood distance}
\PYG{n}{ste}\PYG{o}{=}\PYG{l+m+mi}{0}    \PYG{c+c1}{\PYGZsh{} inital number of caried out steps}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Kohonen\PYGZsq{}s algorithm}
\PYG{k}{for} \PYG{n}{\PYGZus{}} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{150}\PYG{p}{)}\PYG{p}{:}              \PYG{c+c1}{\PYGZsh{} rounds}
    \PYG{n}{eps}\PYG{o}{=}\PYG{n}{eps}\PYG{o}{*}\PYG{l+m+mf}{.98}                   \PYG{c+c1}{\PYGZsh{} dicrease learning speed}
    \PYG{n}{de}\PYG{o}{=}\PYG{n}{de}\PYG{o}{*}\PYG{l+m+mf}{.95}                     \PYG{c+c1}{\PYGZsh{} ... and the neighborhood distance}
    \PYG{k}{for} \PYG{n}{\PYGZus{}} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{100}\PYG{p}{)}\PYG{p}{:}          \PYG{c+c1}{\PYGZsh{} loop over points}
        \PYG{n}{p}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{point\PYGZus{}c}\PYG{p}{(}\PYG{p}{)}          \PYG{c+c1}{\PYGZsh{} random point}
        \PYG{n}{ste}\PYG{o}{=}\PYG{n}{ste}\PYG{o}{+}\PYG{l+m+mi}{1}                 \PYG{c+c1}{\PYGZsh{} count steps}
        \PYG{n}{dist}\PYG{o}{=}\PYG{p}{[}\PYG{n}{func}\PYG{o}{.}\PYG{n}{eucl}\PYG{p}{(}\PYG{n}{p}\PYG{p}{,}\PYG{n}{W}\PYG{p}{[}\PYG{n}{k}\PYG{p}{]}\PYG{p}{)} \PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{num}\PYG{p}{)}\PYG{p}{]} 
         \PYG{c+c1}{\PYGZsh{} array of squares of Euclidean disances between p and the neuron locations}
        \PYG{n}{ind\PYGZus{}min} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{argmin}\PYG{p}{(}\PYG{n}{dist}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} index of the winner}
        \PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{num}\PYG{p}{)}\PYG{p}{:}      \PYG{c+c1}{\PYGZsh{} loop over all the neurons}
            \PYG{n}{W}\PYG{p}{[}\PYG{n}{k}\PYG{p}{]}\PYG{o}{+}\PYG{o}{=}\PYG{n}{eps}\PYG{o}{*}\PYG{n}{phi}\PYG{p}{(}\PYG{n}{ind\PYGZus{}min}\PYG{p}{,}\PYG{n}{k}\PYG{p}{,}\PYG{n}{de}\PYG{p}{)}\PYG{o}{*}\PYG{p}{(}\PYG{n}{p}\PYG{o}{\PYGZhy{}}\PYG{n}{W}\PYG{p}{[}\PYG{n}{k}\PYG{p}{]}\PYG{p}{)} 
             \PYG{c+c1}{\PYGZsh{} update of the neuron locations (weights), depending on proximity}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
As the above algorithm progresses (see \hyperref[\detokenize{docs/som:kohstory-fig}]{Rys.\@ \ref{\detokenize{docs/som:kohstory-fig}}}) the neuron grid first disentangles, and then gradually fills the whole space \(D\) (circle) in such a way that the neurons with adjacent indices are located close to each other.
Figuratively speaking, a new point \(P\) attracts towards itself the nearest neuron (the winner), but also, to a weaker extent, its neighbors. At the beginning of the algorithm the neighborhood distance \sphinxstylestrong{de} is large, so large chunks of the neighboring neurons in the input grid are pulled together towards \(P\), and the arrangement looks as in the top right corner of \hyperref[\detokenize{docs/som:kohstory-fig}]{Rys.\@ \ref{\detokenize{docs/som:kohstory-fig}}}. At later stages \sphinxstylestrong{de} reduces, so only the winner and possibly its very immediate neighbors are attracted to a new point.
After completion (bottom right panel), individual neurons „specialize” (are close to) in a certain data area.

\sphinxAtStartPar
In the present example, after about 20000 steps the result practically stops to change.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{kaall}.png}
\caption{Progress of Kohonen’s algorithm. The line, drawn to guide the eye, connects neurons with adjacent indices.}\label{\detokenize{docs/som:kohstory-fig}}\end{figure}

\begin{sphinxadmonition}{note}{Kohonen’s network as a classifier}

\sphinxAtStartPar
Having the trained network, we may use it as a classifier similarly as in chapter {\hyperref[\detokenize{docs/unsupervised:un-lab}]{\sphinxcrossref{\DUrole{std,std-ref}{Unsupervised learning}}}}. We label a point from \(D\) with the index of the nearest neuron. One can interpret this as a Voronoi construction, see section {\hyperref[\detokenize{docs/unsupervised:vor-lab}]{\sphinxcrossref{\DUrole{std,std-ref}{Voronoi areas}}}}.
\end{sphinxadmonition}

\sphinxAtStartPar
The plots in \hyperref[\detokenize{docs/som:kohstory-fig}]{Rys.\@ \ref{\detokenize{docs/som:kohstory-fig}}} are made in coordinates \((x_1,x_2)\), that is, from the „point of view” of the input \(D\)\sphinxhyphen{}space. One may also look at the result from the point of view of the \(N\)\sphinxhyphen{}space, i.e. plot \(x_1\) and \(x_2\) as functions of the neuron index \(i\).

\begin{sphinxadmonition}{note}{Caution}

\sphinxAtStartPar
When presenting results of Kohonen’s algorithm, one sometimes makes plots in \(D\)\sphinxhyphen{}space, and sometimes in \(N\)\sphinxhyphen{}space, which may lead to some confusion.
\end{sphinxadmonition}

\sphinxAtStartPar
The plots in the \(N\)\sphinxhyphen{}space, fully equivalent in information to the plot in, e.g., the bottom right panel of \hyperref[\detokenize{docs/som:kohstory-fig}]{Rys.\@ \ref{\detokenize{docs/som:kohstory-fig}}}, are following:

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{som_30_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
We note that the jumps in the above plotted curves are small, since the subsequent neurons are close to each other. This feature can be presented quantitatively as in the histogram below, where we can see that the average distance between the neurons is about 0.07, and the spread is between 0.05 and 0.10.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{dd}\PYG{o}{=}\PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{sqrt}\PYG{p}{(}\PYG{p}{(}\PYG{n}{W}\PYG{p}{[}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{n}{W}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{o}{+}\PYG{p}{(}\PYG{n}{W}\PYG{p}{[}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{n}{W}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{)} \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{num}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{]}
        \PYG{c+c1}{\PYGZsh{} array of distances between subsequent neurons in the grid}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mf}{2.8}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{,}\PYG{n}{dpi}\PYG{o}{=}\PYG{l+m+mi}{120}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{distance}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{distribution}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{hist}\PYG{p}{(}\PYG{n}{dd}\PYG{p}{,} \PYG{n}{bins}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{n}{density}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}   \PYG{c+c1}{\PYGZsh{} histogram}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{som_32_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\begin{sphinxadmonition}{note}{Remarks}
\begin{itemize}
\item {} 
\sphinxAtStartPar
We took a situation in which the data space with the dimension \(n = 2\) is „sampled” by a discrete set of neurons forming  \(k=1\)\sphinxhyphen{}dimensional grid. Hence we encounter dimensional reduction.

\item {} 
\sphinxAtStartPar
The outcome of the algorithm is a network in which a given neuron „focuses” on data from its vicinity. In a general case, where the data can be non\sphinxhyphen{}uniformly distributed, the neurons would fill the area containing more data more densely.

\item {} 
\sphinxAtStartPar
The policy of choosing initial \(\delta\) and \(\varepsilon \) parameters and reducing them appropriately in subsequent rounds is based on experience and is non\sphinxhyphen{}trivial. The results depend significantly on this choice.

\item {} 
\sphinxAtStartPar
The final result, even with the same \(\delta\) and \(\varepsilon \) strategy, is not unequivocal, i.e. running the algorithm with a different initialization of the weights (initial positions of neurons) yields different outcomes, usually equally „good”.

\item {} 
\sphinxAtStartPar
Finally, the progress and the result of the algorithm is reminiscent of the construction of the \sphinxhref{https://en.wikipedia.org/wiki/Peano\_curve}{Peano curve} in mathematics, which fills densely an area with a line.
As we increase the number of neurons, the analogy gets closer and closer.

\end{itemize}
\end{sphinxadmonition}


\subsection{2 dim. color map}
\label{\detokenize{docs/som:dim-color-map}}
\sphinxAtStartPar
Now we pass to a case of 3\sphinxhyphen{}dim. data and 2\sphinxhyphen{}dim. neuron grid, which is a situation from the right panel of \hyperref[\detokenize{docs/som:koh-fig}]{Rys.\@ \ref{\detokenize{docs/som:koh-fig}}} (hence also with dimensionality reduction). As we know, an RGB color is described with three numbers \([r,g,b]\) from \([0,1]\), so it can nicely serve as input in our example.

\sphinxAtStartPar
The distance squared between two colors (this is just a distance between two points in the 3\sphinxhyphen{}dim. space) is taken in the Euclidean form:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{dist3}\PYG{p}{(}\PYG{n}{p1}\PYG{p}{,}\PYG{n}{p2}\PYG{p}{)}\PYG{p}{:} 
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    Square of the Euclidean distance between points p1 and p2}
\PYG{l+s+sd}{    in 3 dimensions.}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{k}{return} \PYG{p}{(}\PYG{n}{p1}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{n}{p2}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{o}{+}\PYG{p}{(}\PYG{n}{p1}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{n}{p2}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{o}{+}\PYG{p}{(}\PYG{n}{p1}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{n}{p2}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
The proximity function is now a Gaussian in two dimensions:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{phi2}\PYG{p}{(}\PYG{n}{ix}\PYG{p}{,}\PYG{n}{iy}\PYG{p}{,}\PYG{n}{kx}\PYG{p}{,}\PYG{n}{ky}\PYG{p}{,}\PYG{n}{d}\PYG{p}{)}\PYG{p}{:}  \PYG{c+c1}{\PYGZsh{} proximity function for 2\PYGZhy{}dim. grid}
    \PYG{k}{return} \PYG{n}{np}\PYG{o}{.}\PYG{n}{exp}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{p}{(}\PYG{n}{ix}\PYG{o}{\PYGZhy{}}\PYG{n}{kx}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{o}{+}\PYG{p}{(}\PYG{n}{iy}\PYG{o}{\PYGZhy{}}\PYG{n}{ky}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{o}{/}\PYG{p}{(}\PYG{n}{d}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Gaussian}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
We also decide to normalize the RGB colors such that \(r^2+g^2+b^2=1\). This makes the perceived intensity of colors similar (this normalization could be dropped, as irrelevant for the method to work).

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{rgbn}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{r}\PYG{p}{,}\PYG{n}{g}\PYG{p}{,}\PYG{n}{b}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} random RGB}
    \PYG{n}{norm}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{sqrt}\PYG{p}{(}\PYG{n}{r}\PYG{o}{*}\PYG{n}{r}\PYG{o}{+}\PYG{n}{g}\PYG{o}{*}\PYG{n}{g}\PYG{o}{+}\PYG{n}{b}\PYG{o}{*}\PYG{n}{b}\PYG{p}{)}                                      \PYG{c+c1}{\PYGZsh{} norm}
    \PYG{k}{return} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{r}\PYG{p}{,}\PYG{n}{g}\PYG{p}{,}\PYG{n}{b}\PYG{p}{]}\PYG{o}{/}\PYG{n}{norm}\PYG{p}{)}                                  \PYG{c+c1}{\PYGZsh{} normalized RGB}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
Next, we generate and plot a sample of \sphinxstylestrong{ns} points with (normalized) RGB colors:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{ns}\PYG{o}{=}\PYG{l+m+mi}{40}                            \PYG{c+c1}{\PYGZsh{} number of colors in the sample}
\PYG{n}{samp}\PYG{o}{=}\PYG{p}{[}\PYG{n}{rgbn}\PYG{p}{(}\PYG{p}{)} \PYG{k}{for} \PYG{n}{\PYGZus{}} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ns}\PYG{p}{)}\PYG{p}{]} \PYG{c+c1}{\PYGZsh{} random sample}

\PYG{n}{pls}\PYG{o}{=}\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{4}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{,}\PYG{n}{dpi}\PYG{o}{=}\PYG{l+m+mi}{120}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{axis}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{off}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ns}\PYG{p}{)}\PYG{p}{:} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{i}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{n}{color}\PYG{o}{=}\PYG{n}{samp}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{15}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}    
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{som_42_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
We use a 2\sphinxhyphen{}dim. \sphinxstylestrong{size} x \sphinxstylestrong{size} grid of neurons. Each neuron’s position (that is its color) in the 3\sphinxhyphen{}dim. \(D\)\sphinxhyphen{}space is initialized randomly:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{size}\PYG{o}{=}\PYG{l+m+mi}{40}                        \PYG{c+c1}{\PYGZsh{} neuron array of size x size (40 x 40)}
\PYG{n}{tab}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{p}{(}\PYG{n}{size}\PYG{p}{,}\PYG{n}{size}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{)}    \PYG{c+c1}{\PYGZsh{} create array tab with zeros  }

\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{size}\PYG{p}{)}\PYG{p}{:}          \PYG{c+c1}{\PYGZsh{} i index in the grid    }
    \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{size}\PYG{p}{)}\PYG{p}{:}      \PYG{c+c1}{\PYGZsh{} j index in the grid}
        \PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{:}     \PYG{c+c1}{\PYGZsh{} RGB: k=0\PYGZhy{}red, 1\PYGZhy{}green, 2\PYGZhy{}blue}
            \PYG{n}{tab}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{n}{j}\PYG{p}{,}\PYG{n}{k}\PYG{p}{]}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} random number form [0,1]}
            \PYG{c+c1}{\PYGZsh{} 3 RGB components for neuron in the grid positin (i,j)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{som_45_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
Now we are ready to run Kohonen’s algorithm:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{eps}\PYG{o}{=}\PYG{l+m+mf}{.5}   \PYG{c+c1}{\PYGZsh{} initial parameters}
\PYG{n}{de} \PYG{o}{=} \PYG{l+m+mi}{20}  
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{for} \PYG{n}{\PYGZus{}} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{150}\PYG{p}{)}\PYG{p}{:}    \PYG{c+c1}{\PYGZsh{} rounds}
    \PYG{n}{eps}\PYG{o}{=}\PYG{n}{eps}\PYG{o}{*}\PYG{l+m+mf}{.995}      
    \PYG{n}{de}\PYG{o}{=}\PYG{n}{de}\PYG{o}{*}\PYG{l+m+mf}{.96}           \PYG{c+c1}{\PYGZsh{} de shrinks a bit faster than eps     }
    \PYG{k}{for} \PYG{n}{s} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ns}\PYG{p}{)}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} loop over the points in the data sample       }
        \PYG{n}{p}\PYG{o}{=}\PYG{n}{samp}\PYG{p}{[}\PYG{n}{s}\PYG{p}{]}       \PYG{c+c1}{\PYGZsh{} point from the sample}
        \PYG{n}{dist}\PYG{o}{=}\PYG{p}{[}\PYG{p}{[}\PYG{n}{dist3}\PYG{p}{(}\PYG{n}{p}\PYG{p}{,}\PYG{n}{tab}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{p}{)} \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{size}\PYG{p}{)}\PYG{p}{]} \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{size}\PYG{p}{)}\PYG{p}{]} 
                        \PYG{c+c1}{\PYGZsh{} distance of p from all neurons}
        \PYG{n}{ind\PYGZus{}min} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{argmin}\PYG{p}{(}\PYG{n}{dist}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} the winner index}
        \PYG{n}{ind\PYGZus{}1}\PYG{o}{=}\PYG{n}{ind\PYGZus{}min}\PYG{o}{/}\PYG{o}{/}\PYG{n}{size}       \PYG{c+c1}{\PYGZsh{} a trick to get a 2\PYGZhy{}dim index}
        \PYG{n}{ind\PYGZus{}2}\PYG{o}{=}\PYG{n}{ind\PYGZus{}min}\PYG{o}{\PYGZpc{}}\PYG{k}{size}

        \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{size}\PYG{p}{)}\PYG{p}{:} 
            \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{size}\PYG{p}{)}\PYG{p}{:}
                \PYG{n}{tab}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{o}{+}\PYG{o}{=}\PYG{n}{eps}\PYG{o}{*}\PYG{n}{phi2}\PYG{p}{(}\PYG{n}{ind\PYGZus{}1}\PYG{p}{,}\PYG{n}{ind\PYGZus{}2}\PYG{p}{,}\PYG{n}{i}\PYG{p}{,}\PYG{n}{j}\PYG{p}{,}\PYG{n}{de}\PYG{p}{)}\PYG{o}{*}\PYG{p}{(}\PYG{n}{p}\PYG{o}{\PYGZhy{}}\PYG{n}{tab}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} update         }
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
A word of explanation is in place here, concerning the numpy \sphinxstylestrong{argmin} function. For a 2\sphinxhyphen{}dim. array it provides the index of the minimum in the corresponding \sphinxstylestrong{flattened} array (cf. section {\hyperref[\detokenize{docs/memory:het-lab}]{\sphinxcrossref{\DUrole{std,std-ref}{Pamieć skojarzeniowa (heteroasocjacyjna)}}}}). Hence, to get the indices in the two dimensions, we need to apply the operations \sphinxstylestrong{//} (integer division) and \sphinxstylestrong{\%} (remainder). For instance, in an array \sphinxstylestrong{ind\_min=53}, then \sphinxstylestrong{ind\_1=ind\_min//size=53//10=5} and \sphinxstylestrong{ind\_2=ind\_min\%size=53//10=3}.

\sphinxAtStartPar
As a result of the above code, we get an arrangement of our color sample in two dimensions in such a way that the neighboring areas in the grid have a similar color „specializing” on the color of a given sample point (note the plot is in the \(N\)\sphinxhyphen{}space):

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mf}{2.3}\PYG{p}{,}\PYG{l+m+mf}{2.3}\PYG{p}{)}\PYG{p}{,}\PYG{n}{dpi}\PYG{o}{=}\PYG{l+m+mi}{120}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Kohonen color map}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)} 

\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{size}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{size}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{i}\PYG{p}{,}\PYG{n}{j}\PYG{p}{,}\PYG{n}{color}\PYG{o}{=}\PYG{n}{tab}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{8}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZdl{}i\PYGZdl{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZdl{}j\PYGZdl{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{som_51_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\begin{sphinxadmonition}{note}{Remarks}
\begin{itemize}
\item {} 
\sphinxAtStartPar
The areas for the individual colors of the sample have a comparable area. Generally, the area is proportional to the frequency of the data point in the sample.

\item {} 
\sphinxAtStartPar
To get sharper boundaries between the regions, \sphinxstylestrong{de} would have to shrink even faster compared to \sphinxstylestrong{eps}. Then, in the final stage of learning, the neuron update process takes place within a smaller neighborhood radius and more resolution in the boundaries can be achieved.

\end{itemize}
\end{sphinxadmonition}


\section{\protect\(U\protect\)\sphinxhyphen{}matrix}
\label{\detokenize{docs/som:u-matrix}}
\sphinxAtStartPar
A convenient way to present the results of Kohonen’s algorithm when the grid is 2\sphinxhyphen{}dimensional is via the \sphinxstylestrong{unified distance matrix} (shortly \sphinxstylestrong{\(U\)\sphinxhyphen{}matrix}). The idea is to plot a 2\sphinxhyphen{}dimensional grayscale map in \(N\)\sphinxhyphen{}space with the intensity given by the averaged distance (in \(D\)\sphinxhyphen{}space) of the given neuron to its immediate neighbors, and not a neuron property itself (such as its color in the figure above). This is particularly useful when the dimension of the input space is large, hence it is difficult to visualize the results directly.

\sphinxAtStartPar
The definition of a \(U\)\sphinxhyphen{}matrix element \(U_{ij}\) is explained in \hyperref[\detokenize{docs/som:udm-fig}]{Rys.\@ \ref{\detokenize{docs/som:udm-fig}}}. Let \(d\) be the distance in \(D\)\sphinxhyphen{}space and \([i,j]\) denote the neuron of indices \(i,j\) . We take
\begin{equation*}
\begin{split}
U_{ij}=\sqrt{d\left([i,j],[i+1,j]\right)^2+d\left([i,j],[i-1,j]\right)^2+
        d\left([i,j],[i,j+1]\right)^2+d\left([i,j],[i,j-1]\right)^2 }.
\end{split}
\end{equation*}
\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=150\sphinxpxdimen]{{udm}.png}
\caption{Construction of \(U_{ij}\): a geometric average of the distances along the indicated links.}\label{\detokenize{docs/som:udm-fig}}\end{figure}

\sphinxAtStartPar
The Python implementation of the above definition is following:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{udm}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{p}{(}\PYG{n}{size}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{n}{size}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{)}    \PYG{c+c1}{\PYGZsh{} initiaize U\PYGZhy{}matrix with elements set to 0}

\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{n}{size}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}        \PYG{c+c1}{\PYGZsh{} loops over the neurons in the grid}
    \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{n}{size}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{udm}\PYG{p}{[}\PYG{n}{i}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{[}\PYG{n}{j}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{sqrt}\PYG{p}{(}\PYG{n}{dist3}\PYG{p}{(}\PYG{n}{tab}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{p}{,}\PYG{n}{tab}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{n}{j}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{o}{+}\PYG{n}{dist3}\PYG{p}{(}\PYG{n}{tab}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{p}{,}\PYG{n}{tab}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{n}{j}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{o}{+}
                            \PYG{n}{dist3}\PYG{p}{(}\PYG{n}{tab}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{p}{,}\PYG{n}{tab}\PYG{p}{[}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{p}{)}\PYG{o}{+}\PYG{n}{dist3}\PYG{p}{(}\PYG{n}{tab}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{p}{,}\PYG{n}{tab}\PYG{p}{[}\PYG{n}{i}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}
                                 \PYG{c+c1}{\PYGZsh{} U\PYGZhy{}matrix as explained above}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
The result, corresponding one\sphinxhyphen{}to\sphinxhyphen{}one to the color map above, can be presented in a contour plot:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mf}{2.3}\PYG{p}{,}\PYG{l+m+mf}{2.3}\PYG{p}{)}\PYG{p}{,}\PYG{n}{dpi}\PYG{o}{=}\PYG{l+m+mi}{120}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{U\PYGZhy{}matrix}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)} 

\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{size}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} loops over indices, excluding the boundaries of the grid}
    \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{size}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{n}{j}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{n}{color}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{o}{*}\PYG{n}{udm}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{p}{]}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)} 
                        \PYG{c+c1}{\PYGZsh{} color format: [R,G,B,intensity], 2 just scales up}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZdl{}i\PYGZdl{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZdl{}j\PYGZdl{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{som_59_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
The white regions in the above figure show the clusters (they correspond one\sphinxhyphen{}to\sphinxhyphen{}one to the regions of the same color in the previously shown color map). There, the elements \(U_{ij} \simeq 0\). The clusters are separated with darker boundaries. The higher the dividing ridge between clusters, the darker the intensity.

\sphinxAtStartPar
The result may also be visualized with a 3\sphinxhyphen{}dim. plot:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{fig} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{4}\PYG{p}{,}\PYG{l+m+mi}{4}\PYG{p}{)}\PYG{p}{,}\PYG{n}{dpi}\PYG{o}{=}\PYG{l+m+mi}{120}\PYG{p}{)}
\PYG{n}{axes1} \PYG{o}{=} \PYG{n}{fig}\PYG{o}{.}\PYG{n}{add\PYGZus{}subplot}\PYG{p}{(}\PYG{l+m+mi}{111}\PYG{p}{,} \PYG{n}{projection}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{3d}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{ax} \PYG{o}{=} \PYG{n}{fig}\PYG{o}{.}\PYG{n}{gca}\PYG{p}{(}\PYG{p}{)}

\PYG{n}{xx\PYGZus{}1} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{size}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{xx\PYGZus{}2} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{size}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}

\PYG{n}{x\PYGZus{}1}\PYG{p}{,} \PYG{n}{x\PYGZus{}2} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{meshgrid}\PYG{p}{(}\PYG{n}{xx\PYGZus{}1}\PYG{p}{,} \PYG{n}{xx\PYGZus{}2}\PYG{p}{)}

\PYG{n}{Z}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{n}{udm}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]} \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{size}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{]} \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{size}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}

\PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}zlim}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mf}{.5}\PYG{p}{)}

\PYG{n}{ax}\PYG{o}{.}\PYG{n}{plot\PYGZus{}surface}\PYG{p}{(}\PYG{n}{x\PYGZus{}1}\PYG{p}{,}\PYG{n}{x\PYGZus{}2}\PYG{p}{,} \PYG{n}{Z}\PYG{p}{,} \PYG{n}{cmap}\PYG{o}{=}\PYG{n}{cm}\PYG{o}{.}\PYG{n}{gray}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZdl{}i\PYGZdl{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZdl{}j\PYGZdl{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{U\PYGZhy{}matrix}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{som_61_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
We can now classify a given (new) data point according to the obtained map. We generate a new (normalized) RGB color:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{nd}\PYG{o}{=}\PYG{n}{rgbn}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{som_64_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
It is useful to obtain a map of distances of our grid neurons from this point:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{tad}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{p}{(}\PYG{n}{size}\PYG{p}{,}\PYG{n}{size}\PYG{p}{)}\PYG{p}{)}

\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{size}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{size}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{tad}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{o}{=}\PYG{n}{dist3}\PYG{p}{(}\PYG{n}{nd}\PYG{p}{,}\PYG{n}{tab}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{p}{)}
        

\PYG{n}{ind\PYGZus{}m} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{argmin}\PYG{p}{(}\PYG{n}{tad}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} winner}
\PYG{n}{in\PYGZus{}x}\PYG{o}{=}\PYG{n}{ind\PYGZus{}m}\PYG{o}{/}\PYG{o}{/}\PYG{n}{size}      
\PYG{n}{in\PYGZus{}y}\PYG{o}{=}\PYG{n}{ind\PYGZus{}m}\PYG{o}{\PYGZpc{}}\PYG{k}{size} 

\PYG{n}{da}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{sqrt}\PYG{p}{(}\PYG{n}{tad}\PYG{p}{[}\PYG{n}{in\PYGZus{}x}\PYG{p}{]}\PYG{p}{[}\PYG{n}{in\PYGZus{}y}\PYG{p}{]}\PYG{p}{)}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Closest neuron grid indices: (}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{n}{in\PYGZus{}x}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{n}{in\PYGZus{}y}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{)}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Distance: }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{n}{np}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(}\PYG{n}{da}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
Closest neuron grid indices: ( 19 , 4 )
Distance:  0.012
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{som_67_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
The lightest region in the above figure indicates the cluster, to which the new point belongs. The darker the region, the larger is the distance from the corresponding neuron.

\sphinxAtStartPar
One should stress that we have obtained a classifier which not only assigns a closest cluster to a probed point, but also provides its distances from all other clusters.


\subsection{Mapping colors on a line}
\label{\detokenize{docs/som:mapping-colors-on-a-line}}
\sphinxAtStartPar
In this subsection we present an example of a mapping of 3\sphinxhyphen{}dim. data into a 1\sphinxhyphen{}dim. neuron grid, hence a reduction of three dimensions into one. This proceeds exactly along the lines of the previous subsection, so we are very brief in comments.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{ns}\PYG{o}{=}\PYG{l+m+mi}{8}
\PYG{n}{samp}\PYG{o}{=}\PYG{p}{[}\PYG{n}{rgbn}\PYG{p}{(}\PYG{p}{)} \PYG{k}{for} \PYG{n}{\PYGZus{}} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ns}\PYG{p}{)}\PYG{p}{]}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{4}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Sample colors}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{16}\PYG{p}{)} 

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{axis}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{off}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ns}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{i}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{n}{color}\PYG{o}{=}\PYG{n}{samp}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{400}\PYG{p}{)}
    
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{som_71_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{si}\PYG{o}{=}\PYG{l+m+mi}{50}                    \PYG{c+c1}{\PYGZsh{} 1\PYGZhy{}dim. grid of si neurons, 3 RGB components}
\PYG{n}{tab2}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{p}{(}\PYG{n}{si}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{)}    \PYG{c+c1}{\PYGZsh{} neuron gri}

\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{si}\PYG{p}{)}\PYG{p}{:}      
    \PYG{n}{tab2}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{o}{=}\PYG{n}{rgbn}\PYG{p}{(}\PYG{p}{)}    \PYG{c+c1}{\PYGZsh{} random initialization}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{som_73_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{eps}\PYG{o}{=}\PYG{l+m+mf}{.5}    
\PYG{n}{de} \PYG{o}{=} \PYG{l+m+mi}{20}   
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{for} \PYG{n}{\PYGZus{}} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{200}\PYG{p}{)}\PYG{p}{:} 
    \PYG{n}{eps}\PYG{o}{=}\PYG{n}{eps}\PYG{o}{*}\PYG{l+m+mf}{.99}      
    \PYG{n}{de}\PYG{o}{=}\PYG{n}{de}\PYG{o}{*}\PYG{l+m+mf}{.96}        
    \PYG{k}{for} \PYG{n}{s} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ns}\PYG{p}{)}\PYG{p}{:}       
        \PYG{n}{p}\PYG{o}{=}\PYG{n}{samp}\PYG{p}{[}\PYG{n}{s}\PYG{p}{]}
        \PYG{n}{dist}\PYG{o}{=}\PYG{p}{[}\PYG{n}{dist3}\PYG{p}{(}\PYG{n}{p}\PYG{p}{,}\PYG{n}{tab2}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{)} \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{si}\PYG{p}{)}\PYG{p}{]} 
        \PYG{n}{ind\PYGZus{}min} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{argmin}\PYG{p}{(}\PYG{n}{dist}\PYG{p}{)}          
        \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{si}\PYG{p}{)}\PYG{p}{:}
            \PYG{n}{tab2}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{o}{+}\PYG{o}{=}\PYG{n}{eps}\PYG{o}{*}\PYG{n}{phi}\PYG{p}{(}\PYG{n}{ind\PYGZus{}min}\PYG{p}{,}\PYG{n}{i}\PYG{p}{,}\PYG{n}{de}\PYG{p}{)}\PYG{o}{*}\PYG{p}{(}\PYG{n}{p}\PYG{o}{\PYGZhy{}}\PYG{n}{tab2}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{)} 
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{som_76_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
As expected, we note smooth transitions between colors. The formation of clusters can be seen with the \(U\)\sphinxhyphen{}matrix, which now is, of course, one\sphinxhyphen{}dimensional:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{ta2}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{n}{si}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{2}\PYG{p}{)}

\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{n}{si}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{ta2}\PYG{p}{[}\PYG{n}{i}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{sqrt}\PYG{p}{(}\PYG{n}{dist3}\PYG{p}{(}\PYG{n}{tab2}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{,}\PYG{n}{tab2}\PYG{p}{[}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{o}{+}\PYG{n}{dist3}\PYG{p}{(}\PYG{n}{tab2}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{,}\PYG{n}{tab2}\PYG{p}{[}\PYG{n}{i}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{som_79_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
The minima (there are 8 of them, equal to the multiplicity of the sample) indicate the clusters. The height of the separating peaks shows how much the neighboring colors differ. Again, we see a nicely produced classifier, this time with two dimensions „hidden away”, as we reduce from three to one.


\subsection{Large reduction of dimensionality}
\label{\detokenize{docs/som:large-reduction-of-dimensionality}}
\sphinxAtStartPar
In many situations the input space may have a very large dimension. In the \sphinxhref{https://en.wikipedia.org/wiki/Self-organizing\_map}{Wikipedia example} quoted here, one takes articles from various fields and computes frequencies of used words (for instance, in a given article how  many times the word „goalkeeper” has been used, divided by the total number of words in the article). Essentially, the dimensionality of \(D\) is of the order of the number of all English words, a huge number \(\sim 10^5\)! Then, with a properly defined distance depending on these frequencies, one uses Kohonen’s algorithm to carry out a reduction into a 2\sphinxhyphen{}dim. grid of neurons. The resulting \(U\)\sphinxhyphen{}matrix can be drawn as follows:

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{som_83_0}.jpg}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
Not surprisingly, we notice that articles on sports are special and form a very well defined cluster. The reason is that the sport’s jargon is very specific. The Media are also distinguished, whereas other fields are more\sphinxhyphen{}less uniformly distributed. The example shows how we can, with a very simple method, comprehend data in a multidimensional space and see specific correlations/clusters. Whereas some conclusions may be obvious, such as the fact that sport has a unique jargon, other are less transparent, for instance the emergence of the media cluster and lack of well\sphinxhyphen{}defined clusters for other fields, e.g. for mathematics.


\section{Mapping 2\sphinxhyphen{}dim. data into a 2\sphinxhyphen{}dim. grid}
\label{\detokenize{docs/som:mapping-2-dim-data-into-a-2-dim-grid}}
\sphinxAtStartPar
Finally, we come to a very important case of mapping 2\sphinxhyphen{}dim. data in a 2\sphinxhyphen{}dim. grid, i.e. with no dimensionality reduction. In particular, this case is realized in our vision system between the retina and the visual cortex.

\sphinxAtStartPar
The algorithm proceeds analogously to the previous cases. We initialize an \(n \times n\) grid of neurons and place them randomly in the square \([0,1]\times [0,1]\).

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{n}\PYG{o}{=}\PYG{l+m+mi}{10}
\PYG{n}{sam}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{func}\PYG{o}{.}\PYG{n}{point}\PYG{p}{(}\PYG{p}{)} \PYG{k}{for} \PYG{n}{\PYGZus{}} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{n}\PYG{o}{*}\PYG{n}{n}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
The lines, again drawn to guide the eye, join the adjacent index pairs in the grid: {[}i,j{]} and {[}i+1,j{]}, or {[}i,j{]} and {[}i,j+1{]} (the neurons in the interior of the grid have 4 nearest neighbors, those at the boundary 3, except for the corners, which have only 2).

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{som_89_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
We note a total initial „chaos”, as the neurons are located randomly. Now comes Kohonen’s miracle:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{eps}\PYG{o}{=}\PYG{l+m+mf}{.5}   \PYG{c+c1}{\PYGZsh{} initial learning speed}
\PYG{n}{de} \PYG{o}{=} \PYG{l+m+mi}{3}   \PYG{c+c1}{\PYGZsh{} initial neighborhood distance}
\PYG{n}{nr} \PYG{o}{=} \PYG{l+m+mi}{100} \PYG{c+c1}{\PYGZsh{} number of rounds}
\PYG{n}{rep}\PYG{o}{=} \PYG{l+m+mi}{300} \PYG{c+c1}{\PYGZsh{} number of points in each round}
\PYG{n}{ste}\PYG{o}{=}\PYG{l+m+mi}{0}    \PYG{c+c1}{\PYGZsh{} inital number of caried out steps}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} completely analogous to the previous codes of this chapter}
\PYG{k}{for} \PYG{n}{\PYGZus{}} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{nr}\PYG{p}{)}\PYG{p}{:}   \PYG{c+c1}{\PYGZsh{} rounds}
    \PYG{n}{eps}\PYG{o}{=}\PYG{n}{eps}\PYG{o}{*}\PYG{l+m+mf}{.97}      
    \PYG{n}{de}\PYG{o}{=}\PYG{n}{de}\PYG{o}{*}\PYG{l+m+mf}{.98}         
    \PYG{k}{for} \PYG{n}{\PYGZus{}} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{rep}\PYG{p}{)}\PYG{p}{:}    \PYG{c+c1}{\PYGZsh{} repeat for rep points}
        \PYG{n}{ste}\PYG{o}{=}\PYG{n}{ste}\PYG{o}{+}\PYG{l+m+mi}{1}
        \PYG{n}{p}\PYG{o}{=}\PYG{n}{func}\PYG{o}{.}\PYG{n}{point}\PYG{p}{(}\PYG{p}{)} 
        \PYG{n}{dist}\PYG{o}{=}\PYG{p}{[}\PYG{n}{func}\PYG{o}{.}\PYG{n}{eucl}\PYG{p}{(}\PYG{n}{p}\PYG{p}{,}\PYG{n}{sam}\PYG{p}{[}\PYG{n}{l}\PYG{p}{]}\PYG{p}{)} \PYG{k}{for} \PYG{n}{l} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{n}\PYG{o}{*}\PYG{n}{n}\PYG{p}{)}\PYG{p}{]} 
        \PYG{n}{ind\PYGZus{}min} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{argmin}\PYG{p}{(}\PYG{n}{dist}\PYG{p}{)} 
        \PYG{n}{ind\PYGZus{}i}\PYG{o}{=}\PYG{n}{ind\PYGZus{}min}\PYG{o}{\PYGZpc{}}\PYG{k}{n}
        \PYG{n}{ind\PYGZus{}j}\PYG{o}{=}\PYG{n}{ind\PYGZus{}min}\PYG{o}{/}\PYG{o}{/}\PYG{n}{n}       
        
        \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{n}\PYG{p}{)}\PYG{p}{:} 
            \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{n}\PYG{p}{)}\PYG{p}{:}
                \PYG{n}{sam}\PYG{p}{[}\PYG{n}{i}\PYG{o}{+}\PYG{n}{n}\PYG{o}{*}\PYG{n}{j}\PYG{p}{]}\PYG{o}{+}\PYG{o}{=}\PYG{n}{eps}\PYG{o}{*}\PYG{n}{phi2}\PYG{p}{(}\PYG{n}{ind\PYGZus{}i}\PYG{p}{,}\PYG{n}{ind\PYGZus{}j}\PYG{p}{,}\PYG{n}{i}\PYG{p}{,}\PYG{n}{j}\PYG{p}{,}\PYG{n}{de}\PYG{p}{)}\PYG{o}{*}\PYG{p}{(}\PYG{n}{p}\PYG{o}{\PYGZhy{}}\PYG{n}{sam}\PYG{p}{[}\PYG{n}{i}\PYG{o}{+}\PYG{n}{n}\PYG{o}{*}\PYG{n}{j}\PYG{p}{]}\PYG{p}{)} 
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
Here is the history of a simulation:

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=800\sphinxpxdimen]{{kball}.png}
\caption{Progress of Kohonen’s algorithm. The lines, drawn to guide the eye, connects neurons with adjacent indices.}\label{\detokenize{docs/som:kohstory2-fig}}\end{figure}

\sphinxAtStartPar
As the algorithm progresses, the initial „chaos” gradually changes into a nearly perfect order, with the grid placed uniformly in the square of the data, with only slight displacements from a regular arrangement. On the way, near 40 steps, we notice a phenomenon called „twist”, where the grid is crumpled. In the twist region, many neurons, also of distant indices, have a close location in \((x_1,x_2)\).


\section{Topology}
\label{\detokenize{docs/som:topology}}
\sphinxAtStartPar
Recall the Voronoi construction of categories introduced in section {\hyperref[\detokenize{docs/unsupervised:vor-lab}]{\sphinxcrossref{\DUrole{std,std-ref}{Voronoi areas}}}}. One can use it now again, treating the neurons from a grid as the Voronoi points. The Voronoi construction provides a mapping \(v\) from the data space \(D\) to the neuron space \(N\),
\begin{equation*}
\begin{split} 
v: D \to N 
\end{split}
\end{equation*}
\sphinxAtStartPar
(note that this goes in the opposite direction than function \(f\) defined at the beginning of this chapter).

\sphinxAtStartPar
The procedure is as follows:
We take the final outcome of the algorith, such as in the  bottom right panel of \hyperref[\detokenize{docs/som:kohstory2-fig}]{Rys.\@ \ref{\detokenize{docs/som:kohstory2-fig}}}, construct the Voronoi areas for all the neurons, and thus obtain a mapping \(v\) for all the points in the \((x_1,x_2)\) square. The reader may notice that there is an ambiguity for points lying exactly at the boundaries between the neighboring areas, but this can be taken care of by using an additional prescription (for instance, selecting a neuron lying at a direction which has the lowest azimuthal angle, etc.)

\sphinxAtStartPar
Now a key observation:

\begin{sphinxadmonition}{note}{Topological property}

\sphinxAtStartPar
For situations without twists, such as in the bottom right panel of \hyperref[\detokenize{docs/som:kohstory2-fig}]{Rys.\@ \ref{\detokenize{docs/som:kohstory2-fig}}}, mapping \(v\) has the property that when \(d_1\) and \(d_2\) from \(D\) are close to each other, then also their corresponding neurons are close, i.e. the indices \(v(d_1)\) and \(v(d_2)\) are close.
\end{sphinxadmonition}

\sphinxAtStartPar
This observation is straightforward to prove: Since \(d_1\) and \(d_2\) are close (and we mean very close, closer than the grid spacing), they must belong either to
\begin{itemize}
\item {} 
\sphinxAtStartPar
the same Voronoi area, where \(v(d_1)=v(d_2)\), or

\item {} 
\sphinxAtStartPar
a pair of neighboring Voronoi areas.

\end{itemize}

\sphinxAtStartPar
Since for the considered situation (without twists) the neighboring areas have the grid indices differing by 1, the conclusion that \(v(d_1)\) and \(v(d_2)\) are close follows immediately.

\sphinxAtStartPar
Note that this feature of Kohonen’s maps is far from trivial and does not hold for a general mapping. Imagine for instance that we stop our simulations for \hyperref[\detokenize{docs/som:kohstory2-fig}]{Rys.\@ \ref{\detokenize{docs/som:kohstory2-fig}}} after 40 steps (top central panel) and are left with a „twisted” grid. In the vicinity of the twist, the indices of the adjacent Voronoi areas differ largely, and the advertised topological property no longer holds.

\sphinxAtStartPar
The discussed topological property has mathematically general and far\sphinxhyphen{}reaching consequences. First, it allows to carry over „shapes” from \(D\) to \(N\). We illustrate it on an example.

\sphinxAtStartPar
Imagine that we have a circle \(C\) in \(D\)\sphinxhyphen{}space, of radius \sphinxstylestrong{rad} centered at \sphinxstylestrong{cent}. We need to find the winners in the \(N\) space for any point in \(C\). For this purpose we go around \(C\) in \sphinxstylestrong{npoi} points equally spaced in the azimuthal angle, and for each one find a winner.

\sphinxAtStartPar
\(C\) is parametrized with polar coordinates:
\begin{equation*}
\begin{split}
x_1=r \cos \left( \frac{2\pi \phi}{N}  \right)+c_1, \;\;\;
x_2=r \sin \left( \frac{2\pi \phi}{N}  \right)+c_2.
\end{split}
\end{equation*}
\sphinxAtStartPar
Going to the mathematical notation to Python we use
\(r=\)\sphinxstylestrong{rad}, \(\phi\)=\sphinxstylestrong{ph}, \(N=\)\sphinxstylestrong{npoi}, \((c_1,c_2)=\)\sphinxstylestrong{{[}cent{]}}.
The loop over \sphinxstylestrong{ph} goes around the circle.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{rad}\PYG{o}{=}\PYG{l+m+mf}{0.35}                      \PYG{c+c1}{\PYGZsh{} radius of a circle}
\PYG{n}{cent}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mf}{0.5}\PYG{p}{,}\PYG{l+m+mf}{0.5}\PYG{p}{]}\PYG{p}{)}      \PYG{c+c1}{\PYGZsh{} center of the circle}
\PYG{n}{npoi}\PYG{o}{=}\PYG{l+m+mi}{400}                      \PYG{c+c1}{\PYGZsh{} number of points in the circle}

\PYG{n}{wins}\PYG{o}{=}\PYG{p}{[}\PYG{p}{]}                       \PYG{c+c1}{\PYGZsh{} table of winners}

\PYG{k}{for} \PYG{n}{ph} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{npoi}\PYG{p}{)}\PYG{p}{:}        \PYG{c+c1}{\PYGZsh{} go around the circle}
    \PYG{n}{p}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{rad}\PYG{o}{*}\PYG{n}{np}\PYG{o}{.}\PYG{n}{cos}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{o}{*}\PYG{n}{np}\PYG{o}{.}\PYG{n}{pi}\PYG{o}{/}\PYG{n}{npoi}\PYG{o}{*}\PYG{n}{ph}\PYG{p}{)}\PYG{p}{,}\PYG{n}{rad}\PYG{o}{*}\PYG{n}{np}\PYG{o}{.}\PYG{n}{sin}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{o}{*}\PYG{n}{np}\PYG{o}{.}\PYG{n}{pi}\PYG{o}{/}\PYG{n}{npoi}\PYG{o}{*}\PYG{n}{ph}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}\PYG{o}{+}\PYG{n}{cent}
                              \PYG{c+c1}{\PYGZsh{} the circle in polar coordinates}
    \PYG{n}{dist}\PYG{o}{=}\PYG{p}{[}\PYG{n}{func}\PYG{o}{.}\PYG{n}{eucl}\PYG{p}{(}\PYG{n}{p}\PYG{p}{,}\PYG{n}{sam}\PYG{p}{[}\PYG{n}{l}\PYG{p}{]}\PYG{p}{)} \PYG{k}{for} \PYG{n}{l} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{n}\PYG{o}{*}\PYG{n}{n}\PYG{p}{)}\PYG{p}{]} 
      \PYG{c+c1}{\PYGZsh{} distances from the point on the circle to the neurons in the nxn grid}
    \PYG{n}{ind\PYGZus{}min} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{argmin}\PYG{p}{(}\PYG{n}{dist}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} winner}
    \PYG{n}{wins}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{ind\PYGZus{}min}\PYG{p}{)}      \PYG{c+c1}{\PYGZsh{} add winner to the table}
        
\PYG{n}{ci}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{unique}\PYG{p}{(}\PYG{n}{wins}\PYG{p}{)}            \PYG{c+c1}{\PYGZsh{} remove duplicates from the table      }
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
The result of Kohonen’s algorithm is as follows:

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{som_103_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
The red neurons are the winners for certain sections of the circle. When we draw these winners alone in the \(N\) space (keep in mind we are going from \(D\) to \(N\)), we get

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mf}{2.3}\PYG{p}{,}\PYG{l+m+mf}{2.3}\PYG{p}{)}\PYG{p}{,}\PYG{n}{dpi}\PYG{o}{=}\PYG{l+m+mi}{120}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlim}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{10}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylim}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{10}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{ci}\PYG{o}{/}\PYG{o}{/}\PYG{l+m+mi}{10}\PYG{p}{,}\PYG{n}{ci}\PYG{o}{\PYGZpc{}}\PYG{k}{10},c=\PYGZsq{}red\PYGZsq{},s=5)

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZdl{}i\PYGZdl{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZdl{}j\PYGZdl{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{som_105_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
This looks pretty much as a (rough and discrete) circle. Note that in our example we only have \(n^2=100\) pixels to our disposal \sphinxhyphen{} a very low resolution. The image would look better and better with an increasing \(n\). At some point one would reach the 10M pixel resolution of typical camera, and then the image would seem smooth! We have carried over our circle from \(D\) into \(N\).

\begin{sphinxadmonition}{note}{Vision}

\sphinxAtStartPar
The topological property, such as the one in the discussed Kohonen mappings, has a prime importance in our vision system and the perception of objects. Shapes are carried over from the retina to the visual cortex and are not „warped up” on the way!
\end{sphinxadmonition}

\sphinxAtStartPar
Another key topological feature is the preservation of \sphinxstylestrong{connectedness}. If an area \(A\) in \(D\) is connected (so to speak, is in one piece), then its image \(v(A)\) in \(N\) is also connected (we ignore the desired rigor here as to what „connected” means in a discrete space and rely on intuition). So things do not get „torn into pieces” when transforming from \(D\) to \(N\).

\sphinxAtStartPar
Note that the discussed topological features need not be present when the dimensionality is reduced, as in our previous examples. Take for instance the bottom right panel of \hyperref[\detokenize{docs/som:kohstory-fig}]{Rys.\@ \ref{\detokenize{docs/som:kohstory-fig}}}. There, many neighboring pairs of the Voronoi areas correspond to distant indices, so it is no longer true that \(v(d_1)\) and \(v(d_2)\) in \(N\) are close for close \(d_1\) and \(d_2\) in \(D\), as these points may belong to different Voronoi areas with \sphinxstylestrong{distant} indices.

\sphinxAtStartPar
For that case, our example with the circle looks like this:

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{som_111_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
When we go subsequently along the \sphinxstylestrong{grid indices} (i.e. along the blue connecting line), taking \(i=1,2,\dots,100\), we obtain the plot below. We can see the image of our circle (red dots) as a bunch of \sphinxstylestrong{disconnected} red sections. The circle is torn into pieces, the \sphinxstylestrong{topology is not preserved!}

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{som_113_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
Here is the summarizing statement (NB not made sufficiently clear in the literature):

\begin{sphinxadmonition}{note}{Informacja:}
\sphinxAtStartPar
Topological features of Kohonen’s maps hold for equal dimensionalities of the input space and the neuron grid, \(n=k\), and in general do not hold for the reduced dimensionality cases, \(k<n\).
\end{sphinxadmonition}


\section{Lateral inhibition}
\label{\detokenize{docs/som:lateral-inhibition}}\label{\detokenize{docs/som:lat-lab}}
\sphinxAtStartPar
In the last topic of these lectures, we return to the issue of how the competition for the „winner” is realized in ANNs. Up to now (cf. section {\hyperref[\detokenize{docs/unsupervised:inn-sec}]{\sphinxcrossref{\DUrole{std,std-ref}{Interpretation via neural networks}}}}), we have just been using the minimum (or maximum, when the signal was extended to a hyperphere) in the output, though this is embarrassingly outside of the neural framework. Such an inspection of which neuron yields the strongest signal would require an „external wizard”, or some sort of a control unit. Mathematically, it is easy to imagine, but the challenge is to build it from neurons within the rules of the game.

\sphinxAtStartPar
Actually, if the neurons in a layer „talk” to one another, we can have a „contest” from which a winner may emerge. In particular, an architecture as in \hyperref[\detokenize{docs/som:lat-fig}]{Rys.\@ \ref{\detokenize{docs/som:lat-fig}}} allows for an arrangement of competition and a natural realization of a \sphinxstylestrong{winner\sphinxhyphen{}take\sphinxhyphen{}most} mechanism.

\sphinxAtStartPar
The type of models as presented below is known as the \sphinxhref{https://en.wikipedia.org/wiki/Hopfield\_network}{Hopfield networks}. Note that we depart here from the \sphinxstylestrong{feed\sphinxhyphen{}forward} limitation of \hyperref[\detokenize{docs/intro:ffnn-fig}]{Rys.\@ \ref{\detokenize{docs/intro:ffnn-fig}}} and allow for a recursive, or feed\sphinxhyphen{}back character.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=220\sphinxpxdimen]{{lat3}.png}
\caption{Network with inter\sphinxhyphen{}neuron couplings used for modeling lateral inhibition. All the neurons are connected to one another in both directions (lines without arrows).}\label{\detokenize{docs/som:lat-fig}}\end{figure}

\sphinxAtStartPar
Neuron number \(i\) receives the signal \(s_i = x w_i\), where \(x\) is the input (the same for all the neurons), and \(w_i\) is the weight of neuron \(i\). The neuron produces output \(y_i\), where now a part of it is sent to neurons \(j\) as \(F_{ji} y_i\). Here \(F_{ij}\) denotes the coupling strength (we assume \(F_{ii}=0\) \sphinxhyphen{} no self coupling). Reciprocally, neuron \(i\) also receives output from neurons \(j\) in the form \(F_{ij} y_j\). The summation over all the neurons yields
\begin{equation*}
\begin{split} 
y_i = s_i + \sum_{j\neq i} F_{ij} y_j, 
\end{split}
\end{equation*}
\sphinxAtStartPar
which in the matrix notation becomes \( y = s + F y\), or \(y(I-F)=s\), where \(I\) is the identity matrix. Solving for \(y\) gives formally
\begin{equation}\label{equation:docs/som:eq-lat}
\begin{split}y= (I-F)^{-1} s.\end{split}
\end{equation}
\sphinxAtStartPar
One needs to model appropriately the coupling matrix \(F\). We take

\sphinxAtStartPar
\( F_ {ii} = \) 0,

\sphinxAtStartPar
\( F_ {ij} = - a \exp (- | i-j | / b) ~~ \) for \( i \neq j \), \( ~~ a, b> 0 \),

\sphinxAtStartPar
i.e. assume attenuation (negative feedback), which is strongest for close neighbors and decreases with distance. The decrease is controlled by a characteristic scale \(b\).

\sphinxAtStartPar
The Python implementation is straightforward:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{ns} \PYG{o}{=} \PYG{l+m+mi}{30}\PYG{p}{;}       \PYG{c+c1}{\PYGZsh{} number of neurons}
\PYG{n}{b} \PYG{o}{=} \PYG{l+m+mi}{4}\PYG{p}{;}         \PYG{c+c1}{\PYGZsh{} parameter controlling the decrease of damping with distance}
\PYG{n}{a} \PYG{o}{=} \PYG{l+m+mi}{1}\PYG{p}{;}         \PYG{c+c1}{\PYGZsh{} magnitude of damping}

\PYG{n}{F}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{n}{a}\PYG{o}{*}\PYG{n}{np}\PYG{o}{.}\PYG{n}{exp}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{n}{np}\PYG{o}{.}\PYG{n}{abs}\PYG{p}{(}\PYG{n}{i}\PYG{o}{\PYGZhy{}}\PYG{n}{j}\PYG{p}{)}\PYG{o}{/}\PYG{n}{b}\PYG{p}{)} \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ns}\PYG{p}{)}\PYG{p}{]} \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ns}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)} 
                    \PYG{c+c1}{\PYGZsh{} exponential fall\PYGZhy{}off}
    
\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ns}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{F}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{o}{=}\PYG{l+m+mi}{0}       \PYG{c+c1}{\PYGZsh{} no self\PYGZhy{}coupling}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{som_122_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\sphinxAtStartPar
We assume a bell\sphinxhyphen{}shaped Lorentzian input signal \(s\), with a maximum in the middle neuron. The width is controlled with \sphinxstylestrong{D}:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{D}\PYG{o}{=}\PYG{l+m+mi}{3}
\PYG{n}{s} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{D}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{o}{/}\PYG{p}{(}\PYG{p}{(}\PYG{n}{i} \PYG{o}{\PYGZhy{}} \PYG{n}{ns}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2} \PYG{o}{+} \PYG{n}{D}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{)} \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ns}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} Lorentzian function}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
Next, we solve Eq. \eqref{equation:docs/som:eq-lat} via inverting the \((I-F)\) matrix, performed with the numpy \sphinxstylestrong{linalg.inv} function. Recall that \sphinxstylestrong{dot} multiplies matrices:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{invF}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{linalg}\PYG{o}{.}\PYG{n}{inv}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{identity}\PYG{p}{(}\PYG{n}{ns}\PYG{p}{)}\PYG{o}{\PYGZhy{}}\PYG{n}{F}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} matrix inversion}
\PYG{n}{y}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{invF}\PYG{p}{,}\PYG{n}{s}\PYG{p}{)}                      \PYG{c+c1}{\PYGZsh{} multiplication}
\PYG{n}{y}\PYG{o}{=}\PYG{n}{y}\PYG{o}{/}\PYG{n}{y}\PYG{p}{[}\PYG{l+m+mi}{15}\PYG{p}{]}                             \PYG{c+c1}{\PYGZsh{} normalization (inessential) }
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
What follows is actually quite remarkable: the output signal \(y\) becomes much narrower from the input signal \(s\). This may be interpreted as a realization of the „winner\sphinxhyphen{}take\sphinxhyphen{}all” scenario. The winner „damped” he guys around him, so he puts himself on airs! The effect is smooth, with the signal visibly sharpened.

\begin{sphinxuseclass}{cell}
\begin{sphinxuseclass}{tag_remove-input}\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{som_128_0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\end{sphinxuseclass}
\begin{sphinxadmonition}{note}{Lateral inhibition}

\sphinxAtStartPar
The damping of the response of neighboring neurons is called \sphinxstylestrong{lateral inhibition}. It was discovered in neurobiological networks {[}\hyperlink{cite.docs/conclusion:id14}{HR72}{]}.
\end{sphinxadmonition}

\sphinxAtStartPar
The presented model is certainly too simplistic to be realistic from the point of view of biological networks. Also, it yields unnatural negative signal outside of the central peak (which we can remove with rectification). Nevertheless, the setup shows a possible way to achieve the „winner competition”, essential for unsupervised learning: One needs to allow for the competing neurons to interact.

\begin{sphinxadmonition}{note}{Informacja:}
\sphinxAtStartPar
Actually, \sphinxstylestrong{pyramidal neurons}, present i.a. in the neocortex, have as many as a few thousand dendritic spines and do realize a scenario with numerous synaptic connections. They are believed \sphinxhref{https://www.quantamagazine.org/artificial-neural-nets-finally-yield-clues-to-how-brains-learn-20210218/}{Quantamagazine} to play a crucial role in learning and cognition processes.
\end{sphinxadmonition}

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=300\sphinxpxdimen]{{smi32-pic}.jpg}
\caption{Image of pyramidal neurons (from \sphinxhref{http://brainmaps.org/index.php?p=screenshots}{brainmaps.org})}\label{\detokenize{docs/som:pyr-fig}}\end{figure}


\section{Exercises}
\label{\detokenize{docs/som:exercises}}
\begin{sphinxadmonition}{note}{\protect\(~\protect\)}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Construct a Kohonen mapping form a \sphinxstylestrong{disjoint} 2D shape into a 2D grid of neurons.

\item {} 
\sphinxAtStartPar
Construct a Kohonen mapping for a case where the points in the input space are not distributed uniformly, but denser in some regions.

\item {} 
\sphinxAtStartPar
Create, for a number of countries, fictitious flags which have two colors (hence are described with 6 RGB numbers). Construct a Kohonen map into a 2\sphinxhyphen{}dim. grid. Plot the resulting \(U\)\sphinxhyphen{}matrix and draw conclusions.

\item {} 
\sphinxAtStartPar
\sphinxhref{https://en.wikipedia.org/wiki/Lateral\_inhibition}{Lateral inhibition} has „side\sphinxhyphen{}effects” seen in optical delusions. Describe the \sphinxhref{https://en.wikipedia.org/wiki/Mach\_bands}{Mach illusion}, programming it in Python.

\end{enumerate}
\end{sphinxadmonition}


\chapter{Concluding remarks}
\label{\detokenize{docs/conclusion:concluding-remarks}}\label{\detokenize{docs/conclusion::doc}}
\sphinxAtStartPar
In a programmer’s life, building a well\sphinxhyphen{}functioning ANN, even for simple problems as used for illustrations in these lectures, can be a truly frustrating experience! There are many subtleties involved on the way. To list just a few that we have encountered in this course, one faces a choice of the network architecture and freedom in the initialization of weights (hyperparameters). Then, one has to select an initial learning speed, a neighborhood distance, in general, some parameters controlling the performance/convergence, as well as their update strategy as the algorithm progresses. Further, one frequently tackles with an emergence of a massive number of local minima in the space of hyperparameters, and many optimization methods may be applied here, way more sophisticated than our simplest steepest\sphinxhyphen{}descent method. Moreover, a proper choice of the neuron activation functions is crucial for success, which relates to avoiding the problem of „dead neurons”. And so on and on, many choices to be made before we start gazing in the screen in hopes that the results of our code converge …

\begin{sphinxadmonition}{important}{Ważne:}
\sphinxAtStartPar
Taking the right decisions for the above issues is an \sphinxstylestrong{art} more than science, based on long experience of multitudes of code developers and piles of empty pizza boxes!
\end{sphinxadmonition}

\sphinxAtStartPar
Now, having completed this course and understood the basic principles behind the simplest ANNs inside out, the reader may safely jump to using professional tools of modern machine learning, with the conviction that inside the black boxes there sit essentially the same little codes he met here, but with all the knowledge, experience, tricks, provisions, and options built in. Achieving this conviction, through appreciation of simplicity, has been one of the guiding goals of this course.


\section{Acknowledgments}
\label{\detokenize{docs/conclusion:acknowledgments}}
\sphinxAtStartPar
The author thanks \sphinxhref{https://www.linkedin.com/in/janbroniowski}{Jan Broniowski} for priceless technical help and for remarks to the text.

\sphinxAtStartPar



\chapter{Dodatki}
\label{\detokenize{docs/appendix:dodatki}}\label{\detokenize{docs/appendix::doc}}

\section{Jak uruchamiać kody książki}
\label{\detokenize{docs/appendix:jak-uruchamiac-kody-ksiazki}}\label{\detokenize{docs/appendix:app-run}}

\subsection{Lokalnie}
\label{\detokenize{docs/appendix:lokalnie}}
\sphinxAtStartPar
Czytelnik może pobrać notebooki \sphinxhref{https://jupyter.org}{Jupytera} dla każdego rozdziału, klikając ikonę pobierania (strzałka w dół) po prawej stronie na górnym pasku podczas przeglądania książki.



\sphinxAtStartPar
Po zainstalowaniu Pythona i \sphinxhref{https://jupyter.org}{Jupytera} (najlepiej przez \sphinxhref{https://docs.conda.io/projects/conda/en/latest/user-guide/install}{conda}), Czytelnik może postępować zgodnie z instrukcjami dla danego systemu operacyjnego, aby otworzyć Jupyter i uruchomić w nim notebooki wykładu.


\subsection{Google Colab lub Binder}
\label{\detokenize{docs/appendix:google-colab-lub-binder}}
\sphinxAtStartPar
W danym rozdziale poniżej Wstępu należy kliknąć symbol rakiety w górnym prawym rogu ekranu, co uruchamia możliwość wykonywania (edycji, zabawy) programu w chmurze. Jest to podstawowa funkcjonalnośc wykonywalnej książki Jupyter Book.


\section{Pakiet \sphinxstylestrong{neural}}
\label{\detokenize{docs/appendix:pakiet-neural}}\label{\detokenize{docs/appendix:app-lab}}
\sphinxAtStartPar
Structura pakietu biblioteki jest następująca:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
lib\PYGZus{}nn
└── neural
    ├── \PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}.py
    ├── draw.py
    └── func.py
\end{sphinxVerbatim}

\sphinxAtStartPar
i składa się z dwóch modułów: \sphinxstylestrong{\sphinxhref{http://func.py}{func.py}} and \sphinxstylestrong{\sphinxhref{http://draw.py}{draw.py}}.


\subsection{Moduł \sphinxstylestrong{func.py}}
\label{\detokenize{docs/appendix:modul-func-py}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{Contains functions used in the lecture}
\PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}

\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}


\PYG{k}{def} \PYG{n+nf}{step}\PYG{p}{(}\PYG{n}{s}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    step function}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    s: signal}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    return: 1 if s\PYGZgt{}0, 0 otherwise}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{k}{if} \PYG{n}{s}\PYG{o}{\PYGZgt{}}\PYG{l+m+mi}{0}\PYG{p}{:}
        \PYG{k}{return} \PYG{l+m+mi}{1}
    \PYG{k}{else}\PYG{p}{:}
        \PYG{k}{return} \PYG{l+m+mi}{0}
   
   
\PYG{k}{def} \PYG{n+nf}{neuron}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,}\PYG{n}{w}\PYG{p}{,}\PYG{n}{f}\PYG{o}{=}\PYG{n}{step}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    MCP neuron}

\PYG{l+s+sd}{    x: array of inputs  [x1, x2,...,xn]}
\PYG{l+s+sd}{    w: array of weights [w0, w1, w2,...,wn]}
\PYG{l+s+sd}{    f: activation function, with step as default}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    return: signal=f(w0 + x1 w1 + x2 w2 +...+ xn wn) = f(x.w)}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{k}{return} \PYG{n}{f}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{insert}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{,}\PYG{n}{w}\PYG{p}{)}\PYG{p}{)}
 
 
\PYG{k}{def} \PYG{n+nf}{sig}\PYG{p}{(}\PYG{n}{s}\PYG{p}{,}\PYG{n}{T}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    sigmoid}
\PYG{l+s+sd}{     }
\PYG{l+s+sd}{    s: signal}
\PYG{l+s+sd}{    T: temperature}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    return: sigmoid(s)}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{k}{return} \PYG{l+m+mi}{1}\PYG{o}{/}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{o}{+}\PYG{n}{np}\PYG{o}{.}\PYG{n}{exp}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{n}{s}\PYG{o}{/}\PYG{n}{T}\PYG{p}{)}\PYG{p}{)}
    
    
\PYG{k}{def} \PYG{n+nf}{dsig}\PYG{p}{(}\PYG{n}{s}\PYG{p}{,} \PYG{n}{T}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    derivative of sigmoid}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    s: signal}
\PYG{l+s+sd}{    T: temperature}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    return: dsigmoid(s,T)/ds}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{k}{return} \PYG{n}{sig}\PYG{p}{(}\PYG{n}{s}\PYG{p}{)}\PYG{o}{*}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{o}{\PYGZhy{}}\PYG{n}{sig}\PYG{p}{(}\PYG{n}{s}\PYG{p}{)}\PYG{p}{)}\PYG{o}{/}\PYG{n}{T}
    
    
\PYG{k}{def} \PYG{n+nf}{lin}\PYG{p}{(}\PYG{n}{s}\PYG{p}{,}\PYG{n}{a}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    linear function}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    s: signal}
\PYG{l+s+sd}{    a: constant}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    return: a*s}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{k}{return} \PYG{n}{a}\PYG{o}{*}\PYG{n}{s}
  
  
\PYG{k}{def} \PYG{n+nf}{dlin}\PYG{p}{(}\PYG{n}{s}\PYG{p}{,}\PYG{n}{a}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    derivative of linear function}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    s: signal}
\PYG{l+s+sd}{    a: constant}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    return: a}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{k}{return} \PYG{n}{a}


\PYG{k}{def} \PYG{n+nf}{relu}\PYG{p}{(}\PYG{n}{s}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    ReLU function}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    s: signal}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    return: s if s\PYGZgt{}0, 0 otherwise}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{k}{if} \PYG{n}{s}\PYG{o}{\PYGZgt{}}\PYG{l+m+mi}{0}\PYG{p}{:}
        \PYG{k}{return} \PYG{n}{s}
    \PYG{k}{else}\PYG{p}{:}
        \PYG{k}{return} \PYG{l+m+mi}{0}


\PYG{k}{def} \PYG{n+nf}{drelu}\PYG{p}{(}\PYG{n}{s}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    derivative of ReLU function}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    s: signal}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    return: 1 if s\PYGZgt{}0, 0 otherwise}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{k}{if} \PYG{n}{s}\PYG{o}{\PYGZgt{}}\PYG{l+m+mi}{0}\PYG{p}{:}
        \PYG{k}{return} \PYG{l+m+mi}{1}
    \PYG{k}{else}\PYG{p}{:}
        \PYG{k}{return} \PYG{l+m+mi}{0}
 
 
\PYG{k}{def} \PYG{n+nf}{lrelu}\PYG{p}{(}\PYG{n}{s}\PYG{p}{,}\PYG{n}{a}\PYG{o}{=}\PYG{l+m+mf}{0.1}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    Leaky ReLU function}
\PYG{l+s+sd}{  }
\PYG{l+s+sd}{    s: signal}
\PYG{l+s+sd}{    a: parameter}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    return: s if s\PYGZgt{}0, a*s otherwise}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{k}{if} \PYG{n}{s}\PYG{o}{\PYGZgt{}}\PYG{l+m+mi}{0}\PYG{p}{:}
        \PYG{k}{return} \PYG{n}{s}
    \PYG{k}{else}\PYG{p}{:}
        \PYG{k}{return} \PYG{n}{a}\PYG{o}{*}\PYG{n}{s}


\PYG{k}{def} \PYG{n+nf}{dlrelu}\PYG{p}{(}\PYG{n}{s}\PYG{p}{,}\PYG{n}{a}\PYG{o}{=}\PYG{l+m+mf}{0.1}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    derivative of Leaky ReLU function}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    s: signal}
\PYG{l+s+sd}{    a: parameter}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    return: 1 if s\PYGZgt{}0, a otherwise}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{k}{if} \PYG{n}{s}\PYG{o}{\PYGZgt{}}\PYG{l+m+mi}{0}\PYG{p}{:}
        \PYG{k}{return} \PYG{l+m+mi}{1}
    \PYG{k}{else}\PYG{p}{:}
        \PYG{k}{return} \PYG{n}{a}


\PYG{k}{def} \PYG{n+nf}{softplus}\PYG{p}{(}\PYG{n}{s}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    softplus function}

\PYG{l+s+sd}{    s: signal}

\PYG{l+s+sd}{    return: log(1+exp(s))}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{k}{return} \PYG{n}{np}\PYG{o}{.}\PYG{n}{log}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{o}{+}\PYG{n}{np}\PYG{o}{.}\PYG{n}{exp}\PYG{p}{(}\PYG{n}{s}\PYG{p}{)}\PYG{p}{)}


\PYG{k}{def} \PYG{n+nf}{dsoftplus}\PYG{p}{(}\PYG{n}{s}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    derivative of softplus function}
\PYG{l+s+sd}{ }
\PYG{l+s+sd}{    s: signal}

\PYG{l+s+sd}{    return: 1/(1+exp(\PYGZhy{}s))}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{k}{return} \PYG{l+m+mi}{1}\PYG{o}{/}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{o}{+}\PYG{n}{np}\PYG{o}{.}\PYG{n}{exp}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{n}{s}\PYG{p}{)}\PYG{p}{)}

    
\PYG{k}{def} \PYG{n+nf}{l2}\PYG{p}{(}\PYG{n}{w0}\PYG{p}{,}\PYG{n}{w1}\PYG{p}{,}\PYG{n}{w2}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}for separating line\PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{k}{return} \PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{.1}\PYG{p}{,}\PYG{l+m+mf}{1.1}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{w0}\PYG{o}{\PYGZhy{}}\PYG{n}{w1}\PYG{o}{*}\PYG{l+m+mf}{0.1}\PYG{p}{)}\PYG{o}{/}\PYG{n}{w2}\PYG{p}{,}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{w0}\PYG{o}{+}\PYG{n}{w1}\PYG{o}{*}\PYG{l+m+mf}{1.1}\PYG{p}{)}\PYG{o}{/}\PYG{n}{w2}\PYG{p}{]}


\PYG{k}{def} \PYG{n+nf}{eucl}\PYG{p}{(}\PYG{n}{p1}\PYG{p}{,}\PYG{n}{p2}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    Square of the Euclidean distance between two points in 2\PYGZhy{}dim. space}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    input: p1, p2 \PYGZhy{} arrays in the format [x1,x2]}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    return: square of the Euclidean distance}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{k}{return} \PYG{p}{(}\PYG{n}{p1}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{n}{p2}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{o}{+}\PYG{p}{(}\PYG{n}{p1}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{n}{p2}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}


\PYG{k}{def} \PYG{n+nf}{rn}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    return: random number from [\PYGZhy{}0.5,0.5]}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{k}{return} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{rand}\PYG{p}{(}\PYG{p}{)}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.5}
 
 
\PYG{k}{def} \PYG{n+nf}{point\PYGZus{}c}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    return: array [x,y] with random point from a cirle}
\PYG{l+s+sd}{            centered at [0.5,0.5] and radius 0.4}
\PYG{l+s+sd}{            (used for examples)}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{k}{while} \PYG{k+kc}{True}\PYG{p}{:}
        \PYG{n}{x}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)}
        \PYG{n}{y}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)}
        \PYG{k}{if} \PYG{p}{(}\PYG{n}{x}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.5}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{o}{+}\PYG{p}{(}\PYG{n}{y}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.5}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2} \PYG{o}{\PYGZlt{}} \PYG{l+m+mf}{0.4}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{:}
            \PYG{k}{break}
    \PYG{k}{return} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{x}\PYG{p}{,}\PYG{n}{y}\PYG{p}{]}\PYG{p}{)}
 
 
\PYG{k}{def} \PYG{n+nf}{point}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    return: array [x,y] with random point from [0,1]x[0,1]}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{n}{x}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)}
    \PYG{n}{y}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{x}\PYG{p}{,}\PYG{n}{y}\PYG{p}{]}\PYG{p}{)}


\PYG{k}{def} \PYG{n+nf}{set\PYGZus{}ran\PYGZus{}w}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{,}\PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    Set network weights randomly}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    input:}
\PYG{l+s+sd}{    ar \PYGZhy{} array of numbers of nodes in subsequent layers [n\PYGZus{}0, n\PYGZus{}1,...,n\PYGZus{}l]}
\PYG{l+s+sd}{    (from input layer 0 to output layer l, bias nodes not counted)}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    s \PYGZhy{} scale factor: each weight is in the range [\PYGZhy{}0.s, 0.5s]}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    return:}
\PYG{l+s+sd}{    w \PYGZhy{} dictionary of weights for neuron layers 1, 2,...,l in the format}
\PYG{l+s+sd}{    \PYGZob{}1: array[n\PYGZus{}0+1,n\PYGZus{}1],...,l: array[n\PYGZus{}(l\PYGZhy{}1)+1,n\PYGZus{}l]\PYGZcb{}}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{n}{l}\PYG{o}{=}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{)}
    \PYG{n}{w}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{p}{\PYGZcb{}}
    \PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{w}\PYG{o}{.}\PYG{n}{update}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{n}{k}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{:} \PYG{p}{[}\PYG{p}{[}\PYG{n}{s}\PYG{o}{*}\PYG{n}{rn}\PYG{p}{(}\PYG{p}{)} \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{k}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{]} \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{k}\PYG{p}{]}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{]}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{w}


\PYG{k}{def} \PYG{n+nf}{set\PYGZus{}val\PYGZus{}w}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{,}\PYG{n}{a}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    Set network weights to a constant value}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    input:}
\PYG{l+s+sd}{    ar \PYGZhy{} array of numbers of nodes in subsequent layers [n\PYGZus{}0, n\PYGZus{}1,...,n\PYGZus{}l]}
\PYG{l+s+sd}{    (from input layer 0 to output layer l, bias nodes not counted)}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    a \PYGZhy{} value for each weight}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    return:}
\PYG{l+s+sd}{    w \PYGZhy{} dictionary of weights for neuron layers 1, 2,...,l in the format}
\PYG{l+s+sd}{    \PYGZob{}1: array[n\PYGZus{}0+1,n\PYGZus{}1],...,l: array[n\PYGZus{}(l\PYGZhy{}1)+1,n\PYGZus{}l]\PYGZcb{}}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{n}{l}\PYG{o}{=}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{)}
    \PYG{n}{w}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{p}{\PYGZcb{}}
    \PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{w}\PYG{o}{.}\PYG{n}{update}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{n}{k}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{:} \PYG{p}{[}\PYG{p}{[}\PYG{n}{a} \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{k}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{]} \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{k}\PYG{p}{]}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{]}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{w}
    

\PYG{k}{def} \PYG{n+nf}{feed\PYGZus{}forward}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{,} \PYG{n}{we}\PYG{p}{,} \PYG{n}{x\PYGZus{}in}\PYG{p}{,} \PYG{n}{ff}\PYG{o}{=}\PYG{n}{step}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    Feed\PYGZhy{}forward propagation}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    input:}
\PYG{l+s+sd}{    ar \PYGZhy{} array of numbers of nodes in subsequent layers [n\PYGZus{}0, n\PYGZus{}1,...,n\PYGZus{}l]}
\PYG{l+s+sd}{    (from input layer 0 to output layer l, bias nodes not counted)}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    we \PYGZhy{} dictionary of weights for neuron layers 1, 2,...,l in the format}
\PYG{l+s+sd}{    \PYGZob{}1: array[n\PYGZus{}0+1,n\PYGZus{}1],...,l: array[n\PYGZus{}(l\PYGZhy{}1)+1,n\PYGZus{}l]\PYGZcb{}}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    x\PYGZus{}in \PYGZhy{} input vector of length n\PYGZus{}0 (bias not included)}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    ff \PYGZhy{} activation function (default: step)}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    return:}
\PYG{l+s+sd}{    x \PYGZhy{} dictionary of signals leaving subsequent layers in the format}
\PYG{l+s+sd}{    \PYGZob{}0: array[n\PYGZus{}0+1],...,l\PYGZhy{}1: array[n\PYGZus{}(l\PYGZhy{}1)+1], l: array[nl]\PYGZcb{}}
\PYG{l+s+sd}{    (the output layer carries no bias)}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{n}{l}\PYG{o}{=}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{)}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}                   \PYG{c+c1}{\PYGZsh{} number of neuron layers}
    \PYG{n}{x\PYGZus{}in}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{insert}\PYG{p}{(}\PYG{n}{x\PYGZus{}in}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{)}      \PYG{c+c1}{\PYGZsh{} input, with the bias node inserted}
    
    \PYG{n}{x}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{p}{\PYGZcb{}}                          \PYG{c+c1}{\PYGZsh{} empty dictionary}
    \PYG{n}{x}\PYG{o}{.}\PYG{n}{update}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{l+m+mi}{0}\PYG{p}{:} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{n}{x\PYGZus{}in}\PYG{p}{)}\PYG{p}{\PYGZcb{}}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} add input signal}
    
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}        \PYG{c+c1}{\PYGZsh{} loop over layers till before last one}
        \PYG{n}{s}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{,}\PYG{n}{we}\PYG{p}{[}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}    \PYG{c+c1}{\PYGZsh{} signal, matrix multiplication}
        \PYG{n}{y}\PYG{o}{=}\PYG{p}{[}\PYG{n}{ff}\PYG{p}{(}\PYG{n}{s}\PYG{p}{[}\PYG{n}{k}\PYG{p}{]}\PYG{p}{)} \PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{]} \PYG{c+c1}{\PYGZsh{} output from activation}
        \PYG{n}{x}\PYG{o}{.}\PYG{n}{update}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{:} \PYG{n}{np}\PYG{o}{.}\PYG{n}{insert}\PYG{p}{(}\PYG{n}{y}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{\PYGZcb{}}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} add bias node and update x}

    \PYG{c+c1}{\PYGZsh{} the last layer \PYGZhy{} no adding of the bias node}
    \PYG{n}{s}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{[}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{we}\PYG{p}{[}\PYG{n}{l}\PYG{p}{]}\PYG{p}{)}
    \PYG{n}{y}\PYG{o}{=}\PYG{p}{[}\PYG{n}{ff}\PYG{p}{(}\PYG{n}{s}\PYG{p}{[}\PYG{n}{q}\PYG{p}{]}\PYG{p}{)} \PYG{k}{for} \PYG{n}{q} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{l}\PYG{p}{]}\PYG{p}{)}\PYG{p}{]}
    \PYG{n}{x}\PYG{o}{.}\PYG{n}{update}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{n}{l}\PYG{p}{:} \PYG{n}{y}\PYG{p}{\PYGZcb{}}\PYG{p}{)}          \PYG{c+c1}{\PYGZsh{} update x}
          
    \PYG{k}{return} \PYG{n}{x}


\PYG{k}{def} \PYG{n+nf}{back\PYGZus{}prop}\PYG{p}{(}\PYG{n}{fe}\PYG{p}{,}\PYG{n}{la}\PYG{p}{,} \PYG{n}{p}\PYG{p}{,} \PYG{n}{ar}\PYG{p}{,} \PYG{n}{we}\PYG{p}{,} \PYG{n}{eps}\PYG{p}{,}\PYG{n}{f}\PYG{o}{=}\PYG{n}{sig}\PYG{p}{,} \PYG{n}{df}\PYG{o}{=}\PYG{n}{dsig}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    back propagation algorithm}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    fe \PYGZhy{} array of features}
\PYG{l+s+sd}{    la \PYGZhy{} array of labels}
\PYG{l+s+sd}{    p  \PYGZhy{} index of the used data point}
\PYG{l+s+sd}{    ar \PYGZhy{} array of numbers of nodes in subsequent layers}
\PYG{l+s+sd}{    we \PYGZhy{} dictionary of weights \PYGZhy{} UPDATED}
\PYG{l+s+sd}{    eps \PYGZhy{} learning speed}
\PYG{l+s+sd}{    f   \PYGZhy{} activation function}
\PYG{l+s+sd}{    df  \PYGZhy{} derivative of f}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
 
    \PYG{n}{l}\PYG{o}{=}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{)}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1} \PYG{c+c1}{\PYGZsh{} number of neuron layers (= index of the output layer)}
    \PYG{n}{nl}\PYG{o}{=}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{l}\PYG{p}{]}    \PYG{c+c1}{\PYGZsh{} number of neurons in the otput layer}
   
    \PYG{n}{x}\PYG{o}{=}\PYG{n}{feed\PYGZus{}forward}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{,}\PYG{n}{we}\PYG{p}{,}\PYG{n}{fe}\PYG{p}{[}\PYG{n}{p}\PYG{p}{]}\PYG{p}{,}\PYG{n}{ff}\PYG{o}{=}\PYG{n}{f}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} feed\PYGZhy{}forward of point p}
   
    \PYG{c+c1}{\PYGZsh{} formulas from the derivation in a one\PYGZhy{}to\PYGZhy{}one notation:}
    
    \PYG{n}{D}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{p}{\PYGZcb{}}
    \PYG{n}{D}\PYG{o}{.}\PYG{n}{update}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{n}{l}\PYG{p}{:} \PYG{p}{[}\PYG{l+m+mi}{2}\PYG{o}{*}\PYG{p}{(}\PYG{n}{x}\PYG{p}{[}\PYG{n}{l}\PYG{p}{]}\PYG{p}{[}\PYG{n}{gam}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{n}{la}\PYG{p}{[}\PYG{n}{p}\PYG{p}{]}\PYG{p}{[}\PYG{n}{gam}\PYG{p}{]}\PYG{p}{)}\PYG{o}{*}
                    \PYG{n}{df}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{[}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{we}\PYG{p}{[}\PYG{n}{l}\PYG{p}{]}\PYG{p}{)}\PYG{p}{[}\PYG{n}{gam}\PYG{p}{]}\PYG{p}{)} \PYG{k}{for} \PYG{n}{gam} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{nl}\PYG{p}{)}\PYG{p}{]}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
    \PYG{n}{we}\PYG{p}{[}\PYG{n}{l}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{o}{=}\PYG{n}{eps}\PYG{o}{*}\PYG{n}{np}\PYG{o}{.}\PYG{n}{outer}\PYG{p}{(}\PYG{n}{x}\PYG{p}{[}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{D}\PYG{p}{[}\PYG{n}{l}\PYG{p}{]}\PYG{p}{)}
    
    \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{reversed}\PYG{p}{(}\PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{n}{l}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{u}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{delete}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{we}\PYG{p}{[}\PYG{n}{j}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{D}\PYG{p}{[}\PYG{n}{j}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{)}
        \PYG{n}{v}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{[}\PYG{n}{j}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{we}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{p}{)}
        \PYG{n}{D}\PYG{o}{.}\PYG{n}{update}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{n}{j}\PYG{p}{:} \PYG{p}{[}\PYG{n}{u}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{o}{*}\PYG{n}{df}\PYG{p}{(}\PYG{n}{v}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{)} \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{u}\PYG{p}{)}\PYG{p}{)}\PYG{p}{]}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
        \PYG{n}{we}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{o}{=}\PYG{n}{eps}\PYG{o}{*}\PYG{n}{np}\PYG{o}{.}\PYG{n}{outer}\PYG{p}{(}\PYG{n}{x}\PYG{p}{[}\PYG{n}{j}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{D}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{p}{)}


\PYG{k}{def} \PYG{n+nf}{feed\PYGZus{}forward\PYGZus{}o}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{,} \PYG{n}{we}\PYG{p}{,} \PYG{n}{x\PYGZus{}in}\PYG{p}{,} \PYG{n}{ff}\PYG{o}{=}\PYG{n}{sig}\PYG{p}{,} \PYG{n}{ffo}\PYG{o}{=}\PYG{n}{lin}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    Feed\PYGZhy{}forward propagation with different output activation}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    input:}
\PYG{l+s+sd}{    ar \PYGZhy{} array of numbers of nodes in subsequent layers [n\PYGZus{}0, n\PYGZus{}1,...,n\PYGZus{}l]}
\PYG{l+s+sd}{    (from input layer 0 to output layer l, bias nodes not counted)}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    we \PYGZhy{} dictionary of weights for neuron layers 1, 2,...,l in the format}
\PYG{l+s+sd}{    \PYGZob{}1: array[n\PYGZus{}0+1,n\PYGZus{}1],...,l: array[n\PYGZus{}(l\PYGZhy{}1)+1,n\PYGZus{}l]\PYGZcb{}}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    x\PYGZus{}in \PYGZhy{} input vector of length n\PYGZus{}0 (bias not included)}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    f  \PYGZhy{} activation function (default: sigmoid)}
\PYG{l+s+sd}{    fo \PYGZhy{} activation function in the output layer (default: linear)}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    return:}
\PYG{l+s+sd}{    x \PYGZhy{} dictionary of signals leaving subsequent layers in the format}
\PYG{l+s+sd}{    \PYGZob{}0: array[n\PYGZus{}0+1],...,l\PYGZhy{}1: array[n\PYGZus{}(l\PYGZhy{}1)+1], l: array[nl]\PYGZcb{}}
\PYG{l+s+sd}{    (the output layer carries no bias)}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{n}{l}\PYG{o}{=}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{)}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}                   \PYG{c+c1}{\PYGZsh{} number of neuron layers}
    \PYG{n}{x\PYGZus{}in}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{insert}\PYG{p}{(}\PYG{n}{x\PYGZus{}in}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{)}      \PYG{c+c1}{\PYGZsh{} input, with the bias node inserted}
    
    \PYG{n}{x}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{p}{\PYGZcb{}}                          \PYG{c+c1}{\PYGZsh{} empty dictionary}
    \PYG{n}{x}\PYG{o}{.}\PYG{n}{update}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{l+m+mi}{0}\PYG{p}{:} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{n}{x\PYGZus{}in}\PYG{p}{)}\PYG{p}{\PYGZcb{}}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} add input signal}
    
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}        \PYG{c+c1}{\PYGZsh{} loop over layers till before last one}
        \PYG{n}{s}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{,}\PYG{n}{we}\PYG{p}{[}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}    \PYG{c+c1}{\PYGZsh{} signal, matrix multiplication}
        \PYG{n}{y}\PYG{o}{=}\PYG{p}{[}\PYG{n}{ff}\PYG{p}{(}\PYG{n}{s}\PYG{p}{[}\PYG{n}{k}\PYG{p}{]}\PYG{p}{)} \PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{]} \PYG{c+c1}{\PYGZsh{} output from activation}
        \PYG{n}{x}\PYG{o}{.}\PYG{n}{update}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{:} \PYG{n}{np}\PYG{o}{.}\PYG{n}{insert}\PYG{p}{(}\PYG{n}{y}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{\PYGZcb{}}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} add bias node and update x}

    \PYG{c+c1}{\PYGZsh{} the last layer \PYGZhy{} no adding of the bias node}
    \PYG{n}{s}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{[}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{we}\PYG{p}{[}\PYG{n}{l}\PYG{p}{]}\PYG{p}{)}
    \PYG{n}{y}\PYG{o}{=}\PYG{p}{[}\PYG{n}{ffo}\PYG{p}{(}\PYG{n}{s}\PYG{p}{[}\PYG{n}{q}\PYG{p}{]}\PYG{p}{)} \PYG{k}{for} \PYG{n}{q} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{l}\PYG{p}{]}\PYG{p}{)}\PYG{p}{]} \PYG{c+c1}{\PYGZsh{} output activation function}
    \PYG{n}{x}\PYG{o}{.}\PYG{n}{update}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{n}{l}\PYG{p}{:} \PYG{n}{y}\PYG{p}{\PYGZcb{}}\PYG{p}{)}                    \PYG{c+c1}{\PYGZsh{} update x}
          
    \PYG{k}{return} \PYG{n}{x}


\PYG{k}{def} \PYG{n+nf}{back\PYGZus{}prop\PYGZus{}o}\PYG{p}{(}\PYG{n}{fe}\PYG{p}{,}\PYG{n}{la}\PYG{p}{,} \PYG{n}{p}\PYG{p}{,} \PYG{n}{ar}\PYG{p}{,} \PYG{n}{we}\PYG{p}{,} \PYG{n}{eps}\PYG{p}{,} \PYG{n}{f}\PYG{o}{=}\PYG{n}{sig}\PYG{p}{,} \PYG{n}{df}\PYG{o}{=}\PYG{n}{dsig}\PYG{p}{,} \PYG{n}{fo}\PYG{o}{=}\PYG{n}{lin}\PYG{p}{,} \PYG{n}{dfo}\PYG{o}{=}\PYG{n}{dlin}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    backprop with different output activation}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    fe \PYGZhy{} array of features}
\PYG{l+s+sd}{    la \PYGZhy{} array of labels}
\PYG{l+s+sd}{    p  \PYGZhy{} index of the used data point}
\PYG{l+s+sd}{    ar \PYGZhy{} array of numbers of nodes in subsequent layers}
\PYG{l+s+sd}{    we \PYGZhy{} dictionary of weights \PYGZhy{} UPDATED}
\PYG{l+s+sd}{    eps \PYGZhy{} learning speed}
\PYG{l+s+sd}{    f   \PYGZhy{} activation function}
\PYG{l+s+sd}{    df  \PYGZhy{} derivative of f}
\PYG{l+s+sd}{    fo  \PYGZhy{} activation function in the output layer (default: linear)}
\PYG{l+s+sd}{    dfo \PYGZhy{} derivative of fo}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{n}{l}\PYG{o}{=}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{)}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1} \PYG{c+c1}{\PYGZsh{} number of neuron layers (= index of the output layer)}
    \PYG{n}{nl}\PYG{o}{=}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{l}\PYG{p}{]}    \PYG{c+c1}{\PYGZsh{} number of neurons in the otput layer}
   
    \PYG{n}{x}\PYG{o}{=}\PYG{n}{feed\PYGZus{}forward\PYGZus{}o}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{,}\PYG{n}{we}\PYG{p}{,}\PYG{n}{fe}\PYG{p}{[}\PYG{n}{p}\PYG{p}{]}\PYG{p}{,}\PYG{n}{ff}\PYG{o}{=}\PYG{n}{f}\PYG{p}{,}\PYG{n}{ffo}\PYG{o}{=}\PYG{n}{fo}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} feed\PYGZhy{}forward of point p}
   
    \PYG{c+c1}{\PYGZsh{} formulas from the derivation in a one\PYGZhy{}to\PYGZhy{}one notation:}
    
    \PYG{n}{D}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{p}{\PYGZcb{}}
    \PYG{n}{D}\PYG{o}{.}\PYG{n}{update}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{n}{l}\PYG{p}{:} \PYG{p}{[}\PYG{l+m+mi}{2}\PYG{o}{*}\PYG{p}{(}\PYG{n}{x}\PYG{p}{[}\PYG{n}{l}\PYG{p}{]}\PYG{p}{[}\PYG{n}{gam}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{n}{la}\PYG{p}{[}\PYG{n}{p}\PYG{p}{]}\PYG{p}{[}\PYG{n}{gam}\PYG{p}{]}\PYG{p}{)}\PYG{o}{*}
                   \PYG{n}{dfo}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{[}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{we}\PYG{p}{[}\PYG{n}{l}\PYG{p}{]}\PYG{p}{)}\PYG{p}{[}\PYG{n}{gam}\PYG{p}{]}\PYG{p}{)} \PYG{k}{for} \PYG{n}{gam} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{nl}\PYG{p}{)}\PYG{p}{]}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
    
    \PYG{n}{we}\PYG{p}{[}\PYG{n}{l}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{o}{=}\PYG{n}{eps}\PYG{o}{*}\PYG{n}{np}\PYG{o}{.}\PYG{n}{outer}\PYG{p}{(}\PYG{n}{x}\PYG{p}{[}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{D}\PYG{p}{[}\PYG{n}{l}\PYG{p}{]}\PYG{p}{)}
    
    \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{reversed}\PYG{p}{(}\PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{n}{l}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{u}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{delete}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{we}\PYG{p}{[}\PYG{n}{j}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{D}\PYG{p}{[}\PYG{n}{j}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{)}
        \PYG{n}{v}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{[}\PYG{n}{j}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{we}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{p}{)}
        \PYG{n}{D}\PYG{o}{.}\PYG{n}{update}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{n}{j}\PYG{p}{:} \PYG{p}{[}\PYG{n}{u}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{o}{*}\PYG{n}{df}\PYG{p}{(}\PYG{n}{v}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{)} \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{u}\PYG{p}{)}\PYG{p}{)}\PYG{p}{]}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
        \PYG{n}{we}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{o}{=}\PYG{n}{eps}\PYG{o}{*}\PYG{n}{np}\PYG{o}{.}\PYG{n}{outer}\PYG{p}{(}\PYG{n}{x}\PYG{p}{[}\PYG{n}{j}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{D}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{p}{)}
    
\end{sphinxVerbatim}


\subsection{Moduł \sphinxstylestrong{draw.py}}
\label{\detokenize{docs/appendix:modul-draw-py}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{Plotting functions used in the lecture.}
\PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}

\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}


\PYG{k}{def} \PYG{n+nf}{plot}\PYG{p}{(}\PYG{o}{*}\PYG{n}{args}\PYG{p}{,} \PYG{n}{title}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{activation function}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{x\PYGZus{}label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{signal}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{y\PYGZus{}label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{response}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
         \PYG{n}{start}\PYG{o}{=}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{stop}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{samples}\PYG{o}{=}\PYG{l+m+mi}{100}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    Wrapper on matplotlib.pyplot library.}
\PYG{l+s+sd}{    Plots functions passed as *args.}
\PYG{l+s+sd}{    Functions need to accept a single number argument and return a single number.}
\PYG{l+s+sd}{    Example usage:  plot(func.step,func.sig)}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{n}{s} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{linspace}\PYG{p}{(}\PYG{n}{start}\PYG{p}{,} \PYG{n}{stop}\PYG{p}{,} \PYG{n}{samples}\PYG{p}{)}

    \PYG{n}{ff}\PYG{o}{=}\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mf}{2.8}\PYG{p}{,}\PYG{l+m+mf}{2.3}\PYG{p}{)}\PYG{p}{,}\PYG{n}{dpi}\PYG{o}{=}\PYG{l+m+mi}{120}\PYG{p}{)}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{n}{title}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{n}{x\PYGZus{}label}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{n}{y\PYGZus{}label}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{11}\PYG{p}{)}

    \PYG{k}{for} \PYG{n}{fun} \PYG{o+ow}{in} \PYG{n}{args}\PYG{p}{:}
        \PYG{n}{data\PYGZus{}to\PYGZus{}plot} \PYG{o}{=} \PYG{p}{[}\PYG{n}{fun}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)} \PYG{k}{for} \PYG{n}{x} \PYG{o+ow}{in} \PYG{n}{s}\PYG{p}{]}
        \PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{s}\PYG{p}{,} \PYG{n}{data\PYGZus{}to\PYGZus{}plot}\PYG{p}{)}

    \PYG{k}{return} \PYG{n}{ff}\PYG{p}{;}


\PYG{k}{def} \PYG{n+nf}{plot\PYGZus{}net\PYGZus{}simp}\PYG{p}{(}\PYG{n}{n\PYGZus{}layer}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    Draw the network architecture without bias nodes}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    input: array of numbers of nodes in subsequent layers [n0, n1, n2,...]}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    return: graphics object}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{n}{l\PYGZus{}layer}\PYG{o}{=}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{n\PYGZus{}layer}\PYG{p}{)}
    \PYG{n}{ff}\PYG{o}{=}\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mf}{4.3}\PYG{p}{,}\PYG{l+m+mf}{2.3}\PYG{p}{)}\PYG{p}{,}\PYG{n}{dpi}\PYG{o}{=}\PYG{l+m+mi}{120}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} input nodes}
    \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{n\PYGZus{}layer}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}\PYG{p}{:}
            \PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{j}\PYG{o}{\PYGZhy{}}\PYG{n}{n\PYGZus{}layer}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{50}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{black}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{zorder}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} neuron layer nodes}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{n}{l\PYGZus{}layer}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{n\PYGZus{}layer}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{)}\PYG{p}{:}
            \PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{i}\PYG{p}{,} \PYG{n}{j}\PYG{o}{\PYGZhy{}}\PYG{n}{n\PYGZus{}layer}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{100}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{blue}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{zorder}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}
            
\PYG{c+c1}{\PYGZsh{} bias nodes}
    \PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{n\PYGZus{}layer}\PYG{p}{[}\PYG{n}{l\PYGZus{}layer}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{p}{[}\PYG{n}{l\PYGZus{}layer}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{n}{l\PYGZus{}layer}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{n}{n\PYGZus{}layer}\PYG{p}{[}\PYG{n}{l\PYGZus{}layer}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{n}{n\PYGZus{}layer}\PYG{p}{[}\PYG{n}{l\PYGZus{}layer}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{50}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gray}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{zorder}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} edges}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{l\PYGZus{}layer}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{n\PYGZus{}layer}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{)}\PYG{p}{:}
            \PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{n\PYGZus{}layer}\PYG{p}{[}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{:}
                \PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{n}{j}\PYG{o}{\PYGZhy{}}\PYG{n}{n\PYGZus{}layer}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{n}{k}\PYG{o}{\PYGZhy{}}\PYG{n}{n\PYGZus{}layer}\PYG{p}{[}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,} \PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gray}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{axis}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{off}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

    \PYG{k}{return} \PYG{n}{ff}\PYG{p}{;}


\PYG{k}{def} \PYG{n+nf}{plot\PYGZus{}net}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    Draw network with bias nodes}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    input:}
\PYG{l+s+sd}{    ar \PYGZhy{} array of numbers of nodes in subsequent layers [n\PYGZus{}0, n\PYGZus{}1,...,n\PYGZus{}l]}
\PYG{l+s+sd}{    (from input layer 0 to output layer l, bias nodes not counted)}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    return: graphics object}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{n}{l}\PYG{o}{=}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{)}
    \PYG{n}{ff}\PYG{o}{=}\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mf}{4.3}\PYG{p}{,}\PYG{l+m+mf}{2.3}\PYG{p}{)}\PYG{p}{,}\PYG{n}{dpi}\PYG{o}{=}\PYG{l+m+mi}{120}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} input nodes}
    \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}\PYG{p}{:}
            \PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{j}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{50}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{black}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{zorder}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} neuron layer nodes}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{n}{l}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{)}\PYG{p}{:}
            \PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{i}\PYG{p}{,} \PYG{n}{j}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{100}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{blue}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{zorder}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} bias nodes}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}
            \PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{i}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{50}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gray}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{zorder}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} edges}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}
            \PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{:}
                \PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{n}{j}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{n}{k}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gray}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} the last edge on the right}
    \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{p}{[}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{o}{+}\PYG{l+m+mf}{0.7}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{n}{j}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{n}{j}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gray}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{axis}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{off}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

    \PYG{k}{return} \PYG{n}{ff}\PYG{p}{;}


\PYG{k}{def} \PYG{n+nf}{plot\PYGZus{}net\PYGZus{}w}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{,}\PYG{n}{we}\PYG{p}{,}\PYG{n}{wid}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    Draw the network architecture with weights}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    input:}
\PYG{l+s+sd}{    ar \PYGZhy{} array of numbers of nodes in subsequent layers [n\PYGZus{}0, n\PYGZus{}1,...,n\PYGZus{}l]}
\PYG{l+s+sd}{    (from input layer 0 to output layer l, bias nodes not counted)}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    we \PYGZhy{} dictionary of weights for neuron layers 1, 2,...,l in the format}
\PYG{l+s+sd}{    \PYGZob{}1: array[n\PYGZus{}0+1,n\PYGZus{}1],...,l: array[n\PYGZus{}(l\PYGZhy{}1)+1,n\PYGZus{}l]\PYGZcb{}}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    wid \PYGZhy{} controls the width of the lines}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    return: graphics object}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{n}{l}\PYG{o}{=}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{)}
    \PYG{n}{ff}\PYG{o}{=}\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mf}{4.3}\PYG{p}{,}\PYG{l+m+mf}{2.3}\PYG{p}{)}\PYG{p}{,}\PYG{n}{dpi}\PYG{o}{=}\PYG{l+m+mi}{120}\PYG{p}{)}
    
\PYG{c+c1}{\PYGZsh{} input nodes}
    \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}\PYG{p}{:}
            \PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{j}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{50}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{black}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{zorder}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} neuron layer nodes}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{n}{l}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{)}\PYG{p}{:}
            \PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{i}\PYG{p}{,} \PYG{n}{j}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{100}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{blue}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{zorder}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} bias nodes}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}
            \PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{i}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{50}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gray}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{zorder}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} edges}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}
            \PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{:}
                \PYG{n}{th}\PYG{o}{=}\PYG{n}{wid}\PYG{o}{*}\PYG{n}{we}\PYG{p}{[}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{p}{[}\PYG{n}{k}\PYG{p}{]}
                \PYG{k}{if} \PYG{n}{th}\PYG{o}{\PYGZgt{}}\PYG{l+m+mi}{0}\PYG{p}{:}
                    \PYG{n}{col}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{red}\PYG{l+s+s1}{\PYGZsq{}}
                \PYG{k}{else}\PYG{p}{:}
                    \PYG{n}{col}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{blue}\PYG{l+s+s1}{\PYGZsq{}}
                \PYG{n}{th}\PYG{o}{=}\PYG{n+nb}{abs}\PYG{p}{(}\PYG{n}{th}\PYG{p}{)}
                \PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{n}{j}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{n}{k}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{n}{col}\PYG{p}{,}\PYG{n}{linewidth}\PYG{o}{=}\PYG{n}{th}\PYG{p}{)}
 
\PYG{c+c1}{\PYGZsh{} the last edge on the right}
    \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{p}{[}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{o}{+}\PYG{l+m+mf}{0.7}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{n}{j}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{n}{j}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gray}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{axis}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{off}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

    \PYG{k}{return} \PYG{n}{ff}\PYG{p}{;}


\PYG{k}{def} \PYG{n+nf}{plot\PYGZus{}net\PYGZus{}w\PYGZus{}x}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{,}\PYG{n}{we}\PYG{p}{,}\PYG{n}{wid}\PYG{p}{,}\PYG{n}{x}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    Draw the network architecture with weights and signals}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    input:}
\PYG{l+s+sd}{    ar \PYGZhy{} array of numbers of nodes in subsequent layers [n\PYGZus{}0, n\PYGZus{}1,...,n\PYGZus{}l]}
\PYG{l+s+sd}{    (from input layer 0 to output layer l, bias nodes not counted)}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    we \PYGZhy{} dictionary of weights for neuron layers 1, 2,...,l in the format}
\PYG{l+s+sd}{    \PYGZob{}1: array[n\PYGZus{}0+1,n\PYGZus{}1],...,l: array[n\PYGZus{}(l\PYGZhy{}1)+1,n\PYGZus{}l]\PYGZcb{}}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    wid \PYGZhy{} controls the width of the lines}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    x \PYGZhy{} dictionary the the signal in the format}
\PYG{l+s+sd}{    \PYGZob{}0: array[n\PYGZus{}0+1],...,l\PYGZhy{}1: array[n\PYGZus{}(l\PYGZhy{}1)+1], l: array[nl]\PYGZcb{}}
\PYG{l+s+sd}{    }
\PYG{l+s+sd}{    return: graphics object}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{n}{l}\PYG{o}{=}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{)}
    \PYG{n}{ff}\PYG{o}{=}\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mf}{4.3}\PYG{p}{,}\PYG{l+m+mf}{2.3}\PYG{p}{)}\PYG{p}{,}\PYG{n}{dpi}\PYG{o}{=}\PYG{l+m+mi}{120}\PYG{p}{)}
    
\PYG{c+c1}{\PYGZsh{} input layer}
    \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}\PYG{p}{:}
            \PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{j}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{50}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{black}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{zorder}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}
            \PYG{n}{lab}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(}\PYG{n}{x}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{[}\PYG{n}{j}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}
            \PYG{n}{plt}\PYG{o}{.}\PYG{n}{text}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.27}\PYG{p}{,} \PYG{n}{j}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{o}{+}\PYG{l+m+mf}{0.1}\PYG{p}{,} \PYG{n}{lab}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{7}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} intermediate layer}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{)}\PYG{p}{:}
            \PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{i}\PYG{p}{,} \PYG{n}{j}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{100}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{blue}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{zorder}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}
            \PYG{n}{lab}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(}\PYG{n}{x}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{n}{j}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}
            \PYG{n}{plt}\PYG{o}{.}\PYG{n}{text}\PYG{p}{(}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mf}{0.1}\PYG{p}{,} \PYG{n}{j}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{o}{+}\PYG{l+m+mf}{0.1}\PYG{p}{,} \PYG{n}{lab}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{7}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} output layer}
    \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{j}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{100}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{blue}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{zorder}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}
        \PYG{n}{lab}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{round}\PYG{p}{(}\PYG{n}{x}\PYG{p}{[}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}
        \PYG{n}{plt}\PYG{o}{.}\PYG{n}{text}\PYG{p}{(}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{o}{+}\PYG{l+m+mf}{0.1}\PYG{p}{,} \PYG{n}{j}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{o}{+}\PYG{l+m+mf}{0.1}\PYG{p}{,} \PYG{n}{lab}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{7}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} bias nodes}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}
            \PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{i}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{50}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gray}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{n}{zorder}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} edges}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}
            \PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{:}
                \PYG{n}{th}\PYG{o}{=}\PYG{n}{wid}\PYG{o}{*}\PYG{n}{we}\PYG{p}{[}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]}\PYG{p}{[}\PYG{n}{k}\PYG{p}{]}
                \PYG{k}{if} \PYG{n}{th}\PYG{o}{\PYGZgt{}}\PYG{l+m+mi}{0}\PYG{p}{:}
                    \PYG{n}{col}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{red}\PYG{l+s+s1}{\PYGZsq{}}
                \PYG{k}{else}\PYG{p}{:}
                    \PYG{n}{col}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{blue}\PYG{l+s+s1}{\PYGZsq{}}
                \PYG{n}{th}\PYG{o}{=}\PYG{n+nb}{abs}\PYG{p}{(}\PYG{n}{th}\PYG{p}{)}
                \PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{n}{j}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{n}{k}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{n}{col}\PYG{p}{,}\PYG{n}{linewidth}\PYG{o}{=}\PYG{n}{th}\PYG{p}{)}
 
\PYG{c+c1}{\PYGZsh{} the last edge on the right}
    \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{p}{[}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{o}{+}\PYG{l+m+mf}{0.7}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{n}{j}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{n}{j}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{ar}\PYG{p}{[}\PYG{n}{l}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{o}{/}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,}\PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gray}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{axis}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{off}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

    \PYG{k}{return} \PYG{n}{ff}\PYG{p}{;}
    
    
\PYG{k}{def} \PYG{n+nf}{l2}\PYG{p}{(}\PYG{n}{w0}\PYG{p}{,}\PYG{n}{w1}\PYG{p}{,}\PYG{n}{w2}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}for separating line\PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{k}{return} \PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{.1}\PYG{p}{,}\PYG{l+m+mf}{1.1}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{w0}\PYG{o}{\PYGZhy{}}\PYG{n}{w1}\PYG{o}{*}\PYG{l+m+mf}{0.1}\PYG{p}{)}\PYG{o}{/}\PYG{n}{w2}\PYG{p}{,}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{w0}\PYG{o}{+}\PYG{n}{w1}\PYG{o}{*}\PYG{l+m+mf}{1.1}\PYG{p}{)}\PYG{o}{/}\PYG{n}{w2}\PYG{p}{]}

\end{sphinxVerbatim}


\section{Jak cytować}
\label{\detokenize{docs/appendix:jak-cytowac}}
\sphinxAtStartPar
Jeśli chcesz zacytować tę książkę Jupyter Book, oto wpis w formacie BibTeX do wersji angielskiej:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nd}{@book}\PYG{p}{\PYGZob{}}\PYG{n}{WB2021}\PYG{p}{,}
  \PYG{n}{title}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Explaining neural networks in raw Python: lectures in Jupiter}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{\PYGZcb{}}\PYG{p}{,}
  \PYG{n}{author}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{n}{Wojciech} \PYG{n}{Broniowski}\PYG{p}{\PYGZcb{}}\PYG{p}{,}
  \PYG{n}{isbn}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{l+m+mi}{978}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{83}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{962099}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{0}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{0}\PYG{p}{\PYGZcb{}}\PYG{p}{,}
  \PYG{n}{year}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{l+m+mi}{2021}\PYG{p}{\PYGZcb{}}\PYG{p}{,}
  \PYG{n}{url}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{n}{https}\PYG{p}{:}\PYG{o}{/}\PYG{o}{/}\PYG{n}{ifj}\PYG{o}{.}\PYG{n}{edu}\PYG{o}{.}\PYG{n}{pl}\PYG{o}{/}\PYG{n}{strony}\PYG{o}{/}\PYG{o}{\PYGZti{}}\PYG{n}{broniows}\PYG{o}{/}\PYG{n}{nn}\PYG{p}{\PYGZcb{}}
  \PYG{n}{publisher}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{n}{Wojciech} \PYG{n}{Broniowski}\PYG{p}{\PYGZcb{}}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}

\begin{sphinxthebibliography}{MullerRS}
\bibitem[Bar16]{docs/conclusion:id5}
\sphinxAtStartPar
P. Barry. \sphinxstyleemphasis{Head First Python: A Brain\sphinxhyphen{}Friendly Guide}. O'Reilly Media, 2016. ISBN 9781491919491. URL: \sphinxurl{https://books.google.pl/books?id=NIqNDQAAQBAJ}.
\bibitem[BH69]{docs/conclusion:id12}
\sphinxAtStartPar
A. E. Bryson and Y.\sphinxhyphen{}C. Ho. \sphinxstyleemphasis{Applied optimal control: Optimization, estimation, and control}. Waltham, Mass: Blaisdell Pub. Co., 1969.
\bibitem[FR13]{docs/conclusion:id8}
\sphinxAtStartPar
J. Feldman and R. Rojas. \sphinxstyleemphasis{Neural Networks: A Systematic Introduction}. Springer Berlin Heidelberg, 2013. ISBN 9783642610684.
\bibitem[Fre93]{docs/conclusion:id10}
\sphinxAtStartPar
James A. Freeman. \sphinxstyleemphasis{Simulating Neural Networks with Mathematica}. Addison\sphinxhyphen{}Wesley Professional, 1993. ISBN 9780201566291.
\bibitem[FS91]{docs/conclusion:id11}
\sphinxAtStartPar
James A. Freeman and David M. Skapura. \sphinxstyleemphasis{Neural Networks: Algorithms, Applications, and Programming Techniques (Computation and Neural Systems Series)}. Addison\sphinxhyphen{}Wesley, 1991. ISBN 9780201513769.
\bibitem[Gut16]{docs/conclusion:id3}
\sphinxAtStartPar
John Guttag. \sphinxstyleemphasis{Introduction to computation and programming using Python: With application to understanding data}. MIT Press, 2016.
\bibitem[HR72]{docs/conclusion:id14}
\sphinxAtStartPar
K. K. Hartline and F. Ratcliff. Inhibitory interactions in the retina of limulus. \sphinxstyleemphasis{Handbook of sensory physiology}, VII(2):381–447, 1972.
\bibitem[KSJ+12]{docs/conclusion:id6}
\sphinxAtStartPar
E.R. Kandel, J.H. Schwartz, T.M. Jessell, S.A. Siegelbaum, and A.J. Hudspeth. \sphinxstyleemphasis{Principles of Neural Science, Fifth Edition}. McGraw\sphinxhyphen{}Hill Education, 2012. ISBN 9780071810012. URL: \sphinxurl{https://books.google.pl/books?id=Z2yVUTnlIQsC}.
\bibitem[Mat19]{docs/conclusion:id2}
\sphinxAtStartPar
E. Matthes. \sphinxstyleemphasis{Python Crash Course, 2nd Edition: A Hands\sphinxhyphen{}On, Project\sphinxhyphen{}Based Introduction to Programming}. No Starch Press, 2019. ISBN 9781593279295. URL: \sphinxurl{https://books.google.pl/books?id=boBxDwAAQBAJ}.
\bibitem[MP43]{docs/conclusion:id9}
\sphinxAtStartPar
Warren S. McCulloch and Walter Pitts. The logical calculus of the ideas immanent in nervous activity. \sphinxstyleemphasis{The bulletin of mathematical biophysics}, 5(4):115–133, 1943. URL: \sphinxurl{https://doi.org/10.1007/BF02478259}.
\bibitem[MullerRS12]{docs/conclusion:id7}
\sphinxAtStartPar
B. Müller, J. Reinhardt, and M.T. Strickland. \sphinxstyleemphasis{Neural Networks: An Introduction}. Physics of Neural Networks. Springer Berlin Heidelberg, 2012. ISBN 9783642577604. URL: \sphinxurl{https://books.google.pl/books?id=on0QBwAAQBAJ}.
\bibitem[RIV91]{docs/conclusion:id13}
\sphinxAtStartPar
A. K. Rigler, J. M. Irvine, and Thomas P. Vogl. Rescaling of variables in back propagation learning. \sphinxstyleemphasis{Neural Networks}, 4(2):225–229, 1991. URL: \sphinxurl{https://doi.org/10.1016/0893-6080(91)90006-Q}, \sphinxhref{https://doi.org/10.1016/0893-6080(91)90006-Q}{doi:10.1016/0893\sphinxhyphen{}6080(91)90006\sphinxhyphen{}Q}.
\end{sphinxthebibliography}







\renewcommand{\indexname}{Indeks}
\printindex
\end{document}